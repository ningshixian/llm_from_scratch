import tiktoken
import time

def main():
    print("===== TikToken åŠŸèƒ½å±•ç¤º =====\n")
    
    # 1. åŸºæœ¬ç¼–ç å’Œè§£ç 
    basic_encoding_decoding()
    
    # 2. ä¸åŒç¼–ç å™¨æ¯”è¾ƒ
    different_encoders()
    
    # 3. æ¨¡åž‹ç‰¹å®šç¼–ç å™¨
    model_specific_encoders()
    
    # 4. æŸ¥çœ‹tokenå†…å®¹ - å±•ç¤ºæ¯ä¸ªtokenå¯¹åº”çš„å…·ä½“å­—èŠ‚å’Œæ–‡æœ¬å†…å®¹
    inspect_tokens()
    
    # 5. å¤šè¯­è¨€æ¯”è¾ƒ - æ¯”è¾ƒä¸åŒè¯­è¨€æ–‡æœ¬çš„åˆ†è¯æ•ˆçŽ‡å’Œtokenæ•°é‡
    multilingual_comparison()
    
    # 6. æ‰¹å¤„ç†æ¼”ç¤º - å±•ç¤ºæ‰¹é‡å¤„ç†æ–‡æœ¬çš„æ€§èƒ½ä¼˜åŠ¿
    batch_processing()
    
    # 7. å¯è§†åŒ–åˆ†è¯æ•ˆæžœ - ç›´è§‚å±•ç¤ºæ–‡æœ¬å¦‚ä½•è¢«æ‹†åˆ†æˆtokens
    visualize_tokenization()
    

def basic_encoding_decoding():
    """
    æ¼”ç¤ºtiktokençš„åŸºæœ¬ç¼–ç å’Œè§£ç åŠŸèƒ½
    åŒ…æ‹¬åŸºæœ¬çš„æ–‡æœ¬ç¼–ç ã€è§£ç ä»¥åŠencodeå’Œencode_ordinaryæ–¹æ³•çš„æ¯”è¾ƒ
    """
    print("1. åŸºæœ¬ç¼–ç å’Œè§£ç åŠŸèƒ½:")
    # èŽ·å–cl100k_baseç¼–ç å™¨ï¼Œè¿™æ˜¯GPT-4å’ŒChatGPTä½¿ç”¨çš„ç¼–ç å™¨
    enc = tiktoken.get_encoding("cl100k_base")
    
    # æµ‹è¯•æ–‡æœ¬
    text = "ç†è§£BPEåˆ†è¯åŽŸç†å¯¹ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¾ˆæœ‰å¸®åŠ©ï¼"
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDåˆ—è¡¨
    tokens = enc.encode(text)
    
    print(f"   åŽŸå§‹æ–‡æœ¬: {text}")
    print(f"   ç¼–ç åŽçš„tokens: {tokens}")  # æ˜¾ç¤ºtoken IDåˆ—è¡¨
    print(f"   tokenæ•°é‡: {len(tokens)}")  # è®¡ç®—tokenæ•°é‡
    print(f"   è§£ç åŽçš„æ–‡æœ¬: {enc.decode(tokens)}")  # å°†token IDåˆ—è¡¨è½¬æ¢å›žæ–‡æœ¬
    
    # æ¯”è¾ƒencodeå’Œencode_ordinaryæ–¹æ³•
    # encode_ordinaryä¸ä¼šæ‰§è¡Œç‰¹æ®Štokensçš„å¤„ç†ï¼Œå¦‚BOSã€EOSç­‰
    tokens_ordinary = enc.encode_ordinary(text)
    print(f"   ä½¿ç”¨encode_ordinaryçš„tokens: {tokens_ordinary}")
    print(f"   æ˜¯å¦ä¸Žencodeç»“æžœç›¸åŒ: {tokens == tokens_ordinary}")  # å¯¹æ¯”ä¸¤ç§ç¼–ç æ–¹å¼çš„ç»“æžœ
    print()

def different_encoders():
    print("2. ä¸åŒç¼–ç å™¨æ¯”è¾ƒ:")
    
    text = "GPTæ¨¡åž‹ä½¿ç”¨tiktokenè¿›è¡Œæ–‡æœ¬åˆ†è¯å¤„ç†ã€‚"
    
    encoders = {
        "cl100k_base": tiktoken.get_encoding("cl100k_base"),  # GPT-4, ChatGPT
        "p50k_base": tiktoken.get_encoding("p50k_base"),      # GPT-3, Codex
        "r50k_base": tiktoken.get_encoding("r50k_base")       # GPT-2, æ—©æœŸGPT-3
    }
    
    print(f"   æµ‹è¯•æ–‡æœ¬: {text}")
    for name, enc in encoders.items():
        tokens = enc.encode(text)
        print(f"   {name}: {len(tokens)} tokens - {tokens}")
    print()

def model_specific_encoders():
    print("3. ç‰¹å®šæ¨¡åž‹çš„ç¼–ç å™¨:")
    
    text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•OpenAIæ¨¡åž‹åˆ†è¯çš„ç¤ºä¾‹ã€‚"
    
    models = ["gpt-4", "gpt-3.5-turbo", "text-davinci-003", "text-embedding-ada-002"]
    
    for model in models:
        try:
            enc = tiktoken.encoding_for_model(model)
            tokens = enc.encode(text)
            print(f"   {model}: {len(tokens)} tokens - ä½¿ç”¨ç¼–ç å™¨: {enc.name}")
        except KeyError:
            print(f"   {model}: æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ç¼–ç å™¨")
    print()

def inspect_tokens():
    print("4. æŸ¥çœ‹Tokençš„å…·ä½“å†…å®¹:")
    
    enc = tiktoken.get_encoding("cl100k_base")
    text = "Hello, ä½ å¥½ä¸–ç•Œ!"
    tokens = enc.encode(text)
    
    print(f"   åŽŸå§‹æ–‡æœ¬: {text}")
    print(f"   Token IDs: {tokens}")
    
    print("   æ¯ä¸ªtokenå¯¹åº”çš„å­—èŠ‚å’Œæ–‡æœ¬:")
    for token in tokens:
        bytes_repr = enc.decode_single_token_bytes(token)
        try:
            text_repr = bytes_repr.decode('utf-8')
            print(f"   Token ID: {token}, å­—èŠ‚: {bytes_repr}, æ–‡æœ¬: '{text_repr}'")
        except UnicodeDecodeError:
            print(f"   Token ID: {token}, å­—èŠ‚: {bytes_repr}, æ–‡æœ¬: [æ— æ³•è§£ç ] (éžå®Œæ•´UTF-8å­—èŠ‚)")
    print()

def multilingual_comparison():
    print("5. å¤šè¯­è¨€Tokenæ•°é‡å¯¹æ¯”:")
    
    enc = tiktoken.get_encoding("cl100k_base")
    
    texts = {
        "è‹±æ–‡": "This is a test of the tokenizer.",
        "ä¸­æ–‡": "è¿™æ˜¯ä¸€ä¸ªåˆ†è¯å™¨çš„æµ‹è¯•ã€‚",
        "æ—¥æ–‡": "ã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚",
        "éŸ©æ–‡": "ì´ê²ƒì€ í† í¬ë‚˜ì´ì €ì˜ í…ŒìŠ¤íŠ¸ìž…ë‹ˆë‹¤.",
        "ä¿„æ–‡": "Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°.",
        "è¡¨æƒ…ç¬¦å·": "ðŸ˜€ ðŸ‘ ðŸš€ ðŸŒ ðŸŽ‰"
    }
    
    for lang, text in texts.items():
        tokens = enc.encode(text)
        print(f"   {lang} ({len(text)}å­—ç¬¦): {len(tokens)} tokens - {text}")
    print()

def batch_processing():
    print("6. æ‰¹å¤„ç†ç¤ºä¾‹:")
    
    enc = tiktoken.get_encoding("cl100k_base")
    
    texts = [
        "ç¬¬ä¸€ä¸ªå¥å­å¾ˆçŸ­ã€‚",
        "ç¬¬äºŒä¸ªå¥å­ç¨å¾®é•¿ä¸€ç‚¹ï¼ŒåŒ…å«æ›´å¤šçš„å†…å®¹ã€‚",
        "ç¬¬ä¸‰ä¸ªå¥å­æ˜¯æœ€é•¿çš„ï¼Œå®ƒåŒ…å«äº†å¾ˆå¤šå¾ˆå¤šçš„æ–‡å­—å†…å®¹ï¼Œç›®çš„æ˜¯ä¸ºäº†æµ‹è¯•æ‰¹å¤„ç†çš„æ•ˆæžœã€‚"
    ]
    
    # å•ç‹¬ç¼–ç 
    start_time = time.time()
    tokens_individual = [enc.encode(text) for text in texts]
    individual_time = time.time() - start_time
    
    # æ‰¹é‡ç¼–ç 
    start_time = time.time()
    tokens_batch = enc.encode_batch(texts)
    batch_time = time.time() - start_time
    
    print(f"   å•ç‹¬ç¼–ç ç”¨æ—¶: {individual_time:.6f}ç§’")
    print(f"   æ‰¹é‡ç¼–ç ç”¨æ—¶: {batch_time:.6f}ç§’")
    print(f"   é€Ÿåº¦æå‡: {individual_time/batch_time:.2f}å€")
    
    for i, (text, tokens) in enumerate(zip(texts, tokens_batch)):
        print(f"   æ–‡æœ¬ {i+1}: {text}")
        print(f"   Tokenæ•°é‡: {len(tokens)}")
    print()

def visualize_tokenization():
    print("7. å¯è§†åŒ–åˆ†è¯æ•ˆæžœ:")
    enc = tiktoken.get_encoding("cl100k_base")
    
    texts = [
        "Hello, ä½ å¥½ä¸–ç•Œ!",
        "ç†è§£BPEåˆ†è¯åŽŸç†å¯¹ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¾ˆæœ‰å¸®åŠ©ï¼",
        "GPTæ¨¡åž‹ä½¿ç”¨tiktokenè¿›è¡Œæ–‡æœ¬åˆ†è¯å¤„ç†ã€‚",
        "English and ä¸­æ–‡æ··åˆçš„å¥å­ with some special chars: @#$%"
    ]
    
    for text in texts:
        tokens = enc.encode(text)
        visualization = ""
        start_index = 0
        
        print(f"\n   åŽŸå§‹æ–‡æœ¬: {text}")
        print(f"   Tokenæ•°é‡: {len(tokens)}")
        
        # æ–¹æ³•1: ä½¿ç”¨decode_single_token_bytesç›´æŽ¥é‡å»ºæ–‡æœ¬
        parts = []
        for token in tokens:
            byte_content = enc.decode_single_token_bytes(token)
            try:
                text_part = byte_content.decode("utf-8")
            except UnicodeDecodeError:
                text_part = f"[{byte_content.hex()}]"  # ä»¥åå…­è¿›åˆ¶æ˜¾ç¤ºæ— æ³•è§£ç çš„å­—èŠ‚
            parts.append(text_part)
        
        print(f"   åˆ†è¯ç»“æžœ: {' / '.join(parts)}")
        
        # æ–¹æ³•2: å¯¹å®Œæ•´æ–‡æœ¬å®šä½åˆ‡åˆ†ç‚¹
        cumulative_text = ""
        split_positions = []
        for token in tokens:
            byte_content = enc.decode_single_token_bytes(token)
            try:
                text_part = byte_content.decode("utf-8")
                cumulative_text += text_part
                split_positions.append(len(cumulative_text))
            except UnicodeDecodeError:
                # å¤„ç†æ— æ³•è§£ç çš„æƒ…å†µ
                cumulative_text += "â–¡"  # ä½¿ç”¨å ä½ç¬¦
                split_positions.append(len(cumulative_text))
        
        # åœ¨åŽŸæ–‡ä¸­æ’å…¥åˆ†éš”ç¬¦
        split_text = ""
        last_pos = 0
        for pos in split_positions:
            if pos > 0:
                split_text += text[last_pos:pos] + " / "
                last_pos = pos
        
        if last_pos < len(text):
            split_text += text[last_pos:]
            
        print(f"   åˆ†éš”æ ‡è®°: {split_text}")
    print()



if __name__ == "__main__":
    main()
    
    
"""
===== TikToken åŠŸèƒ½å±•ç¤º =====

1. åŸºæœ¬ç¼–ç å’Œè§£ç åŠŸèƒ½:
   åŽŸå§‹æ–‡æœ¬: ç†è§£BPEåˆ†è¯åŽŸç†å¯¹ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¾ˆæœ‰å¸®åŠ©ï¼
   ç¼–ç åŽçš„tokens: [22649, 50338, 33, 1777, 17620, 6744, 235, 53229, 22649, 33764, 38129, 27384, 25287, 73981, 78244, 54872, 25287, 17599, 230, 19361, 13821, 106, 8239, 102, 6447]
   tokenæ•°é‡: 25
   è§£ç åŽçš„æ–‡æœ¬: ç†è§£BPEåˆ†è¯åŽŸç†å¯¹ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¾ˆæœ‰å¸®åŠ©ï¼
   ä½¿ç”¨encode_ordinaryçš„tokens: [22649, 50338, 33, 1777, 17620, 6744, 235, 53229, 22649, 33764, 38129, 27384, 25287, 73981, 78244, 54872, 25287, 17599, 230, 19361, 13821, 106, 8239, 102, 6447]
   æ˜¯å¦ä¸Žencodeç»“æžœç›¸åŒ: True

2. ä¸åŒç¼–ç å™¨æ¯”è¾ƒ:
   æµ‹è¯•æ–‡æœ¬: GPTæ¨¡åž‹ä½¿ç”¨tiktokenè¿›è¡Œæ–‡æœ¬åˆ†è¯å¤„ç†ã€‚
   cl100k_base: 16 tokens - [38, 2898, 54872, 25287, 38129, 83, 1609, 5963, 72917, 17161, 22656, 17620, 6744, 235, 55642, 1811]
   p50k_base: 31 tokens - [38, 11571, 162, 101, 94, 161, 252, 233, 45635, 18796, 101, 83, 1134, 30001, 32573, 249, 26193, 234, 23877, 229, 17312, 105, 26344, 228, 46237, 235, 13783, 226, 49426, 228, 16764]
   r50k_base: 31 tokens - [38, 11571, 162, 101, 94, 161, 252, 233, 45635, 18796, 101, 83, 1134, 30001, 32573, 249, 26193, 234, 23877, 229, 17312, 105, 26344, 228, 46237, 235, 13783, 226, 49426, 228, 16764]

3. ç‰¹å®šæ¨¡åž‹çš„ç¼–ç å™¨:
   gpt-4: 15 tokens - ä½¿ç”¨ç¼–ç å™¨: cl100k_base
   gpt-3.5-turbo: 15 tokens - ä½¿ç”¨ç¼–ç å™¨: cl100k_base
   text-davinci-003: 30 tokens - ä½¿ç”¨ç¼–ç å™¨: p50k_base
   text-embedding-ada-002: 15 tokens - ä½¿ç”¨ç¼–ç å™¨: cl100k_base

4. æŸ¥çœ‹Tokençš„å…·ä½“å†…å®¹:
   åŽŸå§‹æ–‡æœ¬: Hello, ä½ å¥½ä¸–ç•Œ!
   Token IDs: [9906, 11, 220, 57668, 53901, 3574, 244, 98220, 0]
   æ¯ä¸ªtokenå¯¹åº”çš„å­—èŠ‚å’Œæ–‡æœ¬:
   Token ID: 9906, å­—èŠ‚: b'Hello', æ–‡æœ¬: 'Hello'
   Token ID: 11, å­—èŠ‚: b',', æ–‡æœ¬: ','
   Token ID: 220, å­—èŠ‚: b' ', æ–‡æœ¬: ' '
   Token ID: 57668, å­—èŠ‚: b'\xe4\xbd\xa0', æ–‡æœ¬: 'ä½ '
   Token ID: 53901, å­—èŠ‚: b'\xe5\xa5\xbd', æ–‡æœ¬: 'å¥½'
   Token ID: 3574, å­—èŠ‚: b'\xe4\xb8', æ–‡æœ¬: [æ— æ³•è§£ç ] (éžå®Œæ•´UTF-8å­—èŠ‚)
   Token ID: 244, å­—èŠ‚: b'\x96', æ–‡æœ¬: [æ— æ³•è§£ç ] (éžå®Œæ•´UTF-8å­—èŠ‚)
   Token ID: 98220, å­—èŠ‚: b'\xe7\x95\x8c', æ–‡æœ¬: 'ç•Œ'
   Token ID: 0, å­—èŠ‚: b'!', æ–‡æœ¬: '!'

5. å¤šè¯­è¨€Tokenæ•°é‡å¯¹æ¯”:
   è‹±æ–‡ (32å­—ç¬¦): 8 tokens - This is a test of the tokenizer.
   ä¸­æ–‡ (11å­—ç¬¦): 10 tokens - è¿™æ˜¯ä¸€ä¸ªåˆ†è¯å™¨çš„æµ‹è¯•ã€‚
   æ—¥æ–‡ (17å­—ç¬¦): 15 tokens - ã“ã‚Œã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚
   éŸ©æ–‡ (18å­—ç¬¦): 19 tokens - ì´ê²ƒì€ í† í¬ë‚˜ì´ì €ì˜ í…ŒìŠ¤íŠ¸ìž…ë‹ˆë‹¤.
   ä¿„æ–‡ (22å­—ç¬¦): 11 tokens - Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°.
   è¡¨æƒ…ç¬¦å· (9å­—ç¬¦): 13 tokens - ðŸ˜€ ðŸ‘ ðŸš€ ðŸŒ ðŸŽ‰

6. æ‰¹å¤„ç†ç¤ºä¾‹:
   å•ç‹¬ç¼–ç ç”¨æ—¶: 0.000589ç§’
   æ‰¹é‡ç¼–ç ç”¨æ—¶: 0.002336ç§’
   é€Ÿåº¦æå‡: 0.25å€
   æ–‡æœ¬ 1: ç¬¬ä¸€ä¸ªå¥å­å¾ˆçŸ­ã€‚
   Tokenæ•°é‡: 11
   æ–‡æœ¬ 2: ç¬¬äºŒä¸ªå¥å­ç¨å¾®é•¿ä¸€ç‚¹ï¼ŒåŒ…å«æ›´å¤šçš„å†…å®¹ã€‚
   Tokenæ•°é‡: 19
   æ–‡æœ¬ 3: ç¬¬ä¸‰ä¸ªå¥å­æ˜¯æœ€é•¿çš„ï¼Œå®ƒåŒ…å«äº†å¾ˆå¤šå¾ˆå¤šçš„æ–‡å­—å†…å®¹ï¼Œç›®çš„æ˜¯ä¸ºäº†æµ‹è¯•æ‰¹å¤„ç†çš„æ•ˆæžœã€‚
   Tokenæ•°é‡: 39

7. å¯è§†åŒ–åˆ†è¯æ•ˆæžœ:

   åŽŸå§‹æ–‡æœ¬: Hello, ä½ å¥½ä¸–ç•Œ!
   Tokenæ•°é‡: 9
   åˆ†è¯ç»“æžœ: Hello / , /   / ä½  / å¥½ / [e4b8] / [96] / ç•Œ / !
   åˆ†éš”æ ‡è®°: Hello / , /   / ä½  / å¥½ / ä¸– / ç•Œ / ! /  /

   åŽŸå§‹æ–‡æœ¬: ç†è§£BPEåˆ†è¯åŽŸç†å¯¹ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¾ˆæœ‰å¸®åŠ©ï¼
   Tokenæ•°é‡: 25
   åˆ†è¯ç»“æžœ: ç† / è§£ / B / PE / åˆ† / [e8af] / [8d] / åŽŸ / ç† / å¯¹ / ä½¿ç”¨ / å¤§ / åž‹ / è¯­ / è¨€ / æ¨¡ / åž‹ / [e5be] / [88] / æœ‰ / [e5b8] / [ae] / [e58a] / [a9] / ï¼
   åˆ†éš”æ ‡è®°: ç† / è§£ / B / PE / åˆ† / è¯ / åŽŸ / ç† / å¯¹ / ä½¿ / ç”¨å¤§ / åž‹ / è¯­ / è¨€ / æ¨¡ / åž‹ / å¾ˆ / æœ‰ / å¸® / åŠ© / ï¼ /  /  /  /  /

   åŽŸå§‹æ–‡æœ¬: GPTæ¨¡åž‹ä½¿ç”¨tiktokenè¿›è¡Œæ–‡æœ¬åˆ†è¯å¤„ç†ã€‚
   Tokenæ•°é‡: 16
   åˆ†è¯ç»“æžœ: G / PT / æ¨¡ / åž‹ / ä½¿ç”¨ / t / ik / token / è¿›è¡Œ / æ–‡ / æœ¬ / åˆ† / [e8af] / [8d] / å¤„ç† / ã€‚
   åˆ†éš”æ ‡è®°: G / PT / æ¨¡ / åž‹ / ä½¿ç”¨ / t / ik / token / è¿›è¡Œ / æ–‡ / æœ¬ / åˆ† / è¯ / å¤„ / ç†ã€‚ /  /

   åŽŸå§‹æ–‡æœ¬: English and ä¸­æ–‡æ··åˆçš„å¥å­ with some special chars: @#$%
   Tokenæ•°é‡: 19
   åˆ†è¯ç»“æžœ: English /  and /  ä¸­ / æ–‡ / [e6b7] / [b7] / åˆ / çš„ / [e58f] / [a5] / å­ /  with /  some /  special /  chars / : /  @ / #$ / %
   åˆ†éš”æ ‡è®°: English /  and /  ä¸­ / æ–‡ / æ·· / åˆ / çš„ / å¥ / å­ /   / w / ith s / ome s / pecial c / hars:  / @ / #$ / % /  /
   

"""


"""
8. è§£é‡Šè§£ç å¤±è´¥çš„åŽŸå› :
   åœ¨BPEåˆ†è¯ä¸­ï¼Œæœ‰äº›tokenå¯èƒ½åªåŒ…å«UTF-8å­—ç¬¦çš„éƒ¨åˆ†å­—èŠ‚
   ä¾‹å¦‚ä¸­æ–‡å­—ç¬¦'ä¸–'çš„UTF-8ç¼–ç ä¸º \xe4\xb8\x96ï¼Œå¯èƒ½è¢«åˆ†æˆä¸¤ä¸ªtoken:
   - å‰ä¸¤ä¸ªå­—èŠ‚ \xe4\xb8
   - æœ€åŽä¸€ä¸ªå­—èŠ‚ \x96

   åŽŸæ–‡: 'ä¸–'
   å¯¹åº”çš„UTF-8å­—èŠ‚: b'\xe4\xb8\x96'
   åˆ†è¯ç»“æžœ: [3574, 244]
   å„tokenå¯¹åº”çš„å­—èŠ‚:
   Token 1: ID=3574, å­—èŠ‚=b'\xe4\xb8'
   Token 2: ID=244, å­—èŠ‚=b'\x96'

   å½“æˆ‘ä»¬å°è¯•å•ç‹¬è§£ç è¿™äº›éžå®Œæ•´çš„UTF-8å­—èŠ‚åºåˆ—æ—¶ï¼Œä¼šå‡ºçŽ°è§£ç å¤±è´¥
   åªæœ‰å°†æ‰€æœ‰å­—èŠ‚é‡æ–°ç»„åˆï¼Œæ‰èƒ½æ­£ç¡®è§£ç ä¸ºå®Œæ•´çš„Unicodeå­—ç¬¦
   é‡æ–°ç»„åˆæ‰€æœ‰å­—èŠ‚: b'\xe4\xb8\x96'
   ç»„åˆåŽè§£ç ç»“æžœ: 'ä¸–'

"""