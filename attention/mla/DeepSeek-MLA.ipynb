{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e1d707-4227-42f3-a444-49653b68de90",
   "metadata": {},
   "source": [
    "# 手撕DeepSeek MLA\n",
    "\n",
    "不考虑效率\n",
    "\n",
    "仅复现原论文公式\n",
    "\n",
    "> ref: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753857b-5bec-4b5a-a466-748692b910db",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab411a50-74bb-4712-92cb-9e8d0e34c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba34e285-1b4a-468d-b718-c940df817129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ModelArgs object at 0x3070effd0>\n"
     ]
    }
   ],
   "source": [
    "# @dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 64\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: int =  2\n",
    "\n",
    "    # down 两者远小于 dim\n",
    "    dc_kv: int = 4 \n",
    "    dc_q: int = 4\n",
    "\n",
    "bs = 3\n",
    "seq_len = 5\n",
    "\n",
    "config = ModelArgs()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f86f05e-2cf4-49db-8d21-0e85aba5f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.randn(bs, seq_len, config.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2da701-0413-493d-9b44-47f5ca88b305",
   "metadata": {},
   "source": [
    "## Standard Multi-Heads Attention\n",
    "\n",
    "参考Llama3-GQA， 去除rope简易实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd60e9c3-258b-4231-a687-54532e6f7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref : Llama3 GQA, ./notebook/Llama3-GQA.ipynb\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim) # \n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class MultiHeadsAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "\n",
    "        self.wq = nn.Linear(in_features=args.dim, out_features=args.n_heads * self.head_dim,bias=False,)\n",
    "        self.wk = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False,)\n",
    "        self.wv = nn.Linear(in_features=args.dim, out_features=args.n_kv_heads * self.head_dim,bias=False,)\n",
    "        self.wo = nn.Linear(in_features=args.n_heads * self.head_dim, out_features=args.dim,bias=False,)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # here we ignore RoPE\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        keys = repeat_kv( xk, self.n_rep )  \n",
    "        values = repeat_kv( xv, self.n_rep )  \n",
    "        \n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        values = values.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "\n",
    "        \n",
    "        scores = xq @ keys.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = scores @ values # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        mha_output = self.wo(output)\n",
    "        return mha_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa607562-f290-427b-92aa-e0b0fdf91af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadsAttention(\n",
      "  (wq): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (wk): Linear(in_features=64, out_features=16, bias=False)\n",
      "  (wv): Linear(in_features=64, out_features=16, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadsAttention(config)\n",
    "print(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae17e52-a0a2-4d47-9176-8e9b00e62687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "out = mha(h)\n",
    "print(h.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f7a65-fe89-4cd1-8f63-d03335ce4023",
   "metadata": {},
   "source": [
    "## Multi-Heads Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867810fa-ca2f-4822-95d8-bfe67dd0b1e5",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "1. 以下用down和up权重矩阵，代替直接的wq,wk,wv\n",
    "2. MLA矩阵发生于训练之时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cdab46-f575-416c-b00e-997b1af7ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadsLatentAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        # self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "        self.dc_kv = args.dc_kv\n",
    "        self.dc_q = args.dc_q\n",
    "\n",
    "        # MLA Structure\n",
    "        self.wq_down = nn.Linear(in_features=args.dim, out_features=args.dc_q, bias=False,)\n",
    "        self.wq_up = nn.Linear(in_features=args.dc_q, out_features=args.dim , bias=False,)\n",
    "\n",
    "        self.wkv_down = nn.Linear(in_features=args.dim, out_features=args.dc_kv, bias=False,)\n",
    "        self.wk_up = nn.Linear(in_features=args.dc_kv, out_features=args.dim, bias=False,)\n",
    "        self.wv_up = nn.Linear(in_features=args.dc_kv, out_features=args.dim, bias=False,)\n",
    "        \n",
    "        self.wo = nn.Linear(in_features=args.dim, out_features=args.dim,bias=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb8e36d-d8fb-4768-8810-a1f36e94eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadsLatentAttention(\n",
      "  (wq_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wq_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wkv_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wk_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wv_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mla = MultiHeadsLatentAttention(config)\n",
    "print(mla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3ec01-8073-4a03-b8e5-b4263465974e",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a000b1-e950-4ce8-80eb-1a445521f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# 该段代码是关键，先降维，再升维\n",
    "# xq = mha.wq(h)\n",
    "c_q = mla.wq_down(h)\n",
    "xq = mla.wq_up(c_q)\n",
    "\n",
    "# xk = mha.wk(h)\n",
    "# xk = mha.wv(h)\n",
    "c_kv = mla.wkv_down(h)\n",
    "xk = mla.wk_up(c_kv)\n",
    "xv = mla.wv_up(c_kv)\n",
    "\n",
    "print(xq.shape)\n",
    "print(xk.shape)\n",
    "print(xv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f29a56c-10f6-406b-bc00-b4d992dbc018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n",
      "tensor([[[ 4.5146e-02,  1.5168e-01, -1.3621e-02, -1.7782e-02,  2.9059e-02,\n",
      "          -8.5655e-02,  1.1190e-02, -2.2851e-03, -5.7409e-02,  2.2138e-02,\n",
      "          -4.4059e-02, -6.3881e-02, -4.2939e-02, -1.9402e-02, -2.1850e-02,\n",
      "          -9.7752e-02,  1.3288e-01,  1.2167e-02, -4.2248e-02, -4.9612e-02,\n",
      "           7.4241e-02,  1.2082e-01, -4.5428e-02,  7.9689e-02,  1.0336e-01,\n",
      "          -4.8504e-02, -8.4458e-02,  1.0638e-02,  6.1907e-02, -1.0487e-01,\n",
      "          -5.7420e-02,  3.0349e-02, -5.3687e-02,  3.8531e-02,  1.7106e-02,\n",
      "           9.5486e-02,  2.0908e-03, -1.3231e-01, -7.2697e-02, -1.4581e-01,\n",
      "          -1.2431e-02, -2.2906e-02, -2.7741e-02,  4.6539e-02,  2.8527e-02,\n",
      "          -2.7289e-02,  7.3095e-02,  5.9925e-02, -3.6553e-02,  1.1505e-01,\n",
      "          -1.1351e-01,  2.3730e-02,  8.4835e-02,  7.5701e-02, -8.1039e-02,\n",
      "           6.9898e-02,  6.0617e-02,  5.8915e-03, -3.7927e-02,  7.3171e-03,\n",
      "          -3.7854e-02,  9.2171e-03, -8.1242e-02, -4.6599e-02],\n",
      "         [ 2.1411e-03,  1.5543e-01, -1.8300e-02,  9.1083e-03,  4.1062e-02,\n",
      "          -6.4051e-02,  3.6469e-02, -2.3593e-02, -5.7936e-02,  5.4600e-03,\n",
      "           2.5553e-03, -6.3609e-02, -5.0468e-02, -4.9063e-02, -7.7394e-03,\n",
      "          -8.2370e-02,  7.9191e-02, -2.0073e-02, -2.1766e-02, -5.7207e-02,\n",
      "           1.1239e-01,  8.0574e-02, -7.5745e-02,  9.7053e-02,  1.0221e-01,\n",
      "          -5.8857e-02, -1.0902e-01,  1.3635e-02,  4.6453e-02, -1.0817e-01,\n",
      "          -6.3176e-02, -1.0149e-02, -1.0663e-01, -2.6329e-02, -2.1698e-02,\n",
      "           1.3341e-01, -2.8214e-02, -1.6025e-01, -6.3590e-02, -1.7613e-01,\n",
      "          -6.2244e-03, -2.1046e-02,  7.1788e-03,  4.9289e-02, -7.7482e-04,\n",
      "          -2.1897e-02,  8.4984e-02,  7.5616e-02, -4.1100e-02,  1.2478e-01,\n",
      "          -1.2082e-01, -2.5589e-02,  9.7052e-02,  8.2496e-02, -6.8389e-02,\n",
      "           5.0565e-02,  7.0224e-02, -2.5511e-02, -1.3928e-02,  4.3497e-04,\n",
      "          -8.7198e-02,  5.2853e-03, -7.9466e-02, -8.5218e-02],\n",
      "         [ 4.3191e-02,  1.4951e-01, -1.4661e-02, -1.4341e-02,  2.8501e-02,\n",
      "          -8.5512e-02,  7.4602e-03, -6.8740e-03, -5.4378e-02,  2.1747e-02,\n",
      "          -3.5414e-02, -5.9825e-02, -4.5402e-02, -2.0539e-02, -2.2541e-02,\n",
      "          -9.6845e-02,  1.2862e-01,  1.2616e-02, -3.8576e-02, -5.2654e-02,\n",
      "           7.7054e-02,  1.1907e-01, -5.1347e-02,  8.2389e-02,  1.0007e-01,\n",
      "          -4.9317e-02, -9.0231e-02,  1.0592e-02,  6.0129e-02, -1.0683e-01,\n",
      "          -5.9376e-02,  2.0740e-02, -6.1443e-02,  3.6310e-02,  1.2599e-02,\n",
      "           9.9959e-02, -2.2553e-03, -1.3567e-01, -7.3736e-02, -1.4242e-01,\n",
      "          -1.4083e-02, -2.3367e-02, -2.4099e-02,  4.4729e-02,  2.5160e-02,\n",
      "          -2.2069e-02,  7.3083e-02,  6.4335e-02, -3.6200e-02,  1.1433e-01,\n",
      "          -1.1419e-01,  1.7595e-02,  8.5586e-02,  7.4293e-02, -8.1516e-02,\n",
      "           6.8250e-02,  6.6847e-02,  6.9888e-03, -3.0959e-02,  9.3213e-03,\n",
      "          -4.5511e-02,  8.0053e-03, -8.4670e-02, -4.7194e-02],\n",
      "         [ 1.4448e-02,  1.4633e-01, -2.1310e-02,  6.6593e-03,  3.6860e-02,\n",
      "          -8.1237e-02,  2.8864e-02, -1.2289e-02, -4.8176e-02,  1.2780e-02,\n",
      "          -1.3975e-02, -6.0488e-02, -3.7988e-02, -2.9553e-02, -2.4036e-02,\n",
      "          -8.4824e-02,  1.0299e-01,  5.8181e-03, -2.0508e-02, -5.3398e-02,\n",
      "           8.5701e-02,  1.1564e-01, -8.2822e-02,  7.9968e-02,  1.0306e-01,\n",
      "          -5.8204e-02, -8.7218e-02,  4.2425e-03,  4.8081e-02, -1.0685e-01,\n",
      "          -6.3719e-02, -7.9062e-03, -8.6879e-02,  1.7153e-02, -7.5554e-03,\n",
      "           1.0597e-01, -1.4146e-02, -1.6061e-01, -6.3164e-02, -1.6057e-01,\n",
      "          -1.6666e-02, -2.5458e-02, -2.0534e-02,  5.5639e-02,  1.1449e-02,\n",
      "          -2.2423e-02,  7.2622e-02,  6.2916e-02, -3.2473e-02,  1.2354e-01,\n",
      "          -1.2230e-01, -9.8590e-03,  8.8923e-02,  6.4527e-02, -8.0939e-02,\n",
      "           7.0523e-02,  7.0320e-02,  4.5085e-03, -1.3365e-02,  1.2919e-02,\n",
      "          -4.9998e-02,  2.2644e-03, -9.4556e-02, -5.9249e-02],\n",
      "         [ 7.7285e-02,  1.5142e-01, -1.1445e-02, -2.7082e-02,  3.9227e-02,\n",
      "          -9.6223e-02,  1.3760e-03,  6.1716e-03, -3.5428e-02,  4.4529e-02,\n",
      "          -7.6847e-02, -6.5491e-02, -3.1667e-02, -4.9169e-03, -2.0168e-02,\n",
      "          -8.4785e-02,  1.6253e-01,  4.1933e-02, -5.1853e-02, -5.3739e-02,\n",
      "           4.6118e-02,  1.4497e-01, -3.7681e-02,  6.8519e-02,  9.7371e-02,\n",
      "          -3.3630e-02, -4.8726e-02,  1.1972e-02,  5.3428e-02, -9.4825e-02,\n",
      "          -6.7530e-02,  4.2626e-02, -3.0390e-02,  8.3272e-02,  2.2480e-02,\n",
      "           7.1975e-02,  1.9932e-02, -1.1326e-01, -8.4828e-02, -1.1968e-01,\n",
      "          -1.9309e-02, -2.3599e-02, -4.2722e-02,  4.8539e-02,  3.4218e-02,\n",
      "          -2.7580e-02,  7.3340e-02,  4.7277e-02, -2.5783e-02,  1.0916e-01,\n",
      "          -9.8089e-02,  3.8601e-02,  7.4718e-02,  6.0970e-02, -9.9213e-02,\n",
      "           7.6256e-02,  5.3567e-02,  3.3120e-02, -5.3786e-02,  2.8648e-02,\n",
      "           2.2072e-03,  1.9416e-02, -8.1800e-02, -1.6895e-02]],\n",
      "\n",
      "        [[-6.0899e-02, -1.3976e-01,  2.4072e-02, -1.0069e-02, -5.9405e-02,\n",
      "           9.0782e-02,  2.0404e-02,  3.4656e-02,  5.3847e-02, -4.9761e-03,\n",
      "           9.7675e-02,  8.6634e-02,  5.1537e-02, -2.3564e-03,  4.6700e-02,\n",
      "           5.5124e-02, -1.2512e-01, -5.1468e-02,  3.1893e-03,  3.3044e-02,\n",
      "          -6.4366e-02, -1.2723e-01,  1.3620e-01, -3.3587e-02, -3.3065e-02,\n",
      "           1.4163e-02,  5.2661e-02, -2.3801e-02, -7.4305e-02,  8.5678e-02,\n",
      "           3.3167e-02, -1.6080e-02,  1.0496e-01, -1.6642e-02, -1.1072e-02,\n",
      "          -3.9678e-02,  2.0430e-02,  1.5258e-01,  5.9704e-02,  7.4149e-02,\n",
      "           3.6310e-02,  7.9609e-02,  1.8858e-02,  2.4669e-02, -8.4454e-02,\n",
      "          -1.3680e-02, -1.4452e-01, -1.8276e-01,  1.7144e-02, -1.4259e-01,\n",
      "           9.4282e-02,  1.4550e-02, -4.7675e-02, -8.5653e-02,  1.6861e-01,\n",
      "          -5.7362e-02, -6.3192e-02,  5.0542e-02,  4.0803e-02,  2.5970e-02,\n",
      "          -8.6078e-04, -8.4525e-02,  8.3896e-02, -2.9683e-02],\n",
      "         [-4.9254e-02, -1.8561e-01,  4.8149e-02,  3.2651e-02, -4.3471e-02,\n",
      "           1.2978e-01, -4.9987e-03, -1.3737e-02,  1.2515e-01,  1.6795e-02,\n",
      "           1.3882e-01,  1.1310e-01,  7.6647e-02, -1.4400e-02,  5.5528e-02,\n",
      "           8.4671e-02, -1.4421e-01, -1.8336e-02,  1.1578e-02, -3.3752e-02,\n",
      "          -5.4258e-02, -1.0418e-01,  7.9136e-02, -1.6792e-03, -4.1940e-02,\n",
      "           2.5240e-02,  6.9279e-02, -2.7065e-02, -8.6164e-02,  8.4058e-02,\n",
      "          -1.0095e-02, -4.7756e-02,  1.0244e-01, -1.1637e-02, -3.3934e-02,\n",
      "          -3.9038e-02,  3.3537e-02,  1.6605e-01,  4.9595e-02,  1.0340e-01,\n",
      "           3.6102e-03,  9.4256e-02,  6.3327e-02,  3.0550e-02, -9.2627e-02,\n",
      "          -2.2202e-02, -1.1587e-01, -1.7378e-01,  1.1223e-02, -1.8035e-01,\n",
      "           1.3797e-01, -6.5102e-02, -1.3701e-02, -8.0635e-02,  1.5747e-01,\n",
      "          -5.2979e-02, -6.6707e-02,  4.9496e-02,  4.5587e-02,  3.6468e-02,\n",
      "          -1.9393e-02, -8.7620e-02,  3.6856e-02, -2.3775e-03],\n",
      "         [-3.7436e-02, -1.5599e-01,  2.4000e-02, -1.4340e-02, -8.1089e-02,\n",
      "           8.6679e-02, -2.0861e-02,  2.5418e-02,  6.3953e-02,  1.0865e-02,\n",
      "           1.0108e-01,  1.0630e-01,  3.1989e-02,  2.1756e-04,  3.8975e-02,\n",
      "           4.6964e-02, -1.1611e-01, -3.4479e-02,  4.6377e-03,  3.4353e-02,\n",
      "          -7.7857e-02, -1.2253e-01,  1.3504e-01, -2.4226e-02, -5.0469e-02,\n",
      "           2.4965e-02,  1.7230e-02, -2.8160e-02, -6.5148e-02,  8.3155e-02,\n",
      "           2.6890e-02, -2.2129e-02,  9.8936e-02, -3.7862e-03, -2.9047e-03,\n",
      "          -5.0956e-02,  2.3562e-02,  1.6574e-01,  7.2432e-02,  1.0834e-01,\n",
      "           3.8307e-02,  8.3850e-02,  2.4264e-02,  1.0731e-02, -8.0396e-02,\n",
      "          -2.3509e-03, -1.5210e-01, -1.6633e-01,  1.2506e-02, -1.5582e-01,\n",
      "           1.0102e-01,  1.5286e-02, -4.9377e-02, -7.1845e-02,  1.6860e-01,\n",
      "          -5.1333e-02, -4.6261e-02,  5.0977e-02,  5.3795e-02,  2.3263e-02,\n",
      "          -1.0422e-02, -8.3225e-02,  7.5197e-02, -1.3970e-02],\n",
      "         [-6.1045e-02, -1.4677e-01,  3.5096e-02,  6.5021e-03, -5.4399e-02,\n",
      "           9.5077e-02,  1.2610e-02,  3.2855e-02,  8.1023e-02,  8.7105e-04,\n",
      "           1.1261e-01,  1.0384e-01,  6.2304e-02,  2.4824e-03,  4.3587e-02,\n",
      "           7.5813e-02, -1.2951e-01, -3.6128e-02,  1.3524e-02,  1.8715e-02,\n",
      "          -6.3679e-02, -1.1781e-01,  1.1424e-01, -3.0919e-02, -2.5691e-02,\n",
      "           1.2883e-02,  7.3341e-02, -2.7105e-02, -7.5382e-02,  8.9215e-02,\n",
      "           2.2348e-02, -2.7780e-02,  1.0335e-01, -1.1456e-02, -2.8546e-02,\n",
      "          -4.2148e-02,  2.4397e-02,  1.5507e-01,  5.3518e-02,  8.1391e-02,\n",
      "           2.1826e-02,  8.7339e-02,  2.3792e-02,  2.6383e-02, -8.5340e-02,\n",
      "          -1.7883e-02, -1.4455e-01, -1.9031e-01,  1.8109e-02, -1.4500e-01,\n",
      "           1.0662e-01, -4.0487e-05, -3.9439e-02, -9.0872e-02,  1.6676e-01,\n",
      "          -5.4603e-02, -5.4785e-02,  6.6995e-02,  4.8286e-02,  3.9274e-02,\n",
      "          -1.6555e-03, -8.7921e-02,  7.0614e-02, -1.3394e-02],\n",
      "         [-1.3968e-02, -1.3889e-01,  1.3620e-02, -2.9195e-02, -1.1227e-01,\n",
      "           6.5767e-02, -5.5971e-02,  4.3277e-02,  5.0489e-02,  2.5547e-02,\n",
      "           7.6325e-02,  1.2278e-01,  4.3975e-03,  1.7983e-02,  6.6060e-03,\n",
      "           4.0517e-02, -8.2217e-02, -1.3041e-02,  1.4554e-02,  6.7637e-02,\n",
      "          -1.0162e-01, -1.1607e-01,  1.4184e-01, -3.5422e-02, -4.8818e-02,\n",
      "           3.8630e-02, -7.0562e-03, -3.8494e-02, -4.3332e-02,  7.4010e-02,\n",
      "           3.4484e-02, -1.2120e-02,  8.7556e-02,  1.6009e-02,  8.9429e-03,\n",
      "          -7.7259e-02,  2.9696e-02,  1.7498e-01,  8.3988e-02,  1.1722e-01,\n",
      "           5.2137e-02,  8.4224e-02,  5.7834e-04,  7.0754e-03, -5.7077e-02,\n",
      "           5.5068e-03, -1.7712e-01, -1.6207e-01,  1.7369e-02, -1.4629e-01,\n",
      "           8.9045e-02,  5.6706e-02, -6.9023e-02, -6.7887e-02,  1.6125e-01,\n",
      "          -4.7203e-02, -1.9588e-02,  6.8998e-02,  6.4836e-02,  2.1040e-02,\n",
      "           1.1429e-02, -7.9109e-02,  8.0199e-02,  2.3400e-02]],\n",
      "\n",
      "        [[ 7.5901e-02,  8.1830e-02,  6.7024e-02, -3.3669e-02,  1.1938e-01,\n",
      "          -8.5237e-02, -6.7747e-02,  6.2408e-02, -2.0230e-01,  1.1299e-01,\n",
      "           5.5422e-02,  9.1955e-02, -2.2036e-01, -6.1660e-02,  8.8418e-02,\n",
      "           4.5672e-02, -4.7829e-02,  1.3707e-03, -3.4551e-02, -1.3766e-03,\n",
      "           2.6475e-02,  2.6215e-02, -1.6796e-01,  1.3636e-02, -2.3113e-02,\n",
      "          -6.7115e-02, -7.3877e-02,  5.6907e-02, -6.6139e-02,  5.4220e-02,\n",
      "          -9.1203e-02, -1.7444e-02, -2.2450e-01, -1.3207e-01, -2.2971e-01,\n",
      "           2.1869e-01, -4.3668e-02, -6.1742e-02,  5.4154e-02,  7.2657e-02,\n",
      "           5.1835e-02, -2.1749e-02, -6.2245e-02, -8.8484e-02,  8.0446e-02,\n",
      "           4.8445e-02, -2.9747e-02,  2.9503e-02,  4.0497e-02,  6.9628e-02,\n",
      "           8.2603e-02,  1.4924e-01, -2.3341e-02,  3.2944e-02,  1.5827e-01,\n",
      "          -6.0345e-02,  1.8895e-01,  1.1492e-01,  8.5483e-02,  9.2100e-02,\n",
      "          -1.0859e-01,  1.1652e-01,  6.8353e-02, -8.2331e-02],\n",
      "         [ 9.3392e-02,  7.7868e-02,  5.8457e-02, -4.1797e-02,  1.3302e-01,\n",
      "          -1.1005e-01, -6.6480e-02,  6.0831e-02, -2.1090e-01,  1.1286e-01,\n",
      "           4.3476e-02,  9.9468e-02, -2.0650e-01, -5.2814e-02,  1.0617e-01,\n",
      "           1.1207e-02, -2.2809e-02, -1.5702e-02, -4.4351e-02, -4.3949e-03,\n",
      "           2.8112e-02,  1.7978e-02, -1.2357e-01,  2.0977e-02, -5.4379e-02,\n",
      "          -5.1355e-02, -7.4224e-02,  6.0159e-02, -5.5130e-02,  6.4898e-02,\n",
      "          -9.2277e-02, -2.2273e-03, -1.7997e-01, -9.3037e-02, -1.9260e-01,\n",
      "           2.0794e-01, -3.0811e-02, -4.1428e-02,  3.2456e-02,  9.2103e-02,\n",
      "           6.5022e-02, -3.8148e-02, -7.3632e-02, -1.0242e-01,  6.3583e-02,\n",
      "           4.2497e-02, -2.2865e-02,  2.5867e-02,  4.1936e-02,  4.8898e-02,\n",
      "           7.7478e-02,  1.6804e-01, -2.6379e-02,  2.9819e-02,  1.3962e-01,\n",
      "          -7.3398e-02,  1.5887e-01,  1.0321e-01,  6.3847e-02,  1.0801e-01,\n",
      "          -1.1816e-01,  1.1512e-01,  6.8127e-02, -9.4498e-02],\n",
      "         [ 1.0372e-01,  8.4929e-02,  5.8704e-02, -4.2880e-02,  1.4988e-01,\n",
      "          -1.0644e-01, -5.4528e-02,  6.4098e-02, -2.1031e-01,  1.2271e-01,\n",
      "           2.5717e-02,  8.9841e-02, -2.1477e-01, -5.2682e-02,  1.1723e-01,\n",
      "           1.9945e-02, -1.8112e-02,  1.2519e-03, -4.6712e-02,  1.0211e-03,\n",
      "           1.0236e-02,  2.5270e-02, -1.2547e-01,  2.7181e-02, -4.6968e-02,\n",
      "          -5.1610e-02, -6.5705e-02,  6.4262e-02, -7.1953e-02,  6.6453e-02,\n",
      "          -1.0328e-01,  2.9832e-05, -1.8212e-01, -9.3031e-02, -2.0031e-01,\n",
      "           2.0093e-01, -3.1635e-02, -6.0208e-02,  3.8434e-02,  8.3151e-02,\n",
      "           6.9554e-02, -2.7512e-02, -7.3169e-02, -8.3989e-02,  6.1726e-02,\n",
      "           3.9345e-02, -2.1144e-02,  3.3041e-02,  4.3308e-02,  5.0712e-02,\n",
      "           7.1849e-02,  1.6618e-01, -2.3904e-02,  2.0101e-02,  1.3832e-01,\n",
      "          -6.0701e-02,  1.5912e-01,  1.0625e-01,  6.8961e-02,  1.0494e-01,\n",
      "          -9.9037e-02,  1.1837e-01,  6.7598e-02, -8.8783e-02],\n",
      "         [ 1.2122e-01,  8.7846e-02,  6.5398e-02, -4.7239e-02,  1.6670e-01,\n",
      "          -1.1602e-01, -5.5458e-02,  6.6022e-02, -2.1084e-01,  1.2975e-01,\n",
      "           6.5794e-03,  8.9375e-02, -2.2028e-01, -4.3874e-02,  1.2373e-01,\n",
      "           1.5401e-02, -6.7172e-03,  4.7412e-03, -4.7471e-02,  3.3195e-03,\n",
      "           1.4939e-03,  2.8167e-02, -1.1702e-01,  3.7146e-02, -4.7466e-02,\n",
      "          -4.8068e-02, -6.5330e-02,  6.4698e-02, -8.0403e-02,  6.8071e-02,\n",
      "          -1.1181e-01,  1.3606e-02, -1.7152e-01, -7.8120e-02, -1.9684e-01,\n",
      "           1.9896e-01, -2.7147e-02, -6.6628e-02,  3.1748e-02,  8.3783e-02,\n",
      "           7.3770e-02, -3.1320e-02, -7.3672e-02, -8.2807e-02,  5.9388e-02,\n",
      "           3.2898e-02, -1.3868e-02,  3.1785e-02,  4.1547e-02,  4.7822e-02,\n",
      "           6.9092e-02,  1.7464e-01, -1.8368e-02,  1.4330e-02,  1.2519e-01,\n",
      "          -5.7638e-02,  1.6320e-01,  1.1355e-01,  6.2348e-02,  1.0779e-01,\n",
      "          -9.0074e-02,  1.2628e-01,  7.2116e-02, -8.0544e-02],\n",
      "         [ 1.0729e-01,  8.9395e-02,  6.2722e-02, -4.2425e-02,  1.5491e-01,\n",
      "          -1.0061e-01, -5.3901e-02,  6.5056e-02, -2.1067e-01,  1.2973e-01,\n",
      "           1.9630e-02,  8.5620e-02, -2.2791e-01, -5.2967e-02,  1.1454e-01,\n",
      "           3.2562e-02, -2.0612e-02,  1.5317e-02, -4.5748e-02,  4.8754e-03,\n",
      "           2.6829e-03,  3.0880e-02, -1.4243e-01,  3.1348e-02, -3.6617e-02,\n",
      "          -5.6145e-02, -6.7202e-02,  6.3957e-02, -8.3064e-02,  6.2812e-02,\n",
      "          -1.0909e-01, -2.3811e-03, -1.9834e-01, -1.0408e-01, -2.1589e-01,\n",
      "           2.0338e-01, -3.5869e-02, -7.5834e-02,  4.5431e-02,  7.5349e-02,\n",
      "           7.1343e-02, -1.9140e-02, -6.9257e-02, -7.4245e-02,  6.5830e-02,\n",
      "           4.0620e-02, -2.0986e-02,  3.9329e-02,  4.4307e-02,  5.8660e-02,\n",
      "           7.0908e-02,  1.6310e-01, -2.1492e-02,  1.7550e-02,  1.3973e-01,\n",
      "          -5.3529e-02,  1.7240e-01,  1.1286e-01,  7.8049e-02,  9.9853e-02,\n",
      "          -8.8238e-02,  1.2189e-01,  6.8095e-02, -7.9968e-02]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 与传统多头注意力无差别\n",
    "xq = xq.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "xk = xk.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "xv = xv.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "\n",
    "query = xq.transpose(1,2)\n",
    "key = xk.transpose(1,2) # bs, n_heads, [seq_len, head_dim]\n",
    "value = xv.transpose(1,2)\n",
    "print(query.shape) \n",
    "print(key.shape)\n",
    "\n",
    "scores = query @ key.transpose(2,3) / math.sqrt(mla.head_dim) # keys: bs, n_heads, [head_dim, seq_len]\n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "output = scores @ value \n",
    "output = output.transpose(1, 2).contiguous().view(bs, seq_len, -1)\n",
    "output = mla.wo(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c075a3-6f4d-4c25-8cd9-392be916ff4d",
   "metadata": {},
   "source": [
    "### 矩阵吸收\n",
    "\n",
    "以上参数发生在训练之时，训练完成后，我们可以做吸收操作，具体指wq参数和Wo参数，\n",
    "\n",
    "我们写出如下等式：\n",
    "\n",
    "Q = wq_up * ( wq_down * h ) = (wq_up * wq_down) * h\n",
    "\n",
    "Q = wq * h \n",
    "\n",
    "目的是什么？\n",
    "\n",
    "1. 训练时省显存\n",
    "2. 训练完，推理Q满矩阵保精度，\n",
    "3. 由于KV Cache的存在，decoding阶段时one-by-one token进行q计算\n",
    "4. KV Cache极具减少存储体现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2485230f-5994-4023-afab-6cba4e8b0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq = mla.wq_up.weight.data @ mla.wq_down.weight.data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf785cf2-4dc8-42a0-a2bb-381f60107bd1",
   "metadata": {},
   "source": [
    "### 矩阵吸收后的forward(非训练阶段）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a76ddd-55b8-4589-95f1-c7a66d17e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n",
      "tensor([[[ 2.0484e-02,  1.5552e-01, -1.5591e-02,  1.4401e-03,  3.6842e-02,\n",
      "          -8.2618e-02,  2.8318e-02, -2.0483e-02, -1.3989e-02, -8.7396e-03,\n",
      "          -2.2675e-02, -8.3285e-02, -7.2652e-03, -3.4485e-02, -6.9594e-03,\n",
      "          -9.4004e-02,  1.0591e-01, -1.8713e-02, -2.8423e-02, -7.4676e-02,\n",
      "           1.0796e-01,  1.0757e-01, -4.3560e-02,  7.3838e-02,  8.9302e-02,\n",
      "          -4.2553e-02, -7.6589e-02,  1.4797e-02,  7.5575e-02, -1.0200e-01,\n",
      "          -5.0200e-02,  1.1170e-02, -4.3665e-02,  3.0674e-02,  1.0298e-02,\n",
      "           1.0395e-01, -1.8623e-02, -1.2574e-01, -1.0559e-01, -1.4850e-01,\n",
      "          -4.0997e-02, -2.9282e-02, -8.6471e-03,  4.6638e-02,  1.8475e-02,\n",
      "          -1.7276e-02,  9.6743e-02,  7.0292e-02, -3.9667e-02,  1.1571e-01,\n",
      "          -1.1643e-01, -2.2672e-02,  8.6867e-02,  7.5407e-02, -1.0060e-01,\n",
      "           5.2397e-02,  4.3059e-02, -1.0472e-02, -4.3900e-02,  1.2820e-02,\n",
      "          -5.1049e-02,  5.7975e-03, -8.0848e-02, -6.8384e-02],\n",
      "         [ 8.4235e-02,  1.5779e-01, -1.3211e-02, -2.4927e-02,  5.5507e-02,\n",
      "          -7.7465e-02, -1.2400e-02, -6.8488e-03, -7.3190e-02,  6.9814e-02,\n",
      "          -5.2300e-02, -4.6548e-02, -9.2325e-02, -2.5580e-02, -1.2114e-02,\n",
      "          -8.2927e-02,  1.3914e-01,  4.1588e-02, -4.3859e-02, -5.4378e-02,\n",
      "           3.5213e-02,  1.4147e-01, -6.9640e-02,  9.1738e-02,  1.0061e-01,\n",
      "          -4.4304e-02, -9.8322e-02,  1.2102e-02,  1.9940e-02, -1.1242e-01,\n",
      "          -8.9104e-02,  1.8266e-02, -8.4839e-02,  4.0923e-02, -1.4117e-02,\n",
      "           1.1213e-01,  5.9006e-03, -1.5074e-01, -4.9141e-02, -1.1979e-01,\n",
      "           7.4298e-04, -1.8632e-02, -3.0120e-02,  5.2971e-02,  3.2904e-02,\n",
      "          -1.4720e-02,  7.3940e-02,  5.6872e-02, -3.0511e-02,  1.0742e-01,\n",
      "          -9.7693e-02,  3.8393e-02,  8.4021e-02,  5.5640e-02, -5.4769e-02,\n",
      "           8.7415e-02,  9.6917e-02,  4.4013e-02, -8.3068e-03,  2.1695e-02,\n",
      "          -2.8894e-02,  3.1798e-02, -8.4261e-02, -2.9775e-02],\n",
      "         [ 6.8386e-02,  1.4239e-01, -2.0056e-02, -1.8373e-02,  3.5503e-02,\n",
      "          -7.2365e-02, -4.2336e-03, -1.9742e-02, -4.5733e-02,  6.2000e-02,\n",
      "          -7.6326e-02, -5.3404e-02, -3.3354e-02, -2.7335e-02, -4.1935e-02,\n",
      "          -1.1661e-01,  1.4284e-01,  3.0114e-02, -3.3813e-02, -6.1194e-02,\n",
      "           4.6100e-02,  1.3932e-01, -6.2858e-02,  7.3393e-02,  7.9897e-02,\n",
      "          -1.3410e-02, -8.2629e-02,  1.8874e-02,  3.6793e-02, -1.1104e-01,\n",
      "          -1.0392e-01,  1.0566e-02, -7.6134e-02,  6.7348e-02,  3.0604e-02,\n",
      "           6.7858e-02,  1.8806e-02, -1.1806e-01, -6.3098e-02, -1.2314e-01,\n",
      "          -1.6782e-02, -2.9739e-02, -2.0714e-02,  6.4312e-02,  3.3932e-02,\n",
      "          -3.0646e-02,  7.6381e-02,  5.8680e-02, -3.3464e-02,  9.0042e-02,\n",
      "          -9.6543e-02, -4.5555e-03,  9.5253e-02,  5.3635e-02, -9.1807e-02,\n",
      "           6.3151e-02,  6.0310e-02,  1.3548e-02, -4.9381e-02,  1.8749e-02,\n",
      "          -9.0213e-03,  1.8689e-02, -7.9587e-02, -1.2620e-02],\n",
      "         [ 3.0300e-02,  1.4661e-01, -1.7168e-02, -3.1650e-03,  3.3634e-02,\n",
      "          -7.9656e-02,  2.1652e-02, -1.9017e-02, -1.7961e-02,  1.4651e-02,\n",
      "          -4.6725e-02, -7.3101e-02, -2.9012e-03, -3.0634e-02, -2.4183e-02,\n",
      "          -9.9372e-02,  1.2006e-01,  1.4617e-04, -2.9346e-02, -6.7800e-02,\n",
      "           8.8837e-02,  1.1374e-01, -5.1330e-02,  7.0935e-02,  8.4416e-02,\n",
      "          -2.9868e-02, -6.7380e-02,  1.7984e-02,  6.1909e-02, -1.0062e-01,\n",
      "          -7.0401e-02,  9.4498e-03, -5.7053e-02,  4.4941e-02,  2.0008e-02,\n",
      "           8.2283e-02, -2.9548e-03, -1.1826e-01, -9.3729e-02, -1.4313e-01,\n",
      "          -3.2710e-02, -3.1710e-02, -1.1715e-02,  5.3330e-02,  1.9551e-02,\n",
      "          -2.5664e-02,  8.8384e-02,  6.6511e-02, -3.4880e-02,  1.0668e-01,\n",
      "          -1.0757e-01, -2.2454e-02,  9.0728e-02,  6.6103e-02, -1.0906e-01,\n",
      "           4.6906e-02,  4.2062e-02, -7.1178e-03, -5.4936e-02,  1.6329e-02,\n",
      "          -3.2785e-02,  7.5523e-03, -7.7776e-02, -4.3208e-02],\n",
      "         [ 1.1326e-01,  1.5906e-01, -1.3910e-02, -3.9493e-02,  6.2661e-02,\n",
      "          -6.8303e-02, -2.2176e-02, -6.2338e-03, -1.0448e-01,  1.0509e-01,\n",
      "          -7.9032e-02, -3.8449e-02, -1.3053e-01, -1.7904e-02, -2.3048e-02,\n",
      "          -7.9734e-02,  1.6061e-01,  7.7025e-02, -4.9461e-02, -4.0642e-02,\n",
      "          -2.2020e-03,  1.6426e-01, -8.3557e-02,  1.0006e-01,  1.1535e-01,\n",
      "          -4.4535e-02, -1.0664e-01,  6.9018e-03, -9.7022e-03, -1.2549e-01,\n",
      "          -1.0675e-01,  2.9778e-02, -1.0278e-01,  4.4383e-02, -1.5210e-02,\n",
      "           1.0368e-01,  2.2859e-02, -1.7028e-01, -1.9097e-02, -1.2157e-01,\n",
      "           2.2652e-02, -1.5697e-02, -4.0248e-02,  6.6112e-02,  4.7728e-02,\n",
      "          -2.0854e-02,  6.5819e-02,  4.7525e-02, -2.8198e-02,  1.0453e-01,\n",
      "          -9.5878e-02,  6.3197e-02,  8.6351e-02,  4.2030e-02, -3.8757e-02,\n",
      "           1.1061e-01,  1.1796e-01,  6.6306e-02,  9.2885e-04,  1.1778e-02,\n",
      "          -3.5845e-03,  4.1162e-02, -9.0276e-02, -6.5886e-04]],\n",
      "\n",
      "        [[-5.4726e-02, -1.4302e-01,  1.8632e-02, -6.0136e-03, -6.7782e-02,\n",
      "           9.2156e-02,  1.7568e-02,  2.7721e-02,  5.7489e-02,  5.8647e-03,\n",
      "           6.9991e-02,  8.7888e-02,  5.6208e-02,  1.4847e-03,  3.0921e-02,\n",
      "           5.7767e-02, -1.0098e-01, -3.8199e-02,  1.1241e-02,  3.8645e-02,\n",
      "          -7.9197e-02, -1.2281e-01,  1.2706e-01, -3.6905e-02, -3.3518e-02,\n",
      "           1.9972e-02,  4.5934e-02, -2.8919e-02, -7.7182e-02,  7.4390e-02,\n",
      "           2.7360e-02, -1.3271e-02,  8.4850e-02, -6.8713e-03,  2.9716e-03,\n",
      "          -5.9207e-02,  3.6449e-02,  1.5341e-01,  7.7962e-02,  6.4624e-02,\n",
      "           4.4955e-02,  6.5202e-02,  6.1800e-03,  2.7485e-02, -7.7194e-02,\n",
      "          -2.9540e-02, -1.4440e-01, -1.8055e-01,  1.8663e-02, -1.3996e-01,\n",
      "           8.9551e-02,  2.1475e-02, -5.5533e-02, -9.5419e-02,  1.4708e-01,\n",
      "          -6.7663e-02, -6.5674e-02,  4.6990e-02,  3.2311e-02,  1.4150e-02,\n",
      "           1.7141e-02, -8.4235e-02,  8.4419e-02, -7.1235e-04],\n",
      "         [-5.2018e-02, -1.4131e-01,  2.8573e-02, -1.0454e-02, -6.1318e-02,\n",
      "           9.1255e-02,  2.9405e-03,  3.0660e-02,  6.1483e-02, -2.7757e-03,\n",
      "           1.1164e-01,  9.4796e-02,  4.1381e-02, -1.0680e-03,  4.7227e-02,\n",
      "           5.6541e-02, -1.3016e-01, -4.6261e-02,  5.9873e-03,  2.7884e-02,\n",
      "          -6.3914e-02, -1.2085e-01,  1.3293e-01, -2.9581e-02, -3.3912e-02,\n",
      "           1.7612e-02,  4.4737e-02, -2.8897e-02, -6.7099e-02,  8.4825e-02,\n",
      "           3.4035e-02, -1.7240e-02,  1.1329e-01, -1.4850e-02, -1.6753e-02,\n",
      "          -3.6384e-02,  1.5918e-02,  1.5729e-01,  5.4134e-02,  8.8528e-02,\n",
      "           2.9741e-02,  8.8163e-02,  2.5539e-02,  2.2148e-02, -7.8826e-02,\n",
      "          -4.5457e-03, -1.4388e-01, -1.8252e-01,  1.3306e-02, -1.4689e-01,\n",
      "           9.9868e-02,  1.2227e-02, -4.5724e-02, -8.0276e-02,  1.7713e-01,\n",
      "          -4.8064e-02, -5.2705e-02,  5.9594e-02,  5.0969e-02,  2.9650e-02,\n",
      "          -5.3750e-03, -8.4352e-02,  7.7446e-02, -2.7155e-02],\n",
      "         [-7.4739e-02, -1.5829e-01,  1.7245e-02,  6.2770e-03, -6.3287e-02,\n",
      "           1.0384e-01,  2.5933e-02,  3.4264e-02,  8.2861e-02,  4.9559e-03,\n",
      "           7.3081e-02,  8.4994e-02,  1.0799e-01, -1.4399e-02,  3.1836e-02,\n",
      "           4.4392e-02, -1.1815e-01, -6.2371e-02,  1.8582e-03,  1.0904e-02,\n",
      "          -5.4342e-02, -1.3058e-01,  1.2702e-01, -4.9747e-02, -6.5531e-02,\n",
      "           2.6306e-02,  7.3485e-02, -1.6781e-02, -6.5097e-02,  1.0226e-01,\n",
      "           3.3711e-03, -2.9544e-02,  9.8758e-02,  1.0043e-03,  2.0630e-03,\n",
      "          -7.1056e-02,  3.2420e-02,  1.6961e-01,  6.5496e-02,  9.1307e-02,\n",
      "           3.4539e-02,  6.2374e-02,  2.0305e-02,  1.4585e-02, -9.7833e-02,\n",
      "          -2.6702e-02, -1.3660e-01, -1.7161e-01,  2.3295e-02, -1.5571e-01,\n",
      "           1.1020e-01, -2.7383e-02, -3.6685e-02, -7.5448e-02,  1.3363e-01,\n",
      "          -8.2940e-02, -9.4564e-02,  1.6578e-02,  1.5094e-02,  3.5191e-02,\n",
      "          -9.2940e-04, -7.4801e-02,  8.6099e-02, -1.8302e-02],\n",
      "         [-6.0414e-02, -1.5757e-01,  1.9591e-02,  1.6472e-03, -6.8282e-02,\n",
      "           9.6163e-02,  1.6769e-02,  4.3835e-02,  7.8686e-02,  1.3168e-02,\n",
      "           7.3835e-02,  9.2604e-02,  8.4360e-02, -5.7298e-03,  4.0435e-02,\n",
      "           6.1356e-02, -1.0949e-01, -4.4063e-02, -2.1230e-03,  2.4418e-02,\n",
      "          -7.0590e-02, -1.3662e-01,  1.3363e-01, -3.9480e-02, -5.3451e-02,\n",
      "           1.8358e-02,  6.8841e-02, -1.4260e-02, -7.7876e-02,  9.8319e-02,\n",
      "           1.1583e-02, -2.4901e-02,  8.9050e-02,  6.4251e-04, -6.0625e-03,\n",
      "          -6.2337e-02,  3.4889e-02,  1.6666e-01,  7.2230e-02,  9.2124e-02,\n",
      "           4.0752e-02,  7.0273e-02,  1.7234e-02,  1.3637e-02, -1.0443e-01,\n",
      "          -2.3116e-02, -1.4779e-01, -1.6950e-01,  2.5328e-02, -1.4941e-01,\n",
      "           1.0806e-01, -3.8437e-03, -4.4718e-02, -7.7334e-02,  1.3839e-01,\n",
      "          -7.5619e-02, -7.9534e-02,  3.0700e-02,  2.7134e-02,  3.6536e-02,\n",
      "          -1.8146e-03, -7.8566e-02,  8.7270e-02, -1.3906e-02],\n",
      "         [-5.4327e-02, -1.5572e-01,  1.7254e-02, -1.0257e-03, -6.7710e-02,\n",
      "           9.5877e-02,  1.2628e-02,  2.8836e-02,  6.8652e-02,  1.5908e-02,\n",
      "           6.5922e-02,  9.1842e-02,  7.4202e-02, -2.6098e-03,  2.8634e-02,\n",
      "           4.6211e-02, -1.0302e-01, -4.2692e-02,  8.6355e-03,  2.6421e-02,\n",
      "          -7.6533e-02, -1.2294e-01,  1.2462e-01, -4.0436e-02, -5.3471e-02,\n",
      "           3.0475e-02,  5.0996e-02, -2.1523e-02, -7.6315e-02,  8.4456e-02,\n",
      "           9.7559e-03, -2.3744e-02,  8.3372e-02,  5.4270e-03,  9.8148e-03,\n",
      "          -6.6438e-02,  3.9950e-02,  1.6518e-01,  8.0210e-02,  8.5372e-02,\n",
      "           4.1552e-02,  6.1350e-02,  1.1395e-02,  2.0938e-02, -8.6958e-02,\n",
      "          -3.0632e-02, -1.4753e-01, -1.7473e-01,  2.0379e-02, -1.5171e-01,\n",
      "           1.0367e-01, -1.4862e-03, -4.9715e-02, -8.7288e-02,  1.4072e-01,\n",
      "          -7.5036e-02, -7.7782e-02,  3.2200e-02,  2.5826e-02,  2.6489e-02,\n",
      "           1.0676e-02, -7.8510e-02,  8.4312e-02, -3.7647e-03]],\n",
      "\n",
      "        [[ 7.7181e-02,  8.1357e-02,  5.2365e-02, -3.5014e-02,  1.2601e-01,\n",
      "          -1.0954e-01, -5.4349e-02,  5.8381e-02, -2.0844e-01,  9.9057e-02,\n",
      "           6.6035e-02,  9.4534e-02, -1.9291e-01, -6.3358e-02,  1.1048e-01,\n",
      "           1.4451e-02, -3.2582e-02, -2.4951e-02, -4.3627e-02, -1.1990e-02,\n",
      "           3.6691e-02,  2.3991e-02, -1.2541e-01,  4.9658e-03, -5.3570e-02,\n",
      "          -5.6187e-02, -6.4333e-02,  6.4287e-02, -4.3724e-02,  6.8350e-02,\n",
      "          -8.2559e-02, -1.1473e-02, -1.7992e-01, -9.9675e-02, -1.9236e-01,\n",
      "           2.0897e-01, -3.5161e-02, -3.3658e-02,  3.4977e-02,  8.9993e-02,\n",
      "           5.1992e-02, -3.9076e-02, -8.3174e-02, -9.8777e-02,  7.0607e-02,\n",
      "           4.7823e-02, -2.8143e-02,  2.1934e-02,  4.7229e-02,  5.2401e-02,\n",
      "           8.0088e-02,  1.6493e-01, -3.3418e-02,  3.0697e-02,  1.5835e-01,\n",
      "          -6.7100e-02,  1.4850e-01,  1.0037e-01,  6.7537e-02,  1.1215e-01,\n",
      "          -1.2251e-01,  1.1030e-01,  5.7787e-02, -1.1554e-01],\n",
      "         [ 7.7048e-02,  8.4060e-02,  6.2940e-02, -3.3584e-02,  1.1419e-01,\n",
      "          -1.1538e-01, -5.1407e-02,  6.9709e-02, -1.9929e-01,  1.0164e-01,\n",
      "           4.6757e-02,  9.5898e-02, -1.8794e-01, -5.2338e-02,  1.0569e-01,\n",
      "           2.9120e-02, -2.5182e-02, -1.5619e-02, -4.5947e-02, -8.3463e-04,\n",
      "           4.2319e-02,  2.9991e-03, -1.0965e-01,  1.7560e-02, -5.0060e-02,\n",
      "          -6.0051e-02, -5.6188e-02,  7.4391e-02, -4.1325e-02,  8.0094e-02,\n",
      "          -7.9023e-02, -5.6381e-04, -1.7662e-01, -1.0614e-01, -1.9371e-01,\n",
      "           1.9856e-01, -3.4984e-02, -2.8419e-02,  1.8492e-02,  7.7114e-02,\n",
      "           6.3730e-02, -3.2021e-02, -7.4486e-02, -9.9175e-02,  6.0348e-02,\n",
      "           3.9330e-02, -3.3364e-02,  2.7734e-02,  5.2048e-02,  5.6505e-02,\n",
      "           7.7928e-02,  1.6774e-01, -2.9993e-02,  4.4086e-02,  1.3037e-01,\n",
      "          -9.0215e-02,  1.4542e-01,  8.7045e-02,  5.2409e-02,  1.0385e-01,\n",
      "          -1.3016e-01,  1.1414e-01,  6.6805e-02, -9.5229e-02],\n",
      "         [ 9.0719e-02,  7.7242e-02,  5.3568e-02, -4.1084e-02,  1.4490e-01,\n",
      "          -1.0188e-01, -5.8495e-02,  5.7928e-02, -2.0779e-01,  1.1003e-01,\n",
      "           4.7484e-02,  9.2143e-02, -2.0428e-01, -6.3389e-02,  1.1488e-01,\n",
      "           7.2986e-03, -3.1830e-02, -2.0078e-02, -4.1166e-02, -9.2786e-03,\n",
      "           1.2767e-02,  3.3556e-02, -1.3036e-01,  1.0288e-02, -5.2505e-02,\n",
      "          -4.5965e-02, -6.8173e-02,  5.7428e-02, -6.1663e-02,  6.1735e-02,\n",
      "          -9.8804e-02, -1.0273e-02, -1.7782e-01, -8.8195e-02, -1.9229e-01,\n",
      "           2.0572e-01, -3.2140e-02, -4.8145e-02,  4.9374e-02,  9.3402e-02,\n",
      "           5.1704e-02, -3.4187e-02, -7.6981e-02, -8.8898e-02,  6.9902e-02,\n",
      "           4.3377e-02, -2.1200e-02,  2.2442e-02,  3.6882e-02,  4.3566e-02,\n",
      "           7.7409e-02,  1.5705e-01, -2.5746e-02,  1.7221e-02,  1.6040e-01,\n",
      "          -5.0119e-02,  1.5601e-01,  1.1015e-01,  7.4177e-02,  1.1150e-01,\n",
      "          -1.0245e-01,  1.1545e-01,  6.4120e-02, -1.0858e-01],\n",
      "         [ 8.5188e-02,  8.7788e-02,  5.3366e-02, -3.5501e-02,  1.3099e-01,\n",
      "          -1.0391e-01, -5.1853e-02,  6.2960e-02, -2.0848e-01,  1.0867e-01,\n",
      "           5.3194e-02,  9.0317e-02, -2.0307e-01, -6.2211e-02,  1.1243e-01,\n",
      "           2.4383e-02, -2.9622e-02, -1.2650e-02, -4.4136e-02, -5.7797e-03,\n",
      "           2.8852e-02,  2.4155e-02, -1.2780e-01,  1.2072e-02, -4.3815e-02,\n",
      "          -5.8779e-02, -6.3976e-02,  6.4923e-02, -5.5036e-02,  6.8111e-02,\n",
      "          -8.6636e-02, -8.4258e-03, -1.8616e-01, -1.0738e-01, -2.0385e-01,\n",
      "           2.0830e-01, -3.5710e-02, -4.7619e-02,  3.7156e-02,  8.5506e-02,\n",
      "           5.9562e-02, -3.2814e-02, -8.1225e-02, -9.4648e-02,  6.8634e-02,\n",
      "           4.6963e-02, -2.6210e-02,  2.4678e-02,  4.7843e-02,  5.7450e-02,\n",
      "           7.8240e-02,  1.6511e-01, -3.2991e-02,  2.9851e-02,  1.5723e-01,\n",
      "          -6.4337e-02,  1.5351e-01,  1.0458e-01,  7.2407e-02,  1.0649e-01,\n",
      "          -1.1574e-01,  1.1390e-01,  5.9159e-02, -1.0423e-01],\n",
      "         [ 9.4848e-02,  7.9951e-02,  5.1240e-02, -4.2017e-02,  1.4496e-01,\n",
      "          -9.5905e-02, -5.4225e-02,  6.0561e-02, -2.0991e-01,  1.1696e-01,\n",
      "           4.0493e-02,  8.9161e-02, -2.0844e-01, -5.8912e-02,  1.1762e-01,\n",
      "           1.4668e-02, -2.6306e-02, -7.6309e-03, -4.2716e-02, -4.6663e-03,\n",
      "           1.0921e-02,  3.0537e-02, -1.2632e-01,  1.5975e-02, -4.3583e-02,\n",
      "          -5.1622e-02, -6.4522e-02,  5.9169e-02, -7.0854e-02,  5.8529e-02,\n",
      "          -9.8636e-02, -6.5368e-03, -1.8260e-01, -9.3052e-02, -1.9633e-01,\n",
      "           2.0582e-01, -3.0207e-02, -5.7954e-02,  4.7988e-02,  8.8974e-02,\n",
      "           5.9169e-02, -3.2316e-02, -7.6298e-02, -8.8345e-02,  6.5952e-02,\n",
      "           4.2153e-02, -2.1809e-02,  2.4002e-02,  3.7226e-02,  4.7906e-02,\n",
      "           7.3598e-02,  1.5924e-01, -2.6969e-02,  1.5284e-02,  1.5647e-01,\n",
      "          -5.1783e-02,  1.5572e-01,  1.1002e-01,  7.4417e-02,  1.0447e-01,\n",
      "          -1.0253e-01,  1.1085e-01,  6.3699e-02, -9.8158e-02]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 该段代码是关键，先降维，再升维\n",
    "xq =  h @ wq\n",
    "# c_q = mla.wq_down(h) #去除\n",
    "# xq = mla.wq_up(c_q) #去除\n",
    "\n",
    "c_kv = mla.wkv_down(h)\n",
    "xk = mla.wk_up(c_kv)\n",
    "# xv = mla.wv_up(c_kv) # 去除\n",
    "\n",
    "# 与传统多头注意力无差别\n",
    "xq = xq.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "xk = xk.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "xv = xv.view(bs, seq_len, mla.n_heads, mla.head_dim)\n",
    "\n",
    "query = xq.transpose(1,2)\n",
    "key = xk.transpose(1,2) # bs, n_heads, [seq_len, head_dim]\n",
    "value = xv.transpose(1,2)\n",
    "print(query.shape) \n",
    "print(key.shape)\n",
    "\n",
    "# keys: bs, n_heads, [head_dim, seq_len]\n",
    "scores = query @ key.transpose(2,3) / math.sqrt(mla.head_dim) \n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "output = scores @ value \n",
    "output = output.transpose(1, 2).contiguous().view(bs, seq_len, -1)\n",
    "output = mla.wo(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3340bc98-ba9f-4290-af13-78d137a66544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 tensor 是形状为 (a, b, c) 的张量\n",
    "tensor = torch.randn(2, 3, 4)\n",
    "\n",
    "# 扩展第 1 维，复制 8 份\n",
    "expanded_tensor = tensor.repeat(8, 1, 1)\n",
    "print(expanded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ec2d5-f4f0-4bb2-9461-4946b0e3ce51",
   "metadata": {},
   "source": [
    "### KV cache存的是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1c071-fd97-4e3c-8985-4385afbc70c4",
   "metadata": {},
   "source": [
    "传统MHA的KV Cache大小: `[2, bs, seq_len, n_kv_head * head_dim]`, 其中2代表K和V\n",
    "\n",
    "如果我们存储MLA up后的矩阵K,V cache，那么与MHA存储无差别\n",
    "\n",
    "那么我们可以存储kv down的矩阵：即 c_kv = w_kv_down @ h, 此时：\n",
    "\n",
    "`[1, bs, seq_len, dc_kv]`\n",
    "\n",
    "如果原来  2 * n_kv_head * head_dim = dim = 2 * 4096, 如果dc_kv  = 512\n",
    "\n",
    "那么MLA KV-cache就为MHA的 1 / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe65fa1-04d5-483f-9be0-e0e2d3c28165",
   "metadata": {},
   "source": [
    "### MLA压缩的本质是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3126fc9-e532-4ee8-b73f-cf75cbbf5f85",
   "metadata": {},
   "source": [
    "计算时间换空间\n",
    "\n",
    "存储时刻，w_k_up\n",
    "\n",
    "decoding时刻 即将：\n",
    "\n",
    "`K = w_k_up @ c_kv`\n",
    "\n",
    "`V = w_v_up @ c_kv`\n",
    "\n",
    "压缩KV-Cache的量的意义？以VLLM来说，减少KV-Cache量。推理服务能跑更大的batch-size，从而提高inference，decoding的效率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadf938-8278-4484-a22f-972666622ece",
   "metadata": {},
   "source": [
    "### MLA下位置编码问题\n",
    "\n",
    "1. 常规算法为：RoPE(W_k_up( w_k_down(h) )), 这里的RoPE没法低秩分解\n",
    "2. 可以写为attention： `Rk @ Wkup @ wkdown(h)^T` `@` `[Rq @ Wqup @ wqdown(h)]^T` \n",
    "3. 上式注意到，我们所存储kv cache是wkdown(h)为 `seq x dc_kv`, 那么我们每次计算出了要up，而且还要对**所有token**增加RoPE操作\n",
    "4. 有什么方式可以减少RoPE操作吗？解决方案为在h上，增加额外的参数矩阵比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e368027c-134e-43df-9a37-307c7b2b8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadsLatentAttention_withRoPE(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        # self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads # 18/6 = 3\n",
    "        self.dc_kv = args.dc_kv\n",
    "        self.dc_q = args.dc_q\n",
    "\n",
    "        # MLA Structure\n",
    "        self.wq_down = nn.Linear(in_features=self.dim, out_features=self.dc_q, bias=False,)\n",
    "        self.wq_up = nn.Linear(in_features=self.dc_q, out_features=self.dim , bias=False,)\n",
    "\n",
    "        self.wkv_down = nn.Linear(in_features=self.dim, out_features=self.dc_kv, bias=False,)\n",
    "        self.wk_up = nn.Linear(in_features=self.dc_kv, out_features=self.dim, bias=False,)\n",
    "        self.wv_up = nn.Linear(in_features=self.dc_kv, out_features=self.dim, bias=False,)\n",
    "        \n",
    "        self.wo = nn.Linear(in_features=self.dim, out_features=self.dim,bias=False,)\n",
    "\n",
    "        # RoPE Weight\n",
    "        # K 每头一样， Q每头不一样\n",
    "        self.wq_up_rope = nn.Linear(in_features=self.dc_q, out_features=self.dim , bias=False,)\n",
    "        self.wk_head_rope = nn.Linear(in_features=self.dim, out_features=self.head_dim , bias=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a237ae87-cdfa-4a88-8276-0d618cd67517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadsLatentAttention_withRoPE(\n",
      "  (wq_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wq_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wkv_down): Linear(in_features=64, out_features=4, bias=False)\n",
      "  (wk_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wv_up): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (wq_up_rope): Linear(in_features=4, out_features=64, bias=False)\n",
      "  (wk_head_rope): Linear(in_features=64, out_features=8, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mla_rope = MultiHeadsLatentAttention_withRoPE(config)\n",
    "print(mla_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5796accc-0d9f-4212-8f99-673018fd812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n",
      "torch.Size([3, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# 增加rope\n",
    "c_q = mla_rope.wq_down(h)\n",
    "xq = mla_rope.wq_up(c_q)\n",
    "\n",
    "c_kv = mla_rope.wkv_down(h)\n",
    "xk = mla_rope.wk_up(c_kv)\n",
    "xv = mla_rope.wv_up(c_kv)\n",
    "\n",
    "# print(xq.shape)\n",
    "# print(xk.shape)\n",
    "# print(xv.shape)\n",
    "\n",
    "# 位置编码相关\n",
    "r_q = mla_rope.wq_up_rope(c_q) #多头\n",
    "r_k = mla_rope.wk_head_rope(h) #单头\n",
    "print(r_q.shape)\n",
    "print(r_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d890c66-4891-4264-ba0a-60c9fd56269d",
   "metadata": {},
   "source": [
    "产生新的疑问，r_q 和 r_k 维度不同，那么做rope的dim如何处理？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96c3e98-42d3-492f-b784-2b09d2976757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简易写下\n",
    "rope_matrix_q = [torch.randn(mla_rope.dim, mla_rope.dim)] * seq_len\n",
    "rope_matrix_k = [torch.randn(mla_rope.head_dim, mla_rope.head_dim)] * seq_len\n",
    "def apply_rope_q(x, seq_len, rope_matrix):\n",
    "    for i in range(seq_len):\n",
    "        x[:, i, :] = x[:, i, :] @ rope_matrix[i]\n",
    "    return x\n",
    "\n",
    "rope_q = apply_rope_q(r_q, seq_len, rope_matrix_q)\n",
    "rope_k = apply_rope_q(r_k, seq_len, rope_matrix_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1174ea-6c9c-4b8b-bde0-1e863c87012a",
   "metadata": {},
   "source": [
    "对于q每头，cat不一样的位置信息\n",
    "\n",
    "对于k每头，cat一样的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e9bced-8fd6-43ef-80a9-88ca966998ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n",
      "torch.Size([3, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# 与传统多头注意力无差别\n",
    "xq = xq.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim)\n",
    "xk = xk.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim)\n",
    "xv = xv.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim)\n",
    "\n",
    "\n",
    "query = xq.transpose(1,2)\n",
    "key = xk.transpose(1,2) # bs, n_heads, [seq_len, head_dim]\n",
    "value = xv.transpose(1,2)\n",
    "\n",
    "print(query.shape) \n",
    "print(key.shape)\n",
    "\n",
    "## 嵌入rope\n",
    "rope_q_head = rope_q.view(bs, seq_len, mla_rope.n_heads, mla_rope.head_dim)\n",
    "rope_q_head = rope_q_head.transpose(1,2)\n",
    "\n",
    "rope_k_head = rope_k.unsqueeze(dim = 1).repeat( repeats = [1, mla_rope.n_heads, 1, 1])\n",
    "print(rope_q_head.shape)\n",
    "print(rope_k_head.shape)\n",
    "\n",
    "\n",
    "# cat 操作\n",
    "query_cat = torch.cat((query, rope_q_head), dim = -1)\n",
    "key_cat = torch.cat((query, rope_k_head), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76d7d0b6-c115-48c6-950a-f4c7a2b79d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "### 常规attention\n",
    "# scores = query @ key.transpose(2,3) # keys: bs, n_heads, [head_dim, seq_len]\n",
    "# keys: bs, n_heads, [2head_dim, seq_len]\n",
    "scores = query_cat @ key_cat.transpose(2,3) / math.sqrt(2 * mla_rope.head_dim) # cat后dim维度变化，底数也有变化 \n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "output = scores @ value \n",
    "output = output.transpose(1, 2).contiguous().view(bs, seq_len, -1)\n",
    "output = mla.wo(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b825d-6b48-442c-b0ad-345d7b8e58c4",
   "metadata": {},
   "source": [
    "## 增加问题\n",
    "\n",
    "1. 为什么位置编码要分离\n",
    "2. q和k的位置编码维度不一样，那么rope的维度是否一样\n",
    "3. v_up 如何被 wo 吸收\n",
    "5. 写出带kv_cache版本的MLA，及decoding代码\n",
    "6. MLA训练的显存计算\n",
    "7. MLA并行参数分配、张量并行细节和通信"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174e9d5-9781-49bd-ada2-e162cc4f7305",
   "metadata": {},
   "source": [
    "### 答案1\n",
    "\n",
    "1. 位置编码敏感\n",
    "2. 不分离的话，inference阶段要对kv做重复rope变换，\n",
    "3. 如果分离那么直接拼接算好的rope k(我理解其实位置编码rope k也应该是cache起来的）\n",
    "\n",
    "原论文\n",
    "\n",
    "> To be specific, RoPE is position-sensitive for both keys and queries\n",
    "\n",
    "> As a result, we must recompute the keys for all the prefix tokens during inference, which will significantly hinder the inference efficiency\n",
    "\n",
    "> During inference, the decoupled key should also be cached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b9f4a-a2d4-46b4-898c-eb9be2154494",
   "metadata": {},
   "source": [
    "### 答案2:\n",
    "\n",
    ">  Therefore, DeepSeek-V2 requires a total KV cache containing $(𝑑_𝑐 + 𝑑^𝑅_ℎ)𝑙 $ elements.\n",
    "1. 先说KV cache量是多少, l是长度，d_c是latent维度，$d^R_h$是单头的向量维度\n",
    "\n",
    "2. 为什么QK位置编码有区别，Q是多头$d^R_h * h$, h是头数，那么多头保精度\n",
    "\n",
    "3. rope K是单头，从cache角度来看，存单头k rope是经济的，随着上下文长度增加，如果是k rope多头维度本质上与常规KV无差别\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dad02a-3509-42d4-8d82-13013262be1b",
   "metadata": {},
   "source": [
    "### 答案3\n",
    "\n",
    "原论文公式\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "   [\\mathbf{v}_{t, 1}^{C};\\mathbf{v}_{t, 2}^{C};...;\\mathbf{v}_{t, n_{h}}^{C}] = \\mathbf{v}_{t}^{C} &= W^{UV} \\mathbf{c}_{t}^{KV}, \\\\\n",
    "    \\mathbf{o}_{t, i} &= \\sum_{j=1}^{t} \\operatorname{Softmax}_j(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_{h} + d_{h}^{R}}}) \\mathbf{v}_{j, i}^{C}, \\\\\n",
    "    \\mathbf{u}_{t} &= W^{O} [\\mathbf{o}_{t, 1};\\mathbf{o}_{t, 2};...;\\mathbf{o}_{t, n_{h}}],\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "关于吸收, 原论文提到uv吸收到wo\n",
    "\n",
    "> Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^{O}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b58811-4eb9-4e68-b05e-3f83c34e7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 586.4882,  -14.7992, -678.4225,  116.2531, -610.6373],\n",
      "        [ -59.8604, -180.5592, -280.9830, -789.0940, -244.8712],\n",
      "        [-199.1532,  982.1359, -158.7461,  496.4213,   89.5527],\n",
      "        [ -59.8604, -180.5592, -280.9830, -789.0940, -244.8712],\n",
      "        [-199.1532,  982.1359, -158.7462,  496.4213,   89.5527]])\n",
      "tensor([[ 586.4888,  -14.8000, -678.4230,  116.2542, -610.6379],\n",
      "        [ -59.8597, -180.5598, -280.9839, -789.0929, -244.8703],\n",
      "        [-199.1532,  982.1357, -158.7457,  496.4212,   89.5529],\n",
      "        [ -59.8597, -180.5598, -280.9839, -789.0929, -244.8703],\n",
      "        [-199.1532,  982.1357, -158.7457,  496.4212,   89.5529]])\n"
     ]
    }
   ],
   "source": [
    "# 编程举例 WUV 到 WO的矩阵吸收\n",
    "\n",
    "c = 64\n",
    "d = 4096\n",
    "seq_len = 16 # length\n",
    "\n",
    "c_kv = torch.randn(seq_len, c)\n",
    "W_O = torch.randn(d, d)\n",
    "W_UV = torch.randn(c, d)\n",
    "\n",
    "Q = torch.randn(seq_len, d) # ignore rope q\n",
    "K = torch.randn(seq_len, d) # ignore rope k\n",
    "\n",
    "# 原论文公式代码，忽略底数 \n",
    "V = c_kv @ W_UV\n",
    "S = Q @ K.t()  \n",
    "P = F.softmax(S, dim = -1)\n",
    "O = P @ V\n",
    "U = O @ W_O\n",
    "print(U[:5,:5])\n",
    "\n",
    "# 吸收版本\n",
    "w_uv_absord = W_UV @ W_O  \n",
    "# 那么实际部署时，就保留这个参数矩阵 w_uv_absord。\n",
    "# 原本存 dxd + cxd, 那么现在只存 cxd (减少的参数量体感非常明显)\n",
    "\n",
    "V = c_kv @ w_uv_absord\n",
    "S = Q @ K.t()  \n",
    "P = F.softmax(S, dim = -1)\n",
    "O = P @ V\n",
    "# U = O @ W_o # 不需要再变换了\n",
    "print(O[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2222536-eee7-4684-bdc9-35604b41b552",
   "metadata": {},
   "source": [
    "### 答案4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b12c24-069b-42b7-b613-fc1e67c3ee69",
   "metadata": {},
   "source": [
    "1. 分析有哪些矩阵可以吸收\n",
    "\n",
    "> Fortunately, due to the associative law of matrix multiplication, we can absorb $W^{UK}$ into $W^{UQ}$, and $W^{UV}$ into $W^{O}$.\n",
    "\n",
    "2. 另外q的矩阵可以吸收，q矩阵除了常规的 wq x = Q 之外， 还得做一次 wq_rope x = q_rope, 增加一次矩阵乘\n",
    "\n",
    "3. kv cache要存 1. `rope k`， 2.latent `C`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d95eba-0a30-4abe-8614-c8f75fbcfa19",
   "metadata": {},
   "source": [
    "### 答案5复杂，先skip\n",
    "\n",
    "### 答案6分析\n",
    "\n",
    "#### 切分方案A\n",
    "\n",
    "1. 先说down 参数矩阵， Wk_down ( d x c ), 如果按照c切分，每个GPU 存储 wk_down_shard (d x c'), 输出 h(l,d) -> C(l, c')\n",
    "\n",
    "2. up 矩阵，wk_up (c x d), 按照c切分, 每个gpu存储 wk_up_shard(c' x d) ，每个GPU输出   C(l, c') x  (c', d) -> K(l, d)\n",
    "\n",
    "这里就有问题K在每个GPU上都是满头的， 重新切分：\n",
    "\n",
    "#### 切分方案B \n",
    "\n",
    "1. 先说down 参数矩阵， Wk_down ( d x c ), 如果按照c切分，每个GPU 存储 wk_down_shard (d x c'), 输出 h(l,d) -> C(l, c')\n",
    "\n",
    "2. 将每个 C(l, c') 分发给其他GPU， 那么每块GPU都有完整的 C(l, c)， # 考虑到c‘是非常小量的, 比如c是64， c‘是8\n",
    "\n",
    "3. up 矩阵，wk_up (c x d), 按照d切分,  每个gpu存储 wk_up_shard(c x d'), 每个GPU输出 C(l, c) x  (c, d') -> K(l, d')\n",
    "\n",
    "所以这样，每个头独立存在各GPU上，那么就可以head-parallel算 多头注意力了。\n",
    "\n",
    "- 待分析， 上述未分析是否backward通信友好\n",
    "\n",
    "\n",
    "依次类推可以设计出MLA的tensor parallel架构\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
