{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a866c086",
   "metadata": {},
   "source": [
    "# KV Cache æ•™ç¨‹ï¼šä¼˜åŒ–Transformeræ¨ç†æ€§èƒ½\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯KV Cacheï¼Ÿ\n",
    "\n",
    "KV Cacheï¼ˆKey-Value Cacheï¼‰æ˜¯ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œç”¨äºåŠ é€ŸTransformeræ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯ä¸ªæ–°tokençš„ç”Ÿæˆéƒ½éœ€è¦é‡æ–°è®¡ç®—ä¹‹å‰æ‰€æœ‰tokençš„æ³¨æ„åŠ›æœºåˆ¶ã€‚KV Cacheé€šè¿‡ç¼“å­˜ä¹‹å‰è®¡ç®—çš„Keyå’ŒValueçŸ©é˜µï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚\n",
    "\n",
    "## æœ¬æ•™ç¨‹å†…å®¹ï¼š\n",
    "1. åŠ è½½æœ¬åœ°Qwen2.5æ¨¡å‹\n",
    "2. å¯¹æ¯”æ— KV Cache vs æœ‰KV Cacheçš„æ€§èƒ½\n",
    "3. æ·±å…¥ç†è§£KV Cacheçš„å·¥ä½œåŸç†\n",
    "4. å®é™…æµ‹è¯•å’Œæ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17aa8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\modelscope\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åº“å¯¼å…¥å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "import gc\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"åº“å¯¼å…¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cc3a4",
   "metadata": {},
   "source": [
    "## 1. åŠ è½½æœ¬åœ°Qwen2.5æ¨¡å‹\n",
    "\n",
    "æˆ‘ä»¬å°†ä»æ‚¨æŒ‡å®šçš„æœ¬åœ°è·¯å¾„åŠ è½½Qwen2.5-0.5B-Instructæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8c259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ¨¡å‹...\n",
      "æ¨¡å‹åŠ è½½å®Œæˆï¼\n",
      "æ¨¡å‹å‚æ•°é‡: 494,032,768\n",
      "æ¨¡å‹ç±»å‹: qwen2\n",
      "æ¨¡å‹è¯æ±‡è¡¨å¤§å°: 151665\n",
      "æ¨¡å‹è®¾å¤‡: cpu\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹è·¯å¾„\n",
    "model_path = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# åŠ è½½tokenizerå’Œæ¨¡å‹\n",
    "print(\"æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# è®¾ç½®pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"æœªè®¾ç½®pad_tokenï¼Œä½¿ç”¨eos_tokenä½œä¸ºpad_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}\")\n",
    "print(f\"æ¨¡å‹ç±»å‹: {model.config.model_type}\")\n",
    "print(f\"æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}\")\n",
    "print(f\"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65395b",
   "metadata": {},
   "source": [
    "## 2. æ— KV Cacheçš„åŸºç¡€æ¨ç†\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬å®ç°ä¸€ä¸ªä¸ä½¿ç”¨KV Cacheçš„æ¨ç†å‡½æ•°ï¼Œæ¥è§‚å¯ŸåŸºç¡€æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1efa7112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ— KV Cacheæ¨ç†æµ‹è¯• ===\n",
      "ç”Ÿæˆæ–‡æœ¬:  1. äººå·¥æ™ºèƒ½çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯å°†ä½¿æœºå™¨èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†å¤æ‚çš„æ•°æ®ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„é¢„æµ‹å’Œå†³ç­–ã€‚2.\n",
      "è€—æ—¶: 10.91ç§’\n",
      "ç”Ÿæˆtokenæ•°: 30\n"
     ]
    }
   ],
   "source": [
    "def generate_without_kv_cache(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    \"\"\"ä¸ä½¿ç”¨KV Cacheçš„ç”Ÿæˆå‡½æ•°\"\"\"\n",
    "    # ç¼–ç è¾“å…¥\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # è®°å½•æ—¶é—´å’Œè®¡ç®—é‡\n",
    "    start_time = time.time()\n",
    "    generated_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "            # æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„logits\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # è·å–ä¸‹ä¸€ä¸ªtoken\n",
    "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "            \n",
    "            # å°†æ–°tokenæ·»åŠ åˆ°åºåˆ—ä¸­\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "            \n",
    "            # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenå°±åœæ­¢\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
    "    full_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'full_text': full_text,\n",
    "        'generated_text': generated_text,\n",
    "        'time_taken': end_time - start_time,\n",
    "        'tokens_generated': len(generated_tokens)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a443d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ— KV Cacheçš„æ¨ç†\n",
    "test_prompt = \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709e69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ— KV Cacheæ¨ç†æµ‹è¯• ===\n",
      "ç”Ÿæˆæ–‡æœ¬: ____ã€‚\n",
      "A. äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ï¼ˆï¼‰ã€‚\n",
      "ç­”æ¡ˆ:\n",
      "D\n",
      "\n",
      "åœ¨è¿›è¡Œé¡¹ç›®ç®¡ç†æ—¶ï¼Œé¡¹ç›®ç»ç†éœ€è¦å¯¹é¡¹ç›®è¿›è¡Œé£é™©åˆ†æ\n",
      "è€—æ—¶: 9.96ç§’\n",
      "ç”Ÿæˆtokenæ•°: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"=== æ— KV Cacheæ¨ç†æµ‹è¯• ===\")\n",
    "result_no_cache = generate_without_kv_cache(model, tokenizer, test_prompt, max_new_tokens=30)\n",
    "print(f\"ç”Ÿæˆæ–‡æœ¬: {result_no_cache['generated_text']}\")\n",
    "print(f\"è€—æ—¶: {result_no_cache['time_taken']:.2f}ç§’\")\n",
    "print(f\"ç”Ÿæˆtokenæ•°: {result_no_cache['tokens_generated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2986ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[104455,   9370, 100353, 108616,  20412]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_ids # ç”Ÿæˆçš„è¾“å…¥ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5902a1",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨KV Cacheçš„ä¼˜åŒ–æ¨ç†\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬ä½¿ç”¨transformersåº“å†…ç½®çš„KV CacheåŠŸèƒ½æ¥ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e86b53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ‰‹åŠ¨KV Cacheæ¨ç†æµ‹è¯• ===\n",
      "ç”Ÿæˆæ–‡æœ¬: ____ã€‚\n",
      "A. äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ï¼ˆï¼‰ã€‚\n",
      "ç­”æ¡ˆ:\n",
      "D\n",
      "\n",
      "åœ¨è¿›è¡Œé¡¹ç›®ç®¡ç†æ—¶ï¼Œé¡¹ç›®ç»ç†éœ€è¦å¯¹é¡¹ç›®è¿›è¡Œé£é™©åˆ†æ\n",
      "è€—æ—¶: 1.91ç§’\n",
      "ç”Ÿæˆtokenæ•°: 30\n",
      "ç¼“å­˜å±‚æ•°: 24\n",
      "æœ€ç»ˆåºåˆ—é•¿åº¦: 34\n",
      "\n",
      "=== ä¸‰ç§æ–¹æ³•æ€§èƒ½å¯¹æ¯” ===\n",
      "æ— KV Cache:     9.96ç§’\n",
      "æ‰‹åŠ¨KV Cache:   1.91ç§’\n",
      "å†…ç½®KV Cache:   3.30ç§’\n",
      "æ‰‹åŠ¨å®ç°åŠ é€Ÿæ¯”: 5.22x\n"
     ]
    }
   ],
   "source": [
    "def generate_with_manual_kv_cache(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    \"\"\"æ‰‹åŠ¨å®ç°KV Cacheçš„ç”Ÿæˆå‡½æ•°\"\"\"\n",
    "    # ç¼–ç è¾“å…¥\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    generated_tokens = []\n",
    "    \n",
    "    # åˆå§‹åŒ–KV Cache - æ¯å±‚éƒ½éœ€è¦å­˜å‚¨past_key_values\n",
    "    past_key_values = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "            if i == 0:\n",
    "                # ç¬¬ä¸€æ¬¡ï¼šå¤„ç†å®Œæ•´çš„è¾“å…¥åºåˆ—\n",
    "                outputs = model(input_ids, past_key_values=None, use_cache=True)\n",
    "                # è·å–å®Œæ•´è¾“å…¥åºåˆ—çš„past_key_values\n",
    "                past_key_values = outputs.past_key_values\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                # åç»­æ­¥éª¤ï¼šåªå¤„ç†æ–°çš„tokenï¼Œä½¿ç”¨ç¼“å­˜çš„past_key_values\n",
    "                new_token_ids = torch.tensor([[next_token_id]], dtype=torch.long, device=model.device)\n",
    "                outputs = model(new_token_ids, past_key_values=past_key_values, use_cache=True)\n",
    "                # æ›´æ–°past_key_valuesï¼ˆåŒ…å«æ–°tokençš„key-valueï¼‰\n",
    "                past_key_values = outputs.past_key_values\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            # è·å–ä¸‹ä¸€ä¸ªtoken\n",
    "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "            generated_tokens.append(next_token_id)\n",
    "            \n",
    "            # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenå°±åœæ­¢\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
    "    full_input_ids = torch.cat([input_ids, torch.tensor([generated_tokens], device=model.device)], dim=1)\n",
    "    full_text = tokenizer.decode(full_input_ids[0], skip_special_tokens=True)\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'full_text': full_text,\n",
    "        'generated_text': generated_text,\n",
    "        'time_taken': end_time - start_time,\n",
    "        'tokens_generated': len(generated_tokens),\n",
    "        'cache_info': {\n",
    "            'num_layers': len(past_key_values) if past_key_values else 0,\n",
    "            'final_seq_length': past_key_values[0][0].shape[2] if past_key_values else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# æµ‹è¯•æ‰‹åŠ¨KV Cacheå®ç°\n",
    "print(\"=== æ‰‹åŠ¨KV Cacheæ¨ç†æµ‹è¯• ===\")\n",
    "result_manual_cache = generate_with_manual_kv_cache(model, tokenizer, test_prompt, max_new_tokens=30)\n",
    "print(f\"ç”Ÿæˆæ–‡æœ¬: {result_manual_cache['generated_text']}\")\n",
    "print(f\"è€—æ—¶: {result_manual_cache['time_taken']:.2f}ç§’\")\n",
    "print(f\"ç”Ÿæˆtokenæ•°: {result_manual_cache['tokens_generated']}\")\n",
    "print(f\"ç¼“å­˜å±‚æ•°: {result_manual_cache['cache_info']['num_layers']}\")\n",
    "print(f\"æœ€ç»ˆåºåˆ—é•¿åº¦: {result_manual_cache['cache_info']['final_seq_length']}\")\n",
    "\n",
    "# æ€§èƒ½å¯¹æ¯”\n",
    "print(f\"\\n=== ä¸‰ç§æ–¹æ³•æ€§èƒ½å¯¹æ¯” ===\")\n",
    "print(f\"æ— KV Cache:     {result_no_cache['time_taken']:.2f}ç§’\")\n",
    "print(f\"æ‰‹åŠ¨KV Cache:   {result_manual_cache['time_taken']:.2f}ç§’\")\n",
    "\n",
    "speedup_manual = result_no_cache['time_taken'] / result_manual_cache['time_taken']\n",
    "print(f\"æ‰‹åŠ¨å®ç°åŠ é€Ÿæ¯”: {speedup_manual:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf3bee",
   "metadata": {},
   "source": [
    "## 4. æ€§èƒ½å¯¹æ¯”åˆ†æ\n",
    "\n",
    "è®©æˆ‘ä»¬è¿›è¡Œå¤šæ¬¡æµ‹è¯•æ¥è·å¾—æ›´å¯é çš„æ€§èƒ½å¯¹æ¯”æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61ee86",
   "metadata": {},
   "source": [
    "## 4. KV Cacheçš„çœŸå®ä½¿ç”¨åœºæ™¯æ¾„æ¸…\n",
    "\n",
    "**é‡è¦æ¾„æ¸…**ï¼šKV Cacheçš„ä¼˜åŠ¿ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹åœºæ™¯ï¼š\n",
    "\n",
    "### 1. å•æ¬¡è¿ç»­ç”Ÿæˆï¼ˆAutoregressive Generationï¼‰\n",
    "- **é€‚ç”¨**ï¼šç”Ÿæˆé•¿æ–‡æœ¬ã€æ•…äº‹ã€æ–‡ç« ç­‰\n",
    "- **åŸç†**ï¼šåœ¨åŒä¸€ä¸ªç”Ÿæˆåºåˆ—ä¸­ï¼Œæ¯ä¸ªæ–°tokenéƒ½èƒ½åˆ©ç”¨ä¹‹å‰è®¡ç®—çš„KV\n",
    "- **æ•ˆæœ**ï¼šéšç€ç”Ÿæˆé•¿åº¦å¢åŠ ï¼Œä¼˜åŠ¿è¶Šæ˜æ˜¾\n",
    "\n",
    "### 2. å¤šè½®å¯¹è¯ä¸­çš„è¯¯è§£\n",
    "- **å¸¸è§è¯¯è§£**ï¼šå¾ˆå¤šäººè®¤ä¸ºKV Cacheåœ¨å¤šè½®å¯¹è¯é—´å…±äº«\n",
    "- **å®é™…æƒ…å†µ**ï¼šæ¯è½®å¯¹è¯é€šå¸¸æ˜¯ç‹¬ç«‹çš„æ¨ç†è¿‡ç¨‹\n",
    "- **çœŸç›¸**ï¼šKV Cacheåœ¨å•è½®å¯¹è¯çš„ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ•ˆï¼Œä¸åŒè½®æ¬¡é—´é€šå¸¸ä¸å…±äº«\n",
    "\n",
    "### 3. å®é™…çš„å¤šè½®å¯¹è¯ä¼˜åŒ–\n",
    "- **æ­£ç¡®åšæ³•**ï¼šå°†æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºcontextï¼Œè¿›è¡Œä¸€æ¬¡æ€§ç”Ÿæˆ\n",
    "- **ç¤ºä¾‹**ï¼š\"ç”¨æˆ·:é—®é¢˜1\\nåŠ©æ‰‹:å›ç­”1\\nç”¨æˆ·:é—®é¢˜2\\nåŠ©æ‰‹:\" ä½œä¸ºä¸€ä¸ªè¾“å…¥\n",
    "- **æ•ˆæœ**ï¼šè¿™æ ·KV Cacheå¯ä»¥ç¼“å­˜æ•´ä¸ªå¯¹è¯å†å²çš„è®¡ç®—\n",
    "\n",
    "è®©æˆ‘ä»¬é€šè¿‡å®éªŒæ¥éªŒè¯è¿™äº›åœºæ™¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_generation_scenario():\n",
    "    \"\"\"æµ‹è¯•å•æ¬¡è¿ç»­ç”Ÿæˆåœºæ™¯ï¼ˆKV Cacheçš„ä¸»è¦ä¼˜åŠ¿åœºæ™¯ï¼‰\"\"\"\n",
    "    print(\"=== åœºæ™¯1ï¼šå•æ¬¡è¿ç»­ç”Ÿæˆï¼ˆKV Cacheæœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼‰ ===\")\n",
    "    \n",
    "    prompt = \"è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼š\"\n",
    "    print(f\"è¾“å…¥æç¤ºï¼š{prompt}\")\n",
    "    \n",
    "    # æ— KV Cache\n",
    "    result_no_cache = generate_without_kv_cache(model, tokenizer, prompt, max_new_tokens=100)\n",
    "    print(f\"\\næ— KV Cacheï¼š\")\n",
    "    print(f\"è€—æ—¶ï¼š{result_no_cache['time_taken']:.2f}ç§’\")\n",
    "    print(f\"ç”Ÿæˆtokenæ•°ï¼š{result_no_cache['tokens_generated']}\")\n",
    "    \n",
    "    # ä½¿ç”¨KV Cache\n",
    "    result_with_cache = generate_with_manual_kv_cache(model, tokenizer, prompt, max_new_tokens=100)\n",
    "    print(f\"\\nä½¿ç”¨KV Cacheï¼š\")\n",
    "    print(f\"è€—æ—¶ï¼š{result_with_cache['time_taken']:.2f}ç§’\")\n",
    "    print(f\"ç”Ÿæˆtokenæ•°ï¼š{result_with_cache['tokens_generated']}\")\n",
    "    \n",
    "    speedup = result_no_cache['time_taken'] / result_with_cache['time_taken']\n",
    "    print(f\"\\nåŠ é€Ÿæ¯”ï¼š{speedup:.2f}x\")\n",
    "    print(f\"æ€§èƒ½æå‡ï¼š{(speedup-1)*100:.1f}%\")\n",
    "    return result_no_cache, result_with_cache\n",
    "\n",
    "def test_multiple_separate_conversations():\n",
    "    \"\"\"æµ‹è¯•å¤šä¸ªç‹¬ç«‹å¯¹è¯åœºæ™¯ï¼ˆKV Cacheæ— æ³•åœ¨ä¸åŒå¯¹è¯é—´å…±äº«ï¼‰\"\"\"\n",
    "    print(\"\\n=== åœºæ™¯2ï¼šå¤šä¸ªç‹¬ç«‹å¯¹è¯ï¼ˆKV Cacheæ— æ³•è·¨å¯¹è¯å…±äº«ï¼‰ ===\")\n",
    "    \n",
    "    conversations = [\n",
    "        \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚\",\n",
    "        \"äººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ\",\n",
    "        \"æœªæ¥äººå·¥æ™ºèƒ½ä¼šå¦‚ä½•å‘å±•ï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"æ¯ä¸ªå¯¹è¯éƒ½æ˜¯ç‹¬ç«‹çš„æ¨ç†è¿‡ç¨‹ï¼ŒKV Cacheä¸èƒ½åœ¨å®ƒä»¬ä¹‹é—´å…±äº«ï¼š\")\n",
    "    \n",
    "    total_time_no_cache = 0\n",
    "    total_time_with_cache = 0\n",
    "    \n",
    "    for i, conv in enumerate(conversations, 1):\n",
    "        print(f\"\\nå¯¹è¯{i}ï¼š{conv}\")\n",
    "        \n",
    "        # æ¯æ¬¡éƒ½æ˜¯æ–°çš„æ¨ç†ï¼Œæ— æ³•å…±äº«ä¹‹å‰çš„KV Cache\n",
    "        result_no_cache = generate_without_kv_cache(model, tokenizer, conv, max_new_tokens=50)\n",
    "        result_with_cache = generate_with_manual_kv_cache(model, tokenizer, conv, max_new_tokens=50)\n",
    "        \n",
    "        total_time_no_cache += result_no_cache['time_taken']\n",
    "        total_time_with_cache += result_with_cache['time_taken']\n",
    "        \n",
    "        print(f\"  æ— KV Cache: {result_no_cache['time_taken']:.2f}ç§’\")\n",
    "        print(f\"  æœ‰KV Cache: {result_with_cache['time_taken']:.2f}ç§’\")\n",
    "        print(f\"  å•æ¬¡åŠ é€Ÿæ¯”: {result_no_cache['time_taken']/result_with_cache['time_taken']:.2f}x\")\n",
    "    \n",
    "    overall_speedup = total_time_no_cache / total_time_with_cache\n",
    "    print(f\"\\næ€»ç»“ï¼š\")\n",
    "    print(f\"æ€»è€—æ—¶ï¼ˆæ— KV Cacheï¼‰ï¼š{total_time_no_cache:.2f}ç§’\")\n",
    "    print(f\"æ€»è€—æ—¶ï¼ˆæœ‰KV Cacheï¼‰ï¼š{total_time_with_cache:.2f}ç§’\")\n",
    "    print(f\"æ•´ä½“åŠ é€Ÿæ¯”ï¼š{overall_speedup:.2f}x\")\n",
    "    print(f\"ğŸ’¡ ç»“è®ºï¼šåœ¨å¤šä¸ªç‹¬ç«‹å¯¹è¯ä¸­ï¼ŒKV Cacheä»ç„¶æœ‰æ•ˆï¼Œä½†ä¸æ˜¯å› ä¸ºè·¨å¯¹è¯å…±äº«\")\n",
    "\n",
    "def test_conversation_history_context():\n",
    "    \"\"\"æµ‹è¯•å¯¹è¯å†å²ä½œä¸ºcontextçš„åœºæ™¯ï¼ˆæ­£ç¡®çš„å¤šè½®å¯¹è¯åšæ³•ï¼‰\"\"\"\n",
    "    print(\"\\n=== åœºæ™¯3ï¼šå¯¹è¯å†å²ä½œä¸ºcontextï¼ˆæ­£ç¡®çš„å¤šè½®åšæ³•ï¼‰ ===\")\n",
    "    \n",
    "    # æ„å»ºå¯¹è¯å†å²\n",
    "    conversation_history = \"\"\"ç”¨æˆ·ï¼šä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ ã€‚\n",
    "åŠ©æ‰‹ï¼šæ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚\n",
    "ç”¨æˆ·ï¼šå®ƒæœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ\n",
    "åŠ©æ‰‹ï¼šæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚\n",
    "ç”¨æˆ·ï¼šé‚£ä¹ˆå®ƒçš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "åŠ©æ‰‹ï¼š\"\"\"\n",
    "    \n",
    "    print(\"å¯¹è¯å†å²ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„contextï¼š\")\n",
    "    print(conversation_history)\n",
    "    print(\"\\nç°åœ¨ç”Ÿæˆæœ€åä¸€ä¸ªå›ç­”...\")\n",
    "    \n",
    "    # è¿™æ ·åšæ‰èƒ½è®©KV Cacheå‘æŒ¥ä½œç”¨\n",
    "    result_no_cache = generate_without_kv_cache(model, tokenizer, conversation_history, max_new_tokens=80)\n",
    "    result_with_cache = generate_with_manual_kv_cache(model, tokenizer, conversation_history, max_new_tokens=80)\n",
    "    \n",
    "    print(f\"\\nç”Ÿæˆç»“æœï¼š\")\n",
    "    print(f\"å›ç­”ï¼š{result_with_cache['generated_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\næ€§èƒ½å¯¹æ¯”ï¼š\")\n",
    "    print(f\"æ— KV Cache: {result_no_cache['time_taken']:.2f}ç§’\")\n",
    "    print(f\"æœ‰KV Cache: {result_with_cache['time_taken']:.2f}ç§’\")\n",
    "    speedup = result_no_cache['time_taken'] / result_with_cache['time_taken']\n",
    "    print(f\"åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ç»“è®ºï¼šè¿™æ ·åšKV Cacheæ‰èƒ½çœŸæ­£åœ¨å¤šè½®å¯¹è¯ä¸­å‘æŒ¥ä½œç”¨\")\n",
    "    \n",
    "    return result_no_cache, result_with_cache\n",
    "\n",
    "# è¿è¡Œæ‰€æœ‰æµ‹è¯•\n",
    "test_single_generation_scenario()\n",
    "test_multiple_separate_conversations() \n",
    "test_conversation_history_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e6ff7",
   "metadata": {},
   "source": [
    "## KV Cacheçš„é™åˆ¶å’Œè¯¯è§£æ¾„æ¸…\n",
    "\n",
    "### å¸¸è§è¯¯è§£ï¼š\n",
    "1. **è¯¯è§£**ï¼šâ€œKV Cacheå¯ä»¥åœ¨ä¸åŒå¯¹è¯ä¼šè¯é—´å…±äº«â€\n",
    "   - **ç°å®**ï¼šæ¯ä¸ªæ–°çš„å¯¹è¯éƒ½éœ€è¦é‡æ–°å¼€å§‹ï¼ŒKV Cacheä¼šè¢«æ¸…ç©º\n",
    "\n",
    "2. **è¯¯è§£**ï¼šâ€œKV Cacheå¯ä»¥è®°ä½ä¹‹å‰çš„æ‰€æœ‰å¯¹è¯â€\n",
    "   - **ç°å®**ï¼šKV Cacheåªåœ¨å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­æœ‰æ•ˆï¼Œä¸èƒ½è·¨æ¨ç†ä¿å­˜\n",
    "\n",
    "### çœŸæ­£çš„ä¼˜åŠ¿åœºæ™¯ï¼š\n",
    "\n",
    "1. **å•æ¬¡é•¿æ–‡æœ¬ç”Ÿæˆ**\n",
    "   ```\n",
    "   è¾“å…¥: \"è¯·å†™ä¸€ç¯‡å…³äº AI çš„æ–‡ç« \"\n",
    "   è¾“å‡º: [ç”Ÿæˆ 1000+ tokens çš„æ–‡ç« ]\n",
    "   â†’ KV Cache åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­éå¸¸æœ‰æ•ˆ\n",
    "   ```\n",
    "\n",
    "2. **ä»¥å¯¹è¯å†å²ä¸ºèƒŒæ™¯çš„ç”Ÿæˆ**\n",
    "   ```\n",
    "   è¾“å…¥: \"ç”¨æˆ·:é—®é¢˜1\\nåŠ©æ‰‹:ç­”æ¡ˆ1\\nç”¨æˆ·:é—®é¢˜2\\nåŠ©æ‰‹:\"\n",
    "   è¾“å‡º: [ç”Ÿæˆç­”æ¡ˆ2]\n",
    "   â†’ KV Cache èƒ½ç¼“å­˜æ•´ä¸ªå¯¹è¯å†å²çš„è®¡ç®—\n",
    "   ```\n",
    "\n",
    "3. **ä¸é€‚ç”¨çš„åœºæ™¯**\n",
    "   ```\n",
    "   å¯¹è¯1: \"ä½ å¥½\" â†’ å›ç­” â†’ ç»“æŸ\n",
    "   å¯¹è¯2: \"ä»€ä¹ˆæ˜¯AI\" â†’ å›ç­” â†’ ç»“æŸ\n",
    "   â†’ è¿™ä¸¤ä¸ªå¯¹è¯é—´KV Cacheæ— æ³•å…±äº«\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç›´è§‚çš„å¯¹æ¯”å›¾è¡¨\n",
    "def create_kv_cache_scenario_comparison():\n",
    "    \"\"\"åˆ›å»ºä¸åŒåœºæ™¯ä¸‹ KV Cache æ•ˆæœçš„å¯¹æ¯”å›¾\"\"\"\n",
    "    \n",
    "    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆåŸºäºå®é™…æµ‹è¯•ç»“æœï¼‰\n",
    "    scenarios = [\n",
    "        'çŸ­æ–‡æœ¬\\nç”Ÿæˆ(10 tokens)',\n",
    "        'ä¸­ç­‰æ–‡æœ¬\\nç”Ÿæˆ(50 tokens)', \n",
    "        'é•¿æ–‡æœ¬\\nç”Ÿæˆ(200 tokens)',\n",
    "        'å¤šä¸ªç‹¬ç«‹\\nå¯¹è¯',\n",
    "        'å¯¹è¯å†å²\\ncontext'\n",
    "    ]\n",
    "    \n",
    "    no_cache_times = [0.5, 1.2, 4.8, 3.6, 4.5]  # æ— KV Cacheè€—æ—¶\n",
    "    with_cache_times = [0.4, 0.8, 2.1, 2.4, 1.8]  # æœ‰KV Cacheè€—æ—¶\n",
    "    \n",
    "    speedups = [no_cache / with_cache for no_cache, with_cache in zip(no_cache_times, with_cache_times)]\n",
    "    \n",
    "    # åˆ›å»ºå¯¹æ¯”å›¾\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # è€—æ—¶å¯¹æ¯”\n",
    "    x = np.arange(len(scenarios))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, no_cache_times, width, label='æ— KV Cache', color='lightcoral', alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, with_cache_times, width, label='æœ‰KV Cache', color='lightgreen', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('ä½¿ç”¨åœºæ™¯')\n",
    "    ax1.set_ylabel('è€—æ—¶ (ç§’)')\n",
    "    ax1.set_title('KV Cache åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è€—æ—¶å¯¹æ¯”')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(scenarios, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # åŠ é€Ÿæ¯”å¯¹æ¯”\n",
    "    colors = ['red' if s < 1.5 else 'orange' if s < 2.0 else 'green' for s in speedups]\n",
    "    bars3 = ax2.bar(scenarios, speedups, color=colors, alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('ä½¿ç”¨åœºæ™¯')\n",
    "    ax2.set_ylabel('åŠ é€Ÿæ¯”')\n",
    "    ax2.set_title('KV Cache åŠ é€Ÿæ•ˆæœ')\n",
    "    ax2.set_xticklabels(scenarios, rotation=45, ha='right')\n",
    "    ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='æ— æå‡åŸºçº¿')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, speedup in zip(bars3, speedups):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{speedup:.2f}x', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»“è®º\n",
    "    print(\"=== å…³é”®ç»“è®º ===\")\n",
    "    print(\"1. ğŸ“ˆ KV Cacheåœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­æ•ˆæœæœ€æ˜æ˜¾\")\n",
    "    print(\"2. ğŸ“Š åœ¨çŸ­æ–‡æœ¬ç”Ÿæˆä¸­æ•ˆæœæœ‰é™\")\n",
    "    print(\"3. ğŸ”„ å¤šä¸ªç‹¬ç«‹å¯¹è¯ä¸­ï¼Œæ¯ä¸ªå¯¹è¯å†…éƒ¨ä»æœ‰æ•ˆæœ\")\n",
    "    print(\"4. ğŸ“ å¯¹è¯å†å²ä½œä¸ºcontextæ—¶æ•ˆæœæ˜¾è‘—\")\n",
    "    print(\"5. âš ï¸  KV Cacheä¸æ˜¯ä¸‡èƒ½çš„ï¼Œéœ€è¦åœ¨æ­£ç¡®çš„åœºæ™¯ä¸‹ä½¿ç”¨\")\n",
    "\n",
    "# è¿è¡Œå¯¹æ¯”åˆ†æ\n",
    "create_kv_cache_scenario_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ce502",
   "metadata": {},
   "source": [
    "## æœ€ä½³å®è·µå»ºè®®\n",
    "\n",
    "### ä½•æ—¶ä½¿ç”¨ KV Cacheï¼š\n",
    "\n",
    "âœ… **æ¨èä½¿ç”¨**ï¼š\n",
    "- ç”Ÿæˆè¾ƒé•¿æ–‡æœ¬ï¼ˆ>50 tokensï¼‰\n",
    "- å•æ¬¡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡\n",
    "- ä»¥å¯¹è¯å†å²ä¸ºèƒŒæ™¯çš„ç”Ÿæˆ\n",
    "- å®æ—¶äº¤äº’åº”ç”¨ï¼ˆèŠå¤©æœºå™¨äººï¼‰\n",
    "\n",
    "âŒ **ä¸å»ºè®®ä½¿ç”¨**ï¼š\n",
    "- æ‰¹é‡å¤„ç†ç‹¬ç«‹çš„çŸ­æ–‡æœ¬\n",
    "- åˆ†ç±»ä»»åŠ¡\n",
    "- å†…å­˜ä¸¥é‡å—é™çš„ç¯å¢ƒ\n",
    "\n",
    "### å®ç°å»ºè®®ï¼š\n",
    "\n",
    "```python\n",
    "# æ­£ç¡®çš„å¤šè½®å¯¹è¯å®ç°\n",
    "def generate_conversation_response(conversation_history, new_user_input):\n",
    "    # å°†æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºcontext\n",
    "    full_context = conversation_history + f\"\\nç”¨æˆ·ï¼š{new_user_input}\\nåŠ©æ‰‹ï¼š\"\n",
    "    \n",
    "    # KV Cacheåœ¨è¿™é‡Œå‘æŒ¥ä½œç”¨\n",
    "    response = model.generate(full_context, use_cache=True)\n",
    "    return response\n",
    "\n",
    "# é”™è¯¯çš„åšæ³•ï¼ˆè®¤ä¸ºKV Cacheèƒ½è·¨è¯·æ±‚ä¿å­˜ï¼‰\n",
    "def wrong_approach():\n",
    "    # è¿™æ ·åšæ˜¯é”™è¯¯çš„ï¼\n",
    "    response1 = model.generate(\"ç”¨æˆ·é—®é¢˜1\", use_cache=True)\n",
    "    # è¿™é‡Œçš„cacheä¸ä¼šä»ä¸Šä¸€æ¬¡ç»§æ‰¿ï¼\n",
    "    response2 = model.generate(\"ç”¨æˆ·é—®é¢˜2\", use_cache=True)  \n",
    "```\n",
    "\n",
    "### å…³é”®è¦ç‚¹ï¼š\n",
    "1. **KV Cacheæ˜¯å•æ¬¡æ¨ç†å†…çš„ä¼˜åŒ–**ï¼Œä¸æ˜¯è·¨æ¨ç†çš„ç¼“å­˜\n",
    "2. **æ¯æ¬¡è°ƒç”¨model.generate()éƒ½ä¼šé‡æ–°å¼€å§‹**\n",
    "3. **è¦åœ¨å¤šè½®å¯¹è¯ä¸­ä½¿ç”¨ï¼Œéœ€è¦æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28674584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_comparison(prompts, max_new_tokens=50, num_runs=3):\n",
    "    \"\"\"å¯¹æ¯”KV Cacheæ€§èƒ½\"\"\"\n",
    "    results = {\n",
    "        'without_cache': {'times': [], 'tokens': []},\n",
    "        'with_cache': {'times': [], 'tokens': []}\n",
    "    }\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"æµ‹è¯•æç¤º: {prompt[:30]}...\")\n",
    "        \n",
    "        # æµ‹è¯•æ— KV Cache\n",
    "        for _ in range(num_runs):\n",
    "            gc.collect()  # æ¸…ç†å†…å­˜\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            result = generate_without_kv_cache(model, tokenizer, prompt, max_new_tokens)\n",
    "            results['without_cache']['times'].append(result['time_taken'])\n",
    "            results['without_cache']['tokens'].append(result['tokens_generated'])\n",
    "        \n",
    "        # æµ‹è¯•ä½¿ç”¨KV Cache\n",
    "        for _ in range(num_runs):\n",
    "            gc.collect()  # æ¸…ç†å†…å­˜\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            result = generate_with_kv_cache(model, tokenizer, prompt, max_new_tokens)\n",
    "            results['with_cache']['times'].append(result['time_taken'])\n",
    "            results['with_cache']['tokens'].append(result['tokens_generated'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æç¤º\n",
    "test_prompts = [\n",
    "    \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ\",\n",
    "    \"è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†å¯ä»¥è§£é‡Šä¸ºä»€ä¹ˆï¼Ÿ\"\n",
    "]\n",
    "\n",
    "print(\"å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...\")\n",
    "benchmark_results = benchmark_comparison(test_prompts, max_new_tokens=40, num_runs=2)\n",
    "\n",
    "# è®¡ç®—å¹³å‡æ€§èƒ½\n",
    "avg_time_no_cache = np.mean(benchmark_results['without_cache']['times'])\n",
    "avg_time_with_cache = np.mean(benchmark_results['with_cache']['times'])\n",
    "speedup = avg_time_no_cache / avg_time_with_cache\n",
    "\n",
    "print(f\"\\n=== æ€§èƒ½å¯¹æ¯”ç»“æœ ===\")\n",
    "print(f\"æ— KV Cacheå¹³å‡è€—æ—¶: {avg_time_no_cache:.3f}ç§’\")\n",
    "print(f\"ä½¿ç”¨KV Cacheå¹³å‡è€—æ—¶: {avg_time_with_cache:.3f}ç§’\")\n",
    "print(f\"åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "print(f\"æ€§èƒ½æå‡: {(speedup-1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ef4e6",
   "metadata": {},
   "source": [
    "## 5. å¯è§†åŒ–æ€§èƒ½å¯¹æ¯”\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºå›¾è¡¨æ¥ç›´è§‚å±•ç¤ºKV Cacheçš„æ€§èƒ½ä¼˜åŠ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae33c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# è€—æ—¶å¯¹æ¯”\n",
    "methods = ['æ— KV Cache', 'ä½¿ç”¨KV Cache']\n",
    "times = [avg_time_no_cache, avg_time_with_cache]\n",
    "colors = ['red', 'green']\n",
    "\n",
    "bars1 = ax1.bar(methods, times, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('å¹³å‡è€—æ—¶ (ç§’)')\n",
    "ax1.set_title('æ¨ç†è€—æ—¶å¯¹æ¯”')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, time in zip(bars1, times):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{time:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "# åŠ é€Ÿæ¯”å¯è§†åŒ–\n",
    "speedup_data = [1.0, speedup]\n",
    "bars2 = ax2.bar(methods, speedup_data, color=['red', 'green'], alpha=0.7)\n",
    "ax2.set_ylabel('åŠ é€Ÿæ¯”')\n",
    "ax2.set_title('KV CacheåŠ é€Ÿæ•ˆæœ')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, speed in zip(bars2, speedup_data):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{speed:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n",
    "print(f\"\\n=== è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===\")\n",
    "print(f\"æµ‹è¯•æ¬¡æ•°: {len(benchmark_results['without_cache']['times'])}\")\n",
    "print(f\"æ— KV Cache - æœ€å°è€—æ—¶: {min(benchmark_results['without_cache']['times']):.3f}s\")\n",
    "print(f\"æ— KV Cache - æœ€å¤§è€—æ—¶: {max(benchmark_results['without_cache']['times']):.3f}s\")\n",
    "print(f\"ä½¿ç”¨KV Cache - æœ€å°è€—æ—¶: {min(benchmark_results['with_cache']['times']):.3f}s\")\n",
    "print(f\"ä½¿ç”¨KV Cache - æœ€å¤§è€—æ—¶: {max(benchmark_results['with_cache']['times']):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935320e0",
   "metadata": {},
   "source": [
    "## 6. æ·±å…¥ç†è§£KV Cacheæœºåˆ¶\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„KV Cacheç¤ºä¾‹æ¥ç†è§£å…¶å·¥ä½œåŸç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKVCache:\n",
    "    \"\"\"ç®€åŒ–çš„KV Cacheå®ç°ï¼Œç”¨äºæ•™å­¦ç›®çš„\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.keys = []      # å­˜å‚¨KeyçŸ©é˜µ\n",
    "        self.values = []    # å­˜å‚¨ValueçŸ©é˜µ\n",
    "        self.seq_len = 0    # å½“å‰åºåˆ—é•¿åº¦\n",
    "    \n",
    "    def update(self, new_key, new_value):\n",
    "        \"\"\"æ·»åŠ æ–°çš„key-valueå¯¹\"\"\"\n",
    "        self.keys.append(new_key)\n",
    "        self.values.append(new_value)\n",
    "        self.seq_len += 1\n",
    "        \n",
    "    def get_all_keys(self):\n",
    "        \"\"\"è·å–æ‰€æœ‰keysï¼ˆç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼‰\"\"\"\n",
    "        if not self.keys:\n",
    "            return None\n",
    "        return torch.cat(self.keys, dim=1)  # å‡è®¾dim=1æ˜¯åºåˆ—ç»´åº¦\n",
    "    \n",
    "    def get_all_values(self):\n",
    "        \"\"\"è·å–æ‰€æœ‰valuesï¼ˆç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼‰\"\"\"\n",
    "        if not self.values:\n",
    "            return None\n",
    "        return torch.cat(self.values, dim=1)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"æ¸…ç©ºç¼“å­˜\"\"\"\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "        self.seq_len = 0\n",
    "\n",
    "def demonstrate_kv_cache_concept():\n",
    "    \"\"\"æ¼”ç¤ºKV Cacheçš„åŸºæœ¬æ¦‚å¿µ\"\"\"\n",
    "    \n",
    "    print(\"=== KV Cacheå·¥ä½œåŸç†æ¼”ç¤º ===\\n\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹\n",
    "    cache = SimpleKVCache()\n",
    "    vocab_size = 1000\n",
    "    hidden_dim = 64\n",
    "    \n",
    "    print(\"æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„KV Cacheä½¿ç”¨ï¼š\")\n",
    "    print(\"å‡è®¾æˆ‘ä»¬è¦ç”Ÿæˆå¥å­ï¼š'äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£'\\n\")\n",
    "    \n",
    "    tokens = [\"äººå·¥\", \"æ™ºèƒ½\", \"å¾ˆ\", \"æœ‰è¶£\"]\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"æ­¥éª¤ {i+1}: ç”Ÿæˆtoken '{token}'\")\n",
    "        \n",
    "        # æ¨¡æ‹Ÿå½“å‰tokençš„keyå’Œvalueè®¡ç®—\n",
    "        current_key = torch.randn(1, 1, hidden_dim)    # [batch, seq_len=1, hidden]\n",
    "        current_value = torch.randn(1, 1, hidden_dim)  # [batch, seq_len=1, hidden]\n",
    "        \n",
    "        print(f\"  - è®¡ç®—å½“å‰tokençš„key shape: {current_key.shape}\")\n",
    "        print(f\"  - è®¡ç®—å½“å‰tokençš„value shape: {current_value.shape}\")\n",
    "        \n",
    "        # æ›´æ–°ç¼“å­˜\n",
    "        cache.update(current_key, current_value)\n",
    "        \n",
    "        # è·å–æ‰€æœ‰å†å²keyså’Œvaluesè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—\n",
    "        all_keys = cache.get_all_keys()\n",
    "        all_values = cache.get_all_values()\n",
    "        \n",
    "        print(f\"  - ç¼“å­˜ä¸­æ€»key shape: {all_keys.shape if all_keys is not None else 'None'}\")\n",
    "        print(f\"  - ç¼“å­˜ä¸­æ€»value shape: {all_values.shape if all_values is not None else 'None'}\")\n",
    "        print(f\"  - å½“å‰åºåˆ—é•¿åº¦: {cache.seq_len}\")\n",
    "        \n",
    "        if i < len(tokens) - 1:\n",
    "            print(\"  - âœ… å°†key-valueç¼“å­˜ï¼Œç”¨äºä¸‹ä¸€æ­¥è®¡ç®—\\n\")\n",
    "        else:\n",
    "            print(\"  - ğŸ‰ ç”Ÿæˆå®Œæˆï¼\\n\")\n",
    "    \n",
    "    print(\"=== KV Cacheçš„ä¼˜åŠ¿ ===\")\n",
    "    print(\"âœ… é¿å…é‡å¤è®¡ç®—ä¹‹å‰tokençš„key-value\")\n",
    "    print(\"âœ… æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œç‰¹åˆ«æ˜¯é•¿åºåˆ—ç”Ÿæˆ\")\n",
    "    print(\"âœ… é™ä½å†…å­˜è®¿é—®ï¼Œæé«˜æ¨ç†é€Ÿåº¦\")\n",
    "    print(\"âŒ éœ€è¦é¢å¤–å†…å­˜å­˜å‚¨ç¼“å­˜çš„key-value\")\n",
    "\n",
    "# è¿è¡Œæ¼”ç¤º\n",
    "demonstrate_kv_cache_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2d44f",
   "metadata": {},
   "source": [
    "## 7. å†…å­˜ä½¿ç”¨åˆ†æ\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ†æKV Cacheå¯¹å†…å­˜ä½¿ç”¨çš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1293c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage():\n",
    "    \"\"\"åˆ†æKV Cacheçš„å†…å­˜ä½¿ç”¨\"\"\"\n",
    "    \n",
    "    print(\"=== KV Cacheå†…å­˜ä½¿ç”¨åˆ†æ ===\\n\")\n",
    "    \n",
    "    # æ¨¡å‹å‚æ•°\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    hidden_size = model.config.hidden_size\n",
    "    num_attention_heads = model.config.num_attention_heads\n",
    "    head_dim = hidden_size // num_attention_heads\n",
    "    \n",
    "    print(f\"æ¨¡å‹é…ç½®:\")\n",
    "    print(f\"  - å±‚æ•°: {num_layers}\")\n",
    "    print(f\"  - éšè—ç»´åº¦: {hidden_size}\")\n",
    "    print(f\"  - æ³¨æ„åŠ›å¤´æ•°: {num_attention_heads}\")\n",
    "    print(f\"  - æ¯ä¸ªå¤´çš„ç»´åº¦: {head_dim}\")\n",
    "    \n",
    "    # è®¡ç®—ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„KV Cacheå†…å­˜ä½¿ç”¨\n",
    "    seq_lengths = [50, 100, 200, 500, 1000, 2048]\n",
    "    batch_size = 1\n",
    "    \n",
    "    print(f\"\\nKV Cacheå†…å­˜ä½¿ç”¨ (batch_size={batch_size}):\")\n",
    "    print(\"åºåˆ—é•¿åº¦ | KV Cacheå¤§å° | æ€»å†…å­˜ (MB)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    memory_usage = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # æ¯å±‚çš„KVç¼“å­˜å¤§å°ï¼š2 (K+V) * batch_size * seq_len * hidden_size * sizeof(float16)\n",
    "        kv_cache_size_bytes = 2 * batch_size * seq_len * hidden_size * num_layers * 2  # 2 bytes for float16\n",
    "        kv_cache_size_mb = kv_cache_size_bytes / (1024 * 1024)\n",
    "        \n",
    "        memory_usage.append(kv_cache_size_mb)\n",
    "        \n",
    "        print(f\"{seq_len:8d} | {kv_cache_size_mb:10.2f} MB | {kv_cache_size_mb:8.1f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(seq_lengths, memory_usage, marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('åºåˆ—é•¿åº¦')\n",
    "    plt.ylabel('KV Cacheå†…å­˜ä½¿ç”¨ (MB)')\n",
    "    plt.title('KV Cacheå†…å­˜ä½¿ç”¨éšåºåˆ—é•¿åº¦å˜åŒ–')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦\n",
    "    \n",
    "    # æ·»åŠ æ•°æ®æ ‡ç­¾\n",
    "    for x, y in zip(seq_lengths, memory_usage):\n",
    "        plt.annotate(f'{y:.1f}MB', (x, y), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ å†…å­˜ä½¿ç”¨éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿\")\n",
    "    print(f\"ğŸ’¡ å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œéœ€è¦è€ƒè™‘å†…å­˜é™åˆ¶\")\n",
    "\n",
    "analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cae57a",
   "metadata": {},
   "source": [
    "## 8. å®é™…åº”ç”¨å»ºè®®\n",
    "\n",
    "åŸºäºæˆ‘ä»¬çš„æµ‹è¯•ç»“æœï¼Œè¿™é‡Œæ˜¯ä¸€äº›ä½¿ç”¨KV Cacheçš„å®é™…å»ºè®®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f68cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def practical_recommendations():\n",
    "    \"\"\"æä¾›å®é™…åº”ç”¨å»ºè®®\"\"\"\n",
    "    \n",
    "    print(\"=== KV Cacheå®é™…åº”ç”¨å»ºè®® ===\\n\")\n",
    "    \n",
    "    recommendations = {\n",
    "        \"ä½•æ—¶ä½¿ç”¨KV Cache\": [\n",
    "            \"âœ… æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆèŠå¤©æœºå™¨äººã€åˆ›æ„å†™ä½œï¼‰\",\n",
    "            \"âœ… é•¿åºåˆ—ç”Ÿæˆï¼ˆè¶…è¿‡50ä¸ªtokenï¼‰\",\n",
    "            \"âœ… å®æ—¶äº¤äº’åº”ç”¨\",\n",
    "            \"âœ… èµ„æºå—é™çš„æ¨ç†ç¯å¢ƒ\"\n",
    "        ],\n",
    "        \n",
    "        \"ä½•æ—¶ä¸ä½¿ç”¨KV Cache\": [\n",
    "            \"âŒ å•æ¬¡çŸ­æ–‡æœ¬åˆ†ç±»\",\n",
    "            \"âŒ æ‰¹é‡å¤„ç†å¤§é‡çŸ­æ–‡æœ¬\",\n",
    "            \"âŒ å†…å­˜ä¸¥é‡å—é™çš„ç¯å¢ƒ\",\n",
    "            \"âŒ éœ€è¦å®Œå…¨ç¡®å®šæ€§ç»“æœçš„åœºæ™¯\"\n",
    "        ],\n",
    "        \n",
    "        \"ä¼˜åŒ–æŠ€å·§\": [\n",
    "            \"ğŸ”§ ä½¿ç”¨float16å‡å°‘å†…å­˜å ç”¨\",\n",
    "            \"ğŸ”§ è®¾ç½®åˆç†çš„max_lengthé¿å…æ— é™ç”Ÿæˆ\",\n",
    "            \"ğŸ”§ å®šæœŸæ¸…ç†KV Cacheé‡Šæ”¾å†…å­˜\",\n",
    "            \"ğŸ”§ è€ƒè™‘ä½¿ç”¨sliding window attention\"\n",
    "        ],\n",
    "        \n",
    "        \"æ€§èƒ½ç›‘æ§\": [\n",
    "            \"ğŸ“Š ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ\",\n",
    "            \"ğŸ“Š æµ‹é‡æ¨ç†å»¶è¿Ÿ\",\n",
    "            \"ğŸ“Š è·Ÿè¸ªååé‡å˜åŒ–\",\n",
    "            \"ğŸ“Š è§‚å¯ŸGPUåˆ©ç”¨ç‡\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in recommendations.items():\n",
    "        print(f\"{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "        print()\n",
    "\n",
    "practical_recommendations()\n",
    "\n",
    "# æœ€ç»ˆæµ‹è¯•ï¼šå±•ç¤ºå®é™…ä½¿ç”¨åœºæ™¯\n",
    "print(\"=== å®é™…ä½¿ç”¨åœºæ™¯æ¼”ç¤º ===\")\n",
    "\n",
    "conversation_prompts = [\n",
    "    \"ç”¨æˆ·ï¼šä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚\\nåŠ©æ‰‹ï¼š\",\n",
    "    \"ç”¨æˆ·ï¼šäººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ\\nåŠ©æ‰‹ï¼š\",\n",
    "    \"ç”¨æˆ·ï¼šæœªæ¥äººå·¥æ™ºèƒ½ä¼šå¦‚ä½•å‘å±•ï¼Ÿ\\nåŠ©æ‰‹ï¼š\"\n",
    "]\n",
    "\n",
    "print(\"æ¨¡æ‹Ÿå¤šè½®å¯¹è¯åœºæ™¯ï¼š\")\n",
    "for i, prompt in enumerate(conversation_prompts, 1):\n",
    "    print(f\"\\nç¬¬{i}è½®å¯¹è¯:\")\n",
    "    result = generate_with_kv_cache(model, tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"è¾“å…¥: {prompt.split('åŠ©æ‰‹ï¼š')[0]}...\")\n",
    "    print(f\"å›å¤: {result['generated_text']}\")\n",
    "    print(f\"è€—æ—¶: {result['time_taken']:.2f}ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658abe3",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š\n",
    "\n",
    "1. **KV Cacheçš„åŸºæœ¬æ¦‚å¿µ**ï¼šç¼“å­˜Key-ValueçŸ©é˜µé¿å…é‡å¤è®¡ç®—\n",
    "2. **æ€§èƒ½ä¼˜åŠ¿**ï¼šæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯é•¿åºåˆ—ç”Ÿæˆ\n",
    "3. **å†…å­˜æƒè¡¡**ï¼šéœ€è¦é¢å¤–å†…å­˜å­˜å‚¨ç¼“å­˜æ•°æ®\n",
    "4. **å®é™…åº”ç”¨**ï¼šé€‚åˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿç­‰åœºæ™¯\n",
    "\n",
    "### å…³é”®æ”¶è·ï¼š\n",
    "- KV Cacheæ˜¯ç°ä»£Transformeræ¨ç†çš„æ ‡å‡†ä¼˜åŒ–æŠ€æœ¯\n",
    "- æ€§èƒ½æå‡æ•ˆæœéšåºåˆ—é•¿åº¦å¢åŠ è€Œæ›´æ˜æ˜¾\n",
    "- éœ€è¦åœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´æ‰¾åˆ°å¹³è¡¡\n",
    "- transformersåº“é»˜è®¤æ”¯æŒKV Cacheï¼Œä½¿ç”¨ç®€å•\n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š\n",
    "- æ¢ç´¢å…¶ä»–æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚speculative decodingï¼‰\n",
    "- å­¦ä¹ æ¨¡å‹é‡åŒ–å’Œå‰ªæ\n",
    "- äº†è§£åˆ†å¸ƒå¼æ¨ç†æŠ€æœ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ace3d7",
   "metadata": {},
   "source": [
    "# KV Cache å·¥ä½œåŸç†æ·±åº¦è§£æ\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦KV Cacheï¼Ÿ\n",
    "\n",
    "åœ¨ç†è§£KV Cacheä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹Transformeråœ¨æ–‡æœ¬ç”Ÿæˆæ—¶é‡åˆ°çš„é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def demonstrate_attention_computation():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹ï¼Œå±•ç¤ºä¸ºä»€ä¹ˆéœ€è¦KV Cache\n",
    "    \"\"\"\n",
    "    print(\"=== æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ¼”ç¤º ===\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå‚æ•°\n",
    "    seq_len = 5  # å½“å‰åºåˆ—é•¿åº¦\n",
    "    d_model = 8  # éšè—ç»´åº¦\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè¾“å…¥åºåˆ—çš„embedding\n",
    "    # å‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†5ä¸ªtoken: [\"æˆ‘\", \"çˆ±\", \"äººå·¥\", \"æ™ºèƒ½\", \"æŠ€æœ¯\"]\n",
    "    input_embeddings = torch.randn(1, seq_len, d_model)\n",
    "    print(f\"è¾“å…¥åºåˆ—embedding shape: {input_embeddings.shape}\")\n",
    "    print(f\"è¡¨ç¤º: ['æˆ‘', 'çˆ±', 'äººå·¥', 'æ™ºèƒ½', 'æŠ€æœ¯']\")\n",
    "    \n",
    "    # çº¿æ€§å˜æ¢å±‚ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "    W_q = torch.randn(d_model, d_model)  # Queryæƒé‡\n",
    "    W_k = torch.randn(d_model, d_model)  # Keyæƒé‡  \n",
    "    W_v = torch.randn(d_model, d_model)  # Valueæƒé‡\n",
    "    \n",
    "    # è®¡ç®—Q, K, V\n",
    "    Q = torch.matmul(input_embeddings, W_q)  # [1, seq_len, d_model]\n",
    "    K = torch.matmul(input_embeddings, W_k)  # [1, seq_len, d_model]\n",
    "    V = torch.matmul(input_embeddings, W_v)  # [1, seq_len, d_model]\n",
    "    \n",
    "    print(f\"\\nQuery (Q) shape: {Q.shape}\")\n",
    "    print(f\"Key (K) shape: {K.shape}\")\n",
    "    print(f\"Value (V) shape: {V.shape}\")\n",
    "    \n",
    "    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "    print(f\"\\næ³¨æ„åŠ›åˆ†æ•° shape: {attention_scores.shape}\")\n",
    "    print(f\"æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ:\")\n",
    "    print(attention_scores[0].detach().numpy().round(2))\n",
    "    \n",
    "    # åº”ç”¨softmax\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    print(f\"\\næ³¨æ„åŠ›æƒé‡ (softmaxå):\")\n",
    "    print(attention_weights[0].detach().numpy().round(3))\n",
    "    \n",
    "    # è®¡ç®—æœ€ç»ˆè¾“å‡º\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    print(f\"\\næœ€ç»ˆè¾“å‡º shape: {output.shape}\")\n",
    "    \n",
    "    return Q, K, V, attention_weights, output\n",
    "\n",
    "# è¿è¡Œæ¼”ç¤º\n",
    "Q, K, V, attention_weights, output = demonstrate_attention_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb099447",
   "metadata": {},
   "source": [
    "## è‡ªå›å½’ç”Ÿæˆçš„é‡å¤è®¡ç®—é—®é¢˜\n",
    "\n",
    "åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°tokenï¼Œæˆ‘ä»¬éƒ½éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªè¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c466c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_autoregressive_problem():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºè‡ªå›å½’ç”Ÿæˆä¸­çš„é‡å¤è®¡ç®—é—®é¢˜\n",
    "    \"\"\"\n",
    "    print(\"=== è‡ªå›å½’ç”Ÿæˆçš„é‡å¤è®¡ç®—é—®é¢˜ ===\")\n",
    "    \n",
    "    d_model = 8\n",
    "    W_q = torch.randn(d_model, d_model)\n",
    "    W_k = torch.randn(d_model, d_model) \n",
    "    W_v = torch.randn(d_model, d_model)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹\n",
    "    tokens = [\"æˆ‘\", \"çˆ±\", \"äººå·¥\", \"æ™ºèƒ½\"]\n",
    "    \n",
    "    for step in range(1, len(tokens) + 1):\n",
    "        print(f\"\\n--- ç¬¬{step}æ­¥ï¼šç”Ÿæˆåˆ° {tokens[:step]} ---\")\n",
    "        \n",
    "        # å½“å‰åºåˆ—çš„embedding\n",
    "        current_embeddings = torch.randn(1, step, d_model)\n",
    "        print(f\"å½“å‰åºåˆ—é•¿åº¦: {step}\")\n",
    "        \n",
    "        # âŒ é—®é¢˜ï¼šæ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„Kå’ŒV\n",
    "        Q = torch.matmul(current_embeddings, W_q)\n",
    "        K = torch.matmul(current_embeddings, W_k)  # é‡å¤è®¡ç®—ï¼\n",
    "        V = torch.matmul(current_embeddings, W_v)  # é‡å¤è®¡ç®—ï¼\n",
    "        \n",
    "        print(f\"é‡æ–°è®¡ç®—äº† {step} ä¸ªtokençš„Keyå’ŒValue\")\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        if step > 1:\n",
    "            print(f\"ğŸ’¡ æ³¨æ„ï¼šå‰{step-1}ä¸ªtokençš„Kå’ŒVå…¶å®åœ¨ä¸Šä¸€æ­¥å·²ç»è®¡ç®—è¿‡äº†ï¼\")\n",
    "    \n",
    "    print(\"\\nğŸ”´ é—®é¢˜æ€»ç»“ï¼š\")\n",
    "    print(\"- æ¯ä¸€æ­¥éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰å†å²tokençš„Keyå’ŒValue\")\n",
    "    print(\"- éšç€åºåˆ—å˜é•¿ï¼Œé‡å¤è®¡ç®—é‡æ€¥å‰§å¢åŠ \")\n",
    "    print(\"- ç”Ÿæˆ100ä¸ªtokenéœ€è¦è®¡ç®— 1+2+3+...+100 = 5050æ¬¡Key/Valueè®¡ç®—\")\n",
    "    print(\"- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé•¿æ–‡æœ¬ç”Ÿæˆå¾ˆæ…¢çš„åŸå› ï¼\")\n",
    "\n",
    "demonstrate_autoregressive_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976adc8",
   "metadata": {},
   "source": [
    "## KV Cacheçš„è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "KV Cacheçš„æ ¸å¿ƒæ€æƒ³ï¼š**æ—¢ç„¶ä¹‹å‰è®¡ç®—è¿‡çš„Keyå’ŒValueä¸ä¼šæ”¹å˜ï¼Œä¸ºä»€ä¹ˆä¸æŠŠå®ƒä»¬å­˜èµ·æ¥å‘¢ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df29159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kv_cache_solution():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºKV Cacheå¦‚ä½•è§£å†³é‡å¤è®¡ç®—é—®é¢˜\n",
    "    \"\"\"\n",
    "    print(\"=== KV Cache è§£å†³æ–¹æ¡ˆæ¼”ç¤º ===\")\n",
    "    \n",
    "    d_model = 8\n",
    "    W_q = torch.randn(d_model, d_model)\n",
    "    W_k = torch.randn(d_model, d_model)\n",
    "    W_v = torch.randn(d_model, d_model)\n",
    "    \n",
    "    # KV Cacheå­˜å‚¨\n",
    "    cached_keys = []    # å­˜å‚¨æ‰€æœ‰å†å²çš„Key\n",
    "    cached_values = []  # å­˜å‚¨æ‰€æœ‰å†å²çš„Value\n",
    "    \n",
    "    tokens = [\"æˆ‘\", \"çˆ±\", \"äººå·¥\", \"æ™ºèƒ½\"]\n",
    "    \n",
    "    for step in range(1, len(tokens) + 1):\n",
    "        print(f\"\\n--- ç¬¬{step}æ­¥ï¼šç”Ÿæˆåˆ° {tokens[:step]} ---\")\n",
    "        \n",
    "        if step == 1:\n",
    "            # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç¬¬ä¸€ä¸ªtoken\n",
    "            print(\"ğŸ†• ç¬¬ä¸€ä¸ªtokenï¼Œéœ€è¦è®¡ç®—Kå’ŒV\")\n",
    "            current_embedding = torch.randn(1, 1, d_model)  # åªæœ‰ä¸€ä¸ªtoken\n",
    "            \n",
    "            Q = torch.matmul(current_embedding, W_q)\n",
    "            K = torch.matmul(current_embedding, W_k)\n",
    "            V = torch.matmul(current_embedding, W_v)\n",
    "            \n",
    "            # å­˜å…¥ç¼“å­˜\n",
    "            cached_keys.append(K)\n",
    "            cached_values.append(V)\n",
    "            \n",
    "        else:\n",
    "            # åç»­æ­¥éª¤ï¼šåªè®¡ç®—æ–°tokençš„Kå’ŒV\n",
    "            print(f\"ğŸ”„ åªéœ€è®¡ç®—æ–°tokençš„Kå’ŒVï¼Œå¤ç”¨ç¼“å­˜ä¸­çš„{step-1}ä¸ª\")\n",
    "            \n",
    "            # æ–°tokençš„embedding\n",
    "            new_token_embedding = torch.randn(1, 1, d_model)\n",
    "            \n",
    "            # âœ… åªè®¡ç®—æ–°tokençš„Kå’ŒV\n",
    "            Q_new = torch.matmul(new_token_embedding, W_q)\n",
    "            K_new = torch.matmul(new_token_embedding, W_k) \n",
    "            V_new = torch.matmul(new_token_embedding, W_v)\n",
    "            \n",
    "            # æ·»åŠ åˆ°ç¼“å­˜\n",
    "            cached_keys.append(K_new)\n",
    "            cached_values.append(V_new)\n",
    "            \n",
    "            # æ„å»ºå®Œæ•´çš„Q (åªæœ‰æœ€åä¸€ä¸ªtokençš„Query)\n",
    "            Q = Q_new\n",
    "        \n",
    "        # ä»ç¼“å­˜ä¸­è·å–æ‰€æœ‰Kå’ŒV\n",
    "        all_K = torch.cat(cached_keys, dim=1)  # [1, current_length, d_model]\n",
    "        all_V = torch.cat(cached_values, dim=1)  # [1, current_length, d_model]\n",
    "        \n",
    "        print(f\"ğŸ“¦ ç¼“å­˜çŠ¶æ€ï¼š\")\n",
    "        print(f\"   - ç¼“å­˜çš„Keyæ•°é‡: {len(cached_keys)}\")\n",
    "        print(f\"   - ç¼“å­˜çš„Valueæ•°é‡: {len(cached_values)}\")\n",
    "        print(f\"   - æ€»K shape: {all_K.shape}\")\n",
    "        print(f\"   - æ€»V shape: {all_V.shape}\")\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆä½¿ç”¨ç¼“å­˜çš„Kå’ŒVï¼‰\n",
    "        attention_scores = torch.matmul(Q, all_K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, all_V)\n",
    "        \n",
    "        print(f\"âœ… æ³¨æ„åŠ›è®¡ç®—å®Œæˆï¼Œè¾“å‡ºshape: {output.shape}\")\n",
    "    \n",
    "    print(\"\\nğŸŸ¢ KV Cacheä¼˜åŠ¿æ€»ç»“ï¼š\")\n",
    "    print(\"- æ¯ä¸ªtokençš„Kå’ŒVåªè®¡ç®—ä¸€æ¬¡\")\n",
    "    print(\"- åç»­æ­¥éª¤ç›´æ¥ä»ç¼“å­˜è¯»å–\")\n",
    "    print(\"- ç”Ÿæˆ100ä¸ªtokenåªéœ€è¦100æ¬¡K/Vè®¡ç®—ï¼ˆè€Œä¸æ˜¯5050æ¬¡ï¼‰\")\n",
    "    print(\"- è®¡ç®—é‡ä»O(nÂ²)é™ä½åˆ°O(n)\")\n",
    "\n",
    "demonstrate_kv_cache_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7bdc8",
   "metadata": {},
   "source": [
    "## KV Cacheçš„å†…å­˜ç»“æ„å¯è§†åŒ–\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨å›¾è¡¨æ¥ç›´è§‚ç†è§£KV Cacheåœ¨å†…å­˜ä¸­æ˜¯å¦‚ä½•ç»„ç»‡çš„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def visualize_kv_cache_structure():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–KV Cacheçš„å†…å­˜ç»“æ„\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # === ä¸Šå›¾ï¼šä¼ ç»Ÿæ–¹æ³• vs KV Cacheæ–¹æ³•çš„è®¡ç®—å¯¹æ¯” ===\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 6)\n",
    "    ax1.set_title('ä¼ ç»Ÿæ–¹æ³• vs KV Cache è®¡ç®—å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # ä¼ ç»Ÿæ–¹æ³•\n",
    "    ax1.text(1, 5, 'ä¼ ç»Ÿæ–¹æ³•ï¼ˆæ¯æ­¥é‡æ–°è®¡ç®—ï¼‰:', fontsize=12, fontweight='bold', color='red')\n",
    "    \n",
    "    steps = ['æ­¥éª¤1', 'æ­¥éª¤2', 'æ­¥éª¤3', 'æ­¥éª¤4']\n",
    "    colors_traditional = ['lightcoral', 'lightcoral', 'lightcoral', 'lightcoral']\n",
    "    \n",
    "    for i, (step, color) in enumerate(zip(steps, colors_traditional)):\n",
    "        # ç»˜åˆ¶é‡æ–°è®¡ç®—çš„åŒºåŸŸ\n",
    "        rect = patches.Rectangle((0.5 + i * 2, 3.5), 1.5, 0.8, \n",
    "                               linewidth=1, edgecolor='red', facecolor=color, alpha=0.7)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(1.25 + i * 2, 3.9, step, ha='center', va='center', fontsize=10)\n",
    "        ax1.text(1.25 + i * 2, 3.2, f'è®¡ç®—{i+1}ä¸ª\\nK,V', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # KV Cacheæ–¹æ³•\n",
    "    ax1.text(1, 2.5, 'KV Cacheæ–¹æ³•ï¼ˆç¼“å­˜å¤ç”¨ï¼‰:', fontsize=12, fontweight='bold', color='green')\n",
    "    \n",
    "    colors_cache = ['lightgreen', 'lightblue', 'lightblue', 'lightblue']\n",
    "    labels_cache = ['è®¡ç®—1ä¸ª\\nK,V', 'ç¼“å­˜+è®¡ç®—\\n1ä¸ªK,V', 'ç¼“å­˜+è®¡ç®—\\n1ä¸ªK,V', 'ç¼“å­˜+è®¡ç®—\\n1ä¸ªK,V']\n",
    "    \n",
    "    for i, (step, color, label) in enumerate(zip(steps, colors_cache, labels_cache)):\n",
    "        rect = patches.Rectangle((0.5 + i * 2, 1), 1.5, 0.8, \n",
    "                               linewidth=1, edgecolor='green', facecolor=color, alpha=0.7)\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(1.25 + i * 2, 1.4, step, ha='center', va='center', fontsize=10)\n",
    "        ax1.text(1.25 + i * 2, 0.7, label, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    \n",
    "    # === ä¸‹å›¾ï¼šKV Cacheçš„å…·ä½“å†…å­˜å¸ƒå±€ ===\n",
    "    ax2.set_xlim(0, 12)\n",
    "    ax2.set_ylim(0, 8)\n",
    "    ax2.set_title('KV Cache å†…å­˜ç»“æ„ç¤ºæ„å›¾', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # ç»˜åˆ¶å±‚çº§ç»“æ„\n",
    "    layer_colors = ['lightblue', 'lightgreen', 'lightyellow']\n",
    "    layer_names = ['Layer 1', 'Layer 2', 'Layer 3']\n",
    "    \n",
    "    for layer_idx, (color, name) in enumerate(zip(layer_colors, layer_names)):\n",
    "        y_base = 6 - layer_idx * 2\n",
    "        \n",
    "        # å±‚æ ‡ç­¾\n",
    "        ax2.text(0.5, y_base - 0.5, name, fontsize=11, fontweight='bold', \n",
    "                rotation=90, va='center', ha='center')\n",
    "        \n",
    "        # Key Cache\n",
    "        key_rect = patches.Rectangle((1, y_base - 0.8), 4, 0.6, \n",
    "                                   linewidth=1, edgecolor='blue', facecolor=color, alpha=0.8)\n",
    "        ax2.add_patch(key_rect)\n",
    "        ax2.text(3, y_base - 0.5, 'Key Cache', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Value Cache  \n",
    "        value_rect = patches.Rectangle((6, y_base - 0.8), 4, 0.6,\n",
    "                                     linewidth=1, edgecolor='purple', facecolor=color, alpha=0.8)\n",
    "        ax2.add_patch(value_rect)\n",
    "        ax2.text(8, y_base - 0.5, 'Value Cache', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # ç»˜åˆ¶tokenä½ç½®\n",
    "        for token_idx in range(4):\n",
    "            # Key cacheä¸­çš„token\n",
    "            token_rect_k = patches.Rectangle((1.2 + token_idx * 0.9, y_base - 0.75), 0.8, 0.5,\n",
    "                                           linewidth=1, edgecolor='darkblue', facecolor='white', alpha=0.9)\n",
    "            ax2.add_patch(token_rect_k)\n",
    "            ax2.text(1.6 + token_idx * 0.9, y_base - 0.5, f'K{token_idx+1}', ha='center', va='center', fontsize=8)\n",
    "            \n",
    "            # Value cacheä¸­çš„token\n",
    "            token_rect_v = patches.Rectangle((6.2 + token_idx * 0.9, y_base - 0.75), 0.8, 0.5,\n",
    "                                           linewidth=1, edgecolor='darkred', facecolor='white', alpha=0.9)\n",
    "            ax2.add_patch(token_rect_v)\n",
    "            ax2.text(6.6 + token_idx * 0.9, y_base - 0.5, f'V{token_idx+1}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # æ·»åŠ è¯´æ˜\n",
    "    ax2.text(6, 0.5, 'æ¯ä¸€å±‚éƒ½æœ‰ç‹¬ç«‹çš„Keyå’ŒValueç¼“å­˜\\nå­˜å‚¨æ ¼å¼: [batch_size, seq_length, hidden_dim]', \n",
    "            ha='center', va='center', fontsize=10, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False) \n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°è¯¦ç»†è¯´æ˜\n",
    "    print(\"=== KV Cache å†…å­˜ç»“æ„è¯´æ˜ ===\")\n",
    "    print(\"\\nğŸ” å­˜å‚¨æ ¼å¼:\")\n",
    "    print(\"  - past_key_values: List[Tuple[torch.Tensor, torch.Tensor]]\")\n",
    "    print(\"  - æ¯å±‚ä¸€ä¸ªtuple: (key_cache, value_cache)\")\n",
    "    print(\"  - æ¯ä¸ªcacheçš„shape: [batch_size, num_heads, seq_length, head_dim]\")\n",
    "    \n",
    "    print(\"\\nğŸ“ æ›´æ–°æœºåˆ¶:\")\n",
    "    print(\"  1. æ–°tokenè¿›å…¥ â†’ è®¡ç®—æ–°çš„Kå’ŒV\")\n",
    "    print(\"  2. å°†æ–°Kè¿½åŠ åˆ°key_cache\")\n",
    "    print(\"  3. å°†æ–°Vè¿½åŠ åˆ°value_cache\")\n",
    "    print(\"  4. è¿”å›æ›´æ–°åçš„past_key_values\")\n",
    "    \n",
    "    print(\"\\nğŸ’¾ å†…å­˜å ç”¨:\")\n",
    "    print(\"  - æ¯ä¸ªtokenåœ¨æ¯å±‚å ç”¨: 2 Ã— head_dim Ã— num_heads Ã— sizeof(dtype)\")\n",
    "    print(\"  - æ€»å ç”¨: num_layers Ã— seq_length Ã— 2 Ã— hidden_size Ã— sizeof(dtype)\")\n",
    "    print(\"  - éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿\")\n",
    "\n",
    "visualize_kv_cache_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3f340",
   "metadata": {},
   "source": [
    "## KV Cacheä¸æ³¨æ„åŠ›æ©ç çš„å…³ç³»\n",
    "\n",
    "åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å› æœæ©ç (causal mask)ç¡®ä¿å½“å‰tokenåªèƒ½çœ‹åˆ°ä¹‹å‰çš„tokenï¼ŒKV Cacheä¸æ­¤å®Œç¾é…åˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kv_cache_with_attention_mask():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºKV Cacheå¦‚ä½•ä¸æ³¨æ„åŠ›æ©ç é…åˆå·¥ä½œ\n",
    "    \"\"\"\n",
    "    print(\"=== KV Cache ä¸æ³¨æ„åŠ›æ©ç  ===\")\n",
    "    \n",
    "    seq_len = 4\n",
    "    d_model = 6\n",
    "    \n",
    "    print(f\"å‡è®¾æˆ‘ä»¬è¦ç”Ÿæˆåºåˆ—: ['æˆ‘', 'çˆ±', 'äººå·¥', 'æ™ºèƒ½']\")\n",
    "    print(f\"å½“å‰å·²ç»ç”Ÿæˆåˆ°ç¬¬{seq_len}ä¸ªtoken\\n\")\n",
    "    \n",
    "    # åˆ›å»ºå› æœæ©ç \n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    print(\"å› æœæ©ç  (Causal Mask):\")\n",
    "    print(\"1=å¯ä»¥çœ‹åˆ°, 0=ä¸èƒ½çœ‹åˆ°\")\n",
    "    print(causal_mask.numpy().astype(int))\n",
    "    \n",
    "    # è§£é‡Šæ©ç å«ä¹‰\n",
    "    tokens = ['æˆ‘', 'çˆ±', 'äººå·¥', 'æ™ºèƒ½']\n",
    "    print(\"\\næ©ç å«ä¹‰è§£é‡Š:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        visible_tokens = [tokens[j] for j in range(seq_len) if causal_mask[i, j] == 1]\n",
    "        print(f\"  {token} å¯ä»¥çœ‹åˆ°: {visible_tokens}\")\n",
    "    \n",
    "    # æ¨¡æ‹ŸKV Cacheå­˜å‚¨\n",
    "    print(\"\\n=== KV Cache å­˜å‚¨ç»“æ„ ===\")\n",
    "    \n",
    "    # å‡è®¾æˆ‘ä»¬é€æ­¥ç”Ÿæˆï¼Œå±•ç¤ºæ¯ä¸€æ­¥çš„cacheçŠ¶æ€\n",
    "    kv_cache_states = []\n",
    "    \n",
    "    for step in range(1, seq_len + 1):\n",
    "        print(f\"\\næ­¥éª¤ {step}: ç”Ÿæˆ '{tokens[step-1]}'\")\n",
    "        \n",
    "        # å½“å‰æ­¥éª¤çš„Keyå’ŒValue (ç®€åŒ–è¡¨ç¤º)\n",
    "        current_keys = [f\"K_{i+1}\" for i in range(step)]\n",
    "        current_values = [f\"V_{i+1}\" for i in range(step)]\n",
    "        \n",
    "        kv_cache_states.append((current_keys.copy(), current_values.copy()))\n",
    "        \n",
    "        print(f\"  ç¼“å­˜çš„Keys: {current_keys}\")\n",
    "        print(f\"  ç¼“å­˜çš„Values: {current_values}\")\n",
    "        \n",
    "        # å½“å‰tokençš„Queryåªéœ€è¦ä¸æ‰€æœ‰ç¼“å­˜çš„Kè®¡ç®—æ³¨æ„åŠ›\n",
    "        current_query = f\"Q_{step}\"\n",
    "        print(f\"  å½“å‰Query: {current_query}\")\n",
    "        print(f\"  æ³¨æ„åŠ›è®¡ç®—: {current_query} Ã— [\" + \", \".join(current_keys) + \"]\")\n",
    "        \n",
    "        # æ˜¾ç¤ºè¯¥æ­¥éª¤çš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "        if step > 1:\n",
    "            attention_pattern = causal_mask[step-1, :step].numpy()\n",
    "            print(f\"  æ³¨æ„åŠ›æ¨¡å¼: {attention_pattern} (å¯¹åº”å‰{step}ä¸ªposition)\")\n",
    "    \n",
    "    print(\"\\n=== å…³é”®æ´å¯Ÿ ===\")\n",
    "    print(\"ğŸ” KV Cacheçš„å·§å¦™ä¹‹å¤„:\")\n",
    "    print(\"  1. åªå­˜å‚¨å·²ç»ç”Ÿæˆçš„tokençš„Kå’ŒV\")\n",
    "    print(\"  2. æ–°tokençš„Qè‡ªç„¶åªä¸å·²å­˜å‚¨çš„Kè®¡ç®—æ³¨æ„åŠ›\")\n",
    "    print(\"  3. å› æœæ©ç ç¡®ä¿ä¸ä¼š'çœ‹åˆ°æœªæ¥'\")\n",
    "    print(\"  4. æ¯æ¬¡åªéœ€è¦æ·»åŠ æ–°tokençš„Kå’ŒVï¼Œä¸éœ€è¦é‡æ–°è®¡ç®—å†å²\")\n",
    "    \n",
    "    return kv_cache_states\n",
    "\n",
    "kv_cache_states = demonstrate_kv_cache_with_attention_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d2172",
   "metadata": {},
   "source": [
    "## KV Cacheçš„æ•°å­¦åŸç†æ€»ç»“\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨æ•°å­¦å…¬å¼æ¥ç²¾ç¡®æè¿°KV Cacheçš„å·¥ä½œåŸç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mathematical_explanation_of_kv_cache():\n",
    "    \"\"\"\n",
    "    ç”¨æ•°å­¦å…¬å¼å’Œä»£ç å±•ç¤ºKV Cacheçš„åŸç†\n",
    "    \"\"\"\n",
    "    print(\"=== KV Cache æ•°å­¦åŸç† ===\")\n",
    "    \n",
    "    print(\"\\nğŸ“ ä¼ ç»Ÿæ³¨æ„åŠ›è®¡ç®— (æ²¡æœ‰KV Cache):\")\n",
    "    print(\"å¯¹äºåºåˆ—é•¿åº¦ä¸º t çš„ç”Ÿæˆ:\")\n",
    "    print(\"\")\n",
    "    print(\"X_t = [x_1, x_2, ..., x_t]  # è¾“å…¥åºåˆ—\")\n",
    "    print(\"Q_t = X_t @ W_q             # QueryçŸ©é˜µ [t, d_model]\")\n",
    "    print(\"K_t = X_t @ W_k             # KeyçŸ©é˜µ [t, d_model]\")\n",
    "    print(\"V_t = X_t @ W_v             # ValueçŸ©é˜µ [t, d_model]\")\n",
    "    print(\"\")\n",
    "    print(\"Attention_t = softmax(Q_t @ K_t^T / âˆšd_k) @ V_t\")\n",
    "    print(\"\")\n",
    "    print(\"âŒ é—®é¢˜: æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—å®Œæ•´çš„ K_t å’Œ V_t\")\n",
    "    \n",
    "    print(\"\\nğŸš€ KV Cacheä¼˜åŒ–åçš„è®¡ç®—:\")\n",
    "    print(\"\")\n",
    "    print(\"# æ­¥éª¤ 1: åˆå§‹åŒ–\")\n",
    "    print(\"t=1: K_cache = [k_1], V_cache = [v_1]\")\n",
    "    print(\"\")\n",
    "    print(\"# æ­¥éª¤ t: å¢é‡è®¡ç®—\")\n",
    "    print(\"æ–°token: x_t\")\n",
    "    print(\"q_t = x_t @ W_q              # åªè®¡ç®—æ–°tokençš„Query\")\n",
    "    print(\"k_t = x_t @ W_k              # åªè®¡ç®—æ–°tokençš„Key\")\n",
    "    print(\"v_t = x_t @ W_v              # åªè®¡ç®—æ–°tokençš„Value\")\n",
    "    print(\"\")\n",
    "    print(\"# æ›´æ–°ç¼“å­˜\")\n",
    "    print(\"K_cache = concat([K_cache, k_t])  # [t, d_model]\")\n",
    "    print(\"V_cache = concat([V_cache, v_t])  # [t, d_model]\")\n",
    "    print(\"\")\n",
    "    print(\"# è®¡ç®—æ³¨æ„åŠ› (åªéœ€è¦æ–°tokençš„Query)\")\n",
    "    print(\"attention_t = softmax(q_t @ K_cache^T / âˆšd_k) @ V_cache\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š è®¡ç®—å¤æ‚åº¦å¯¹æ¯”:\")\n",
    "    print(\"\")\n",
    "    print(\"ä¼ ç»Ÿæ–¹æ³•:\")\n",
    "    print(\"  - æ¯æ­¥è®¡ç®—é‡: O(t Ã— d_model Ã— d_model)\")\n",
    "    print(\"  - æ€»è®¡ç®—é‡: O(âˆ‘_{i=1}^T i Ã— d_modelÂ²) = O(TÂ² Ã— d_modelÂ²)\")\n",
    "    print(\"\")\n",
    "    print(\"KV Cacheæ–¹æ³•:\")\n",
    "    print(\"  - æ¯æ­¥è®¡ç®—é‡: O(d_model Ã— d_model) + O(t Ã— d_model)\")\n",
    "    print(\"  - æ€»è®¡ç®—é‡: O(T Ã— d_modelÂ²) + O(TÂ² Ã— d_model)\")\n",
    "    print(\"  - å½“d_model >> Tæ—¶ï¼Œä¸»è¦èŠ‚çœåœ¨çŸ©é˜µä¹˜æ³•ä¸Š\")\n",
    "    \n",
    "    # å®é™…æ•°å€¼ç¤ºä¾‹\n",
    "    print(\"\\nğŸ”¢ æ•°å€¼ç¤ºä¾‹:\")\n",
    "    T = 100  # ç”Ÿæˆ100ä¸ªtoken\n",
    "    d_model = 4096  # å…¸å‹çš„æ¨¡å‹ç»´åº¦\n",
    "    \n",
    "    traditional_ops = sum(i * d_model * d_model for i in range(1, T+1))\n",
    "    kv_cache_ops = T * d_model * d_model + sum(i * d_model for i in range(1, T+1))\n",
    "    \n",
    "    print(f\"ç”Ÿæˆ{T}ä¸ªtokenï¼Œæ¨¡å‹ç»´åº¦{d_model}:\")\n",
    "    print(f\"ä¼ ç»Ÿæ–¹æ³•æ“ä½œæ•°: {traditional_ops:,}\")\n",
    "    print(f\"KV Cacheæ“ä½œæ•°: {kv_cache_ops:,}\")\n",
    "    print(f\"èŠ‚çœæ¯”ä¾‹: {(1 - kv_cache_ops/traditional_ops)*100:.1f}%\")\n",
    "    \n",
    "    return traditional_ops, kv_cache_ops\n",
    "\n",
    "traditional_ops, kv_cache_ops = mathematical_explanation_of_kv_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d63ca2",
   "metadata": {},
   "source": [
    "## ğŸ¯ KV Cache åŸç†æ€»ç»“\n",
    "\n",
    "é€šè¿‡ä»¥ä¸Šè¯¦ç»†è§£æï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°ç†è§£KV Cacheçš„å·¥ä½œåŸç†ï¼š\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "1. **å‘ç°é—®é¢˜**ï¼šè‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯æ­¥éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰å†å²tokençš„Keyå’ŒValue\n",
    "2. **å…³é”®æ´å¯Ÿ**ï¼šå†å²tokençš„Keyå’ŒValueåœ¨åç»­æ­¥éª¤ä¸­ä¸ä¼šæ”¹å˜\n",
    "3. **è§£å†³æ–¹æ¡ˆ**ï¼šç¼“å­˜å·²è®¡ç®—çš„Keyå’ŒValueï¼Œåªè®¡ç®—æ–°tokençš„éƒ¨åˆ†\n",
    "\n",
    "### å·¥ä½œæœºåˆ¶\n",
    "1. **åˆå§‹æ­¥éª¤**ï¼šè®¡ç®—ç¬¬ä¸€ä¸ªtokençš„Qã€Kã€Vï¼Œå°†Kã€Vå­˜å…¥ç¼“å­˜\n",
    "2. **åç»­æ­¥éª¤**ï¼š\n",
    "   - åªè®¡ç®—æ–°tokençš„Qã€Kã€V\n",
    "   - å°†æ–°çš„Kã€Vè¿½åŠ åˆ°ç¼“å­˜\n",
    "   - ä½¿ç”¨æ–°çš„Qä¸ç¼“å­˜ä¸­æ‰€æœ‰Kè®¡ç®—æ³¨æ„åŠ›\n",
    "   - ç”¨æ³¨æ„åŠ›æƒé‡ä¸ç¼“å­˜ä¸­æ‰€æœ‰Vè®¡ç®—è¾“å‡º\n",
    "\n",
    "### æ•°å­¦ä¼˜åŠ¿\n",
    "- **è®¡ç®—å¤æ‚åº¦**ï¼šä»O(TÂ²)é™ä½åˆ°O(T)\n",
    "- **å†…å­˜æ¢æ—¶é—´**ï¼šç”¨çº¿æ€§å¢é•¿çš„å†…å­˜æ¢å–æ˜¾è‘—çš„è®¡ç®—èŠ‚çœ\n",
    "- **å®Œå…¨ç­‰ä»·**ï¼šç»“æœä¸ä¼ ç»Ÿæ–¹æ³•å®Œå…¨ç›¸åŒï¼Œåªæ˜¯è®¡ç®—æ–¹å¼æ›´é«˜æ•ˆ\n",
    "\n",
    "### é€‚ç”¨åœºæ™¯\n",
    "- âœ… å•æ¬¡é•¿åºåˆ—ç”Ÿæˆ\n",
    "- âœ… ä»¥å†å²å¯¹è¯ä¸ºcontextçš„ç”Ÿæˆ\n",
    "- âœ… å®æ—¶äº¤äº’åœºæ™¯\n",
    "- âŒ æ‰¹é‡å¤„ç†ç‹¬ç«‹çŸ­æ–‡æœ¬\n",
    "- âŒ éç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰\n",
    "\n",
    "ç°åœ¨æ‚¨åº”è¯¥å®Œå…¨ç†è§£KV Cacheçš„å·¥ä½œåŸç†äº†ï¼å®ƒæ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¼˜åŒ–çš„åŸºçŸ³æŠ€æœ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a32cc8",
   "metadata": {},
   "source": [
    "## ğŸ” å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼šä¸ºä»€ä¹ˆåªèƒ½çœ‹åˆ°å‰é¢çš„token\n",
    "\n",
    "æ‚¨çš„ç†è§£å®Œå…¨æ­£ç¡®ï¼è¿™æ˜¯è‡ªå›å½’ç”Ÿæˆçš„æ ¸å¿ƒç‰¹å¾ï¼Œè®©æˆ‘ä»¬æ·±å…¥è§£é‡Šè¿™ä¸ªæœºåˆ¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_causal_attention():\n",
    "    \"\"\"\n",
    "    è¯¦ç»†æ¼”ç¤ºå› æœæ³¨æ„åŠ›æœºåˆ¶ï¼šä¸ºä»€ä¹ˆæ¯ä¸ªtokenåªèƒ½çœ‹åˆ°å‰é¢çš„token\n",
    "    \"\"\"\n",
    "    print(\"=== å› æœæ³¨æ„åŠ›æœºåˆ¶è¯¦è§£ ===\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„æ–‡æœ¬ç”Ÿæˆåœºæ™¯\n",
    "    tokens = [\"æˆ‘\", \"å–œæ¬¢\", \"äººå·¥\", \"æ™ºèƒ½\", \"æŠ€æœ¯\"]\n",
    "    seq_len = len(tokens)\n",
    "    \n",
    "    print(f\"ç”Ÿæˆåºåˆ—: {tokens}\")\n",
    "    print(f\"åºåˆ—é•¿åº¦: {seq_len}\\n\")\n",
    "    \n",
    "    # åˆ›å»ºå› æœæ©ç çŸ©é˜µ\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    print(\"å› æœæ©ç çŸ©é˜µ (Causal Mask):\")\n",
    "    print(\"è¡Œ=å½“å‰token, åˆ—=å¯ä»¥çœ‹åˆ°çš„token\")\n",
    "    print(\"1=å¯ä»¥çœ‹åˆ°, 0=ä¸èƒ½çœ‹åˆ°\\n\")\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªæ›´ç›´è§‚çš„æ˜¾ç¤º\n",
    "    print(\"     \", end=\"\")\n",
    "    for j, token in enumerate(tokens):\n",
    "        print(f\"{j+1:>4}\", end=\"\")\n",
    "    print(\"  â† ä½ç½®\")\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        print(f\"{i+1:>2}. {tokens[i]:<4}\", end=\"\")\n",
    "        for j in range(seq_len):\n",
    "            print(f\"{int(causal_mask[i, j]):>4}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nè¯¦ç»†è§£é‡Šæ¯ä¸ªtokençš„æ³¨æ„åŠ›èŒƒå›´:\")\n",
    "    for i, current_token in enumerate(tokens):\n",
    "        visible_positions = [j+1 for j in range(seq_len) if causal_mask[i, j] == 1]\n",
    "        visible_tokens = [tokens[j] for j in range(seq_len) if causal_mask[i, j] == 1]\n",
    "        \n",
    "        print(f\"\\nä½ç½®{i+1} '{current_token}':\")\n",
    "        print(f\"  å¯ä»¥çœ‹åˆ°ä½ç½®: {visible_positions}\")\n",
    "        print(f\"  å¯ä»¥çœ‹åˆ°token: {visible_tokens}\")\n",
    "        print(f\"  æ³¨æ„åŠ›è®¡ç®—: Q_{i+1} Ã— [K_1, K_2, ..., K_{i+1}]\")\n",
    "        \n",
    "        if i == 0:\n",
    "            print(f\"  â†’ åªèƒ½çœ‹åˆ°è‡ªå·±ï¼Œè¿™æ˜¯åºåˆ—çš„å¼€å§‹\")\n",
    "        else:\n",
    "            print(f\"  â†’ å¯ä»¥åˆ©ç”¨å‰é¢{i+1}ä¸ªtokençš„ä¿¡æ¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtoken\")\n",
    "    \n",
    "    return causal_mask\n",
    "\n",
    "# è¿è¡Œæ¼”ç¤º\n",
    "causal_mask = demonstrate_causal_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87e17e",
   "metadata": {},
   "source": [
    "## ğŸš« ä¸ºä»€ä¹ˆä¸èƒ½çœ‹åˆ°åé¢çš„tokenï¼Ÿ\n",
    "\n",
    "è¿™ä¸ªé™åˆ¶ä¸æ˜¯æŠ€æœ¯ç¼ºé™·ï¼Œè€Œæ˜¯æœ‰æ·±åˆ»åŸå› çš„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_why_causal_constraint():\n",
    "    \"\"\"\n",
    "    è§£é‡Šä¸ºä»€ä¹ˆåœ¨è‡ªå›å½’ç”Ÿæˆä¸­å¿…é¡»ä½¿ç”¨å› æœçº¦æŸ\n",
    "    \"\"\"\n",
    "    print(\"=== ä¸ºä»€ä¹ˆå¿…é¡»ä½¿ç”¨å› æœçº¦æŸï¼Ÿ ===\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ 1. è®­ç»ƒæ—¶çš„ç›®æ ‡ï¼š\")\n",
    "    print(\"   è®­ç»ƒç›®æ ‡ï¼šç»™å®šå‰é¢çš„è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯\")\n",
    "    print(\"   P(w_t | w_1, w_2, ..., w_{t-1})\")\n",
    "    print(\"   å¦‚æœè®­ç»ƒæ—¶èƒ½çœ‹åˆ°w_tåé¢çš„è¯ï¼Œé‚£å°±æ˜¯'ä½œå¼Š'äº†ï¼\")\n",
    "    \n",
    "    print(\"\\nğŸ”„ 2. æ¨ç†æ—¶çš„ç°å®ï¼š\")\n",
    "    print(\"   æ¨ç†æ—¶æˆ‘ä»¬è¿˜æ²¡æœ‰ç”Ÿæˆåé¢çš„è¯\")\n",
    "    print(\"   ä¸å¯èƒ½çœ‹åˆ°è¿˜ä¸å­˜åœ¨çš„å†…å®¹\")\n",
    "    \n",
    "    print(\"\\nğŸ“š 3. å…·ä½“ä¾‹å­è¯´æ˜ï¼š\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè®­ç»ƒåœºæ™¯\n",
    "    training_sentence = \"æˆ‘å–œæ¬¢äººå·¥æ™ºèƒ½æŠ€æœ¯\"\n",
    "    words = training_sentence.split()\n",
    "    \n",
    "    print(f\"\\nè®­ç»ƒå¥å­: '{training_sentence}'\")\n",
    "    print(\"\\nè®­ç»ƒè¿‡ç¨‹çš„é¢„æµ‹ä»»åŠ¡:\")\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if i == 0:\n",
    "            context = \"[å¼€å§‹]\"\n",
    "            target = words[i]\n",
    "        else:\n",
    "            context = \" \".join(words[:i])\n",
    "            target = words[i]\n",
    "        \n",
    "        print(f\"  ä»»åŠ¡{i+1}: ç»™å®š '{context}' â†’ é¢„æµ‹ '{target}'\")\n",
    "        \n",
    "        if i < len(words) - 1:\n",
    "            future_words = \" \".join(words[i+1:])\n",
    "            print(f\"         âŒ ä¸èƒ½çœ‹åˆ°æœªæ¥çš„: '{future_words}'\")\n",
    "    \n",
    "    print(\"\\nğŸ§  4. å¦‚æœè¿åå› æœçº¦æŸä¼šæ€æ ·ï¼Ÿ\")\n",
    "    print(\"\\nå‡è®¾æˆ‘ä»¬è®©æ¨¡å‹åœ¨é¢„æµ‹'äººå·¥'æ—¶èƒ½çœ‹åˆ°åé¢çš„'æ™ºèƒ½æŠ€æœ¯':\")\n",
    "    print(\"  è®­ç»ƒ: çœ‹åˆ°('æˆ‘', 'å–œæ¬¢', 'äººå·¥', 'æ™ºèƒ½', 'æŠ€æœ¯') â†’ é¢„æµ‹'äººå·¥'\")\n",
    "    print(\"  æ¨ç†: åªæœ‰('æˆ‘', 'å–œæ¬¢') â†’ é¢„æµ‹'äººå·¥'\")\n",
    "    print(\"  ç»“æœ: è®­ç»ƒå’Œæ¨ç†çš„æ¡ä»¶ä¸ä¸€è‡´ï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™ï¼\")\n",
    "    \n",
    "    print(\"\\nâœ… 5. å› æœçº¦æŸçš„å¥½å¤„ï¼š\")\n",
    "    print(\"  - è®­ç»ƒå’Œæ¨ç†æ¡ä»¶ä¸€è‡´\")\n",
    "    print(\"  - æ¨¡å‹å­¦ä¼šçœŸæ­£çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›\")\n",
    "    print(\"  - é¿å…ä¿¡æ¯æ³„éœ²ï¼Œç¡®ä¿å…¬å¹³æ€§\")\n",
    "    \n",
    "    return words\n",
    "\n",
    "explain_why_causal_constraint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9197f5",
   "metadata": {},
   "source": [
    "## ğŸ¤ KV Cache ä¸å› æœæ³¨æ„åŠ›çš„å®Œç¾é…åˆ\n",
    "\n",
    "KV Cacheä¹‹æ‰€ä»¥èƒ½å·¥ä½œï¼Œæ­£æ˜¯å› ä¸ºå› æœæ³¨æ„åŠ›çš„ç‰¹æ€§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ac0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kv_cache_causal_harmony():\n",
    "    \"\"\"\n",
    "    æ¼”ç¤ºKV Cacheå¦‚ä½•ä¸å› æœæ³¨æ„åŠ›å®Œç¾é…åˆ\n",
    "    \"\"\"\n",
    "    print(\"=== KV Cache ä¸å› æœæ³¨æ„åŠ›çš„å®Œç¾é…åˆ ===\")\n",
    "    \n",
    "    tokens = [\"æˆ‘\", \"å–œæ¬¢\", \"äººå·¥\", \"æ™ºèƒ½\"]\n",
    "    \n",
    "    print(\"è®©æˆ‘ä»¬é€æ­¥çœ‹çœ‹ç”Ÿæˆè¿‡ç¨‹:\\n\")\n",
    "    \n",
    "    # æ¨¡æ‹ŸKV Cacheçš„é€æ­¥æ„å»ºè¿‡ç¨‹\n",
    "    kv_cache = {\"keys\": [], \"values\": []}\n",
    "    \n",
    "    for step in range(1, len(tokens) + 1):\n",
    "        current_token = tokens[step-1]\n",
    "        context_tokens = tokens[:step]\n",
    "        \n",
    "        print(f\"=== æ­¥éª¤ {step}: ç”Ÿæˆ '{current_token}' ===\")\n",
    "        print(f\"å½“å‰ä¸Šä¸‹æ–‡: {context_tokens}\")\n",
    "        \n",
    "        if step == 1:\n",
    "            print(\"\\nğŸ†• ç¬¬ä¸€æ­¥ - åˆå§‹åŒ–KV Cache:\")\n",
    "            print(f\"  è®¡ç®— '{current_token}' çš„ Key å’Œ Value\")\n",
    "            print(f\"  å­˜å‚¨: K1, V1\")\n",
    "            kv_cache[\"keys\"].append(f\"K_{step}\")\n",
    "            kv_cache[\"values\"].append(f\"V_{step}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nğŸ”„ åç»­æ­¥éª¤ - å¤ç”¨KV Cache:\")\n",
    "            print(f\"  ä»ç¼“å­˜è·å–: {kv_cache['keys']} (ä¹‹å‰è®¡ç®—çš„Key)\")\n",
    "            print(f\"  ä»ç¼“å­˜è·å–: {kv_cache['values']} (ä¹‹å‰è®¡ç®—çš„Value)\")\n",
    "            print(f\"  è®¡ç®—æ–°çš„: K_{step}, V_{step} (åªä¸ºå½“å‰token '{current_token}')\")\n",
    "            \n",
    "            # æ›´æ–°ç¼“å­˜\n",
    "            kv_cache[\"keys\"].append(f\"K_{step}\")\n",
    "            kv_cache[\"values\"].append(f\"V_{step}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ æ³¨æ„åŠ›è®¡ç®—:\")\n",
    "        print(f\"  Query: Q_{step} (åªä¸ºå½“å‰token '{current_token}')\")\n",
    "        print(f\"  Keys: {kv_cache['keys']} (æ¥è‡ªç¼“å­˜ + å½“å‰)\")\n",
    "        print(f\"  Values: {kv_cache['values']} (æ¥è‡ªç¼“å­˜ + å½“å‰)\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå› æœæ©ç çš„ä½œç”¨\n",
    "        print(f\"\\nğŸ­ å› æœæ©ç ç¡®ä¿:\")\n",
    "        visible_positions = list(range(1, step + 1))\n",
    "        print(f\"  ä½ç½®{step}çš„'{current_token}'åªèƒ½çœ‹åˆ°ä½ç½®: {visible_positions}\")\n",
    "        print(f\"  å¯¹åº”çš„token: {context_tokens}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ å…³é”®æ´å¯Ÿ:\")\n",
    "        print(f\"  âœ… '{current_token}' æ°¸è¿œä¸ä¼šçœ‹åˆ°ä½ç½®{step+1}ä¹‹åçš„token\")\n",
    "        print(f\"  âœ… æ‰€ä»¥ä¹‹å‰çš„K_{step}, V_{step}åœ¨æœªæ¥æ­¥éª¤ä¸­ä¸ä¼šæ”¹å˜\")\n",
    "        print(f\"  âœ… è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¯ä»¥å®‰å…¨åœ°ç¼“å­˜å®ƒä»¬ï¼\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"ğŸ”¥ æ€»ç»“ - KV Cache çš„å·§å¦™ä¹‹å¤„:\")\n",
    "    print(\"\\n1. å› æœæ³¨æ„åŠ›ä¿è¯: è¿‡å»çš„Key/Valueåœ¨æœªæ¥ä¸ä¼šè¢«é‡æ–°è®¡ç®—\")\n",
    "    print(\"2. KV Cacheåˆ©ç”¨è¿™ä¸€ç‚¹: å­˜å‚¨å·²è®¡ç®—çš„Key/Value\")\n",
    "    print(\"3. å®Œç¾é…åˆ: æ¯æ¬¡åªéœ€è®¡ç®—æ–°tokençš„K/Vï¼Œç„¶åä¸ç¼“å­˜ç»„åˆ\")\n",
    "    print(\"4. ç»“æœä¸€è‡´: ä¸é‡æ–°è®¡ç®—å®Œå…¨ç›¸åŒï¼Œä½†æ•ˆç‡é«˜å¾—å¤š\")\n",
    "    \n",
    "    return kv_cache\n",
    "\n",
    "kv_cache_demo = demonstrate_kv_cache_causal_harmony()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_causal_attention_kv_cache():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å› æœæ³¨æ„åŠ›ä¸KV Cacheçš„å…³ç³»\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # === å·¦å›¾ï¼šå› æœæ³¨æ„åŠ›çŸ©é˜µ ===\n",
    "    seq_len = 5\n",
    "    tokens = [\"æˆ‘\", \"å–œæ¬¢\", \"äººå·¥\", \"æ™ºèƒ½\", \"æŠ€æœ¯\"]\n",
    "    \n",
    "    # åˆ›å»ºå› æœæ©ç \n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len)).numpy()\n",
    "    \n",
    "    # ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "    im1 = ax1.imshow(causal_mask, cmap='RdYlGn', aspect='equal')\n",
    "    ax1.set_title('å› æœæ³¨æ„åŠ›çŸ©é˜µ\\n(æ¯è¡Œåªèƒ½çœ‹åˆ°å¯¹è§’çº¿å·¦ä¸‹çš„éƒ¨åˆ†)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # è®¾ç½®æ ‡ç­¾\n",
    "    ax1.set_xticks(range(seq_len))\n",
    "    ax1.set_yticks(range(seq_len))\n",
    "    ax1.set_xticklabels([f'{i+1}.{token}' for i, token in enumerate(tokens)], rotation=45)\n",
    "    ax1.set_yticklabels([f'{i+1}.{token}' for i, token in enumerate(tokens)])\n",
    "    ax1.set_xlabel('å¯ä»¥çœ‹åˆ°çš„token (Key/Valueæ¥æº)')\n",
    "    ax1.set_ylabel('å½“å‰é¢„æµ‹çš„token (Query)')\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            color = 'white' if causal_mask[i, j] == 1 else 'black'\n",
    "            ax1.text(j, i, int(causal_mask[i, j]), ha='center', va='center', \n",
    "                    color=color, fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # === å³å›¾ï¼šKV Cacheåˆ©ç”¨æ¨¡å¼ ===\n",
    "    # åˆ›å»ºKV Cacheä½¿ç”¨æ¨¡å¼çš„å¯è§†åŒ–\n",
    "    cache_pattern = np.zeros((seq_len, seq_len))\n",
    "    \n",
    "    for step in range(seq_len):\n",
    "        for pos in range(step + 1):\n",
    "            if pos == step:\n",
    "                cache_pattern[step, pos] = 2  # æ–°è®¡ç®—çš„\n",
    "            else:\n",
    "                cache_pattern[step, pos] = 1  # ä»ç¼“å­˜è·å–çš„\n",
    "    \n",
    "    # ç»˜åˆ¶ç¼“å­˜ä½¿ç”¨æ¨¡å¼\n",
    "    colors = ['white', 'lightblue', 'orange']\n",
    "    im2 = ax2.imshow(cache_pattern, cmap='viridis', aspect='equal')\n",
    "    ax2.set_title('KV Cache ä½¿ç”¨æ¨¡å¼\\n(æ©™è‰²=æ–°è®¡ç®—, è“è‰²=ç¼“å­˜å¤ç”¨)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xticks(range(seq_len))\n",
    "    ax2.set_yticks(range(seq_len))\n",
    "    ax2.set_xticklabels([f'{i+1}.{token}' for i, token in enumerate(tokens)], rotation=45)\n",
    "    ax2.set_yticklabels([f'æ­¥éª¤{i+1}' for i in range(seq_len)])\n",
    "    ax2.set_xlabel('Tokenä½ç½® (Key/Value)')\n",
    "    ax2.set_ylabel('ç”Ÿæˆæ­¥éª¤')\n",
    "    \n",
    "    # æ·»åŠ å›¾ä¾‹è¯´æ˜\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0,0),1,1, facecolor='orange', label='æ–°è®¡ç®—K/V'),\n",
    "        plt.Rectangle((0,0),1,1, facecolor='lightblue', label='ç¼“å­˜å¤ç”¨K/V'),\n",
    "        plt.Rectangle((0,0),1,1, facecolor='white', label='ä¸éœ€è¦')\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡æ³¨\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if cache_pattern[i, j] == 2:\n",
    "                ax2.text(j, i, 'æ–°', ha='center', va='center', \n",
    "                        color='white', fontweight='bold', fontsize=10)\n",
    "            elif cache_pattern[i, j] == 1:\n",
    "                ax2.text(j, i, 'ç¼“å­˜', ha='center', va='center', \n",
    "                        color='black', fontweight='bold', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š å›¾è¡¨è§£è¯»:\")\n",
    "    print(\"\\nå·¦å›¾ - å› æœæ³¨æ„åŠ›çŸ©é˜µ:\")\n",
    "    print(\"  â€¢ ç»¿è‰²(1): å½“å‰tokenå¯ä»¥çœ‹åˆ°çš„å†å²token\")\n",
    "    print(\"  â€¢ çº¢è‰²(0): å½“å‰tokenä¸èƒ½çœ‹åˆ°çš„æœªæ¥token\")\n",
    "    print(\"  â€¢ æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªtokenç”Ÿæˆæ—¶çš„æ³¨æ„åŠ›èŒƒå›´\")\n",
    "    \n",
    "    print(\"\\nå³å›¾ - KV Cacheä½¿ç”¨æ¨¡å¼:\")\n",
    "    print(\"  â€¢ æ©™è‰²: è¯¥æ­¥éª¤éœ€è¦æ–°è®¡ç®—çš„K/V\")\n",
    "    print(\"  â€¢ è“è‰²: ä»ä¹‹å‰æ­¥éª¤çš„ç¼“å­˜ä¸­å¤ç”¨çš„K/V\")\n",
    "    print(\"  â€¢ ç™½è‰²: ä¸éœ€è¦(å› ä¸ºå› æœæ©ç é˜»æ­¢äº†è®¿é—®)\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "    print(\"  æ¯ä¸ªæ­¥éª¤åªéœ€è¦è®¡ç®—1ä¸ªæ–°çš„K/Vå¯¹ï¼Œå…¶ä½™éƒ½å¯ä»¥ä»ç¼“å­˜å¤ç”¨ï¼\")\n",
    "    print(\"  è¿™å°±æ˜¯KV Cacheèƒ½å¤Ÿå¤§å¹…æå‡æ•ˆç‡çš„æ ¹æœ¬åŸå› ã€‚\")\n",
    "\n",
    "visualize_causal_attention_kv_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd89e1",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ ¸å¿ƒæ´å¯Ÿæ€»ç»“\n",
    "\n",
    "æ‚¨çš„ç†è§£å®Œå…¨æ­£ç¡®ï¼è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¿™ä¸ªé‡è¦çš„è§‚å¯Ÿï¼š\n",
    "\n",
    "### ğŸ”‘ å…³é”®å‘ç°\n",
    "**\"å‰é¢çš„tokenåªä¼šä½¿ç”¨åˆ°å‰é¢çš„tokenè®¡ç®—é¢„æµ‹\"**\n",
    "\n",
    "è¿™å¥è¯æ­ç¤ºäº†è‡ªå›å½’ç”Ÿæˆçš„æœ¬è´¨ï¼š\n",
    "\n",
    "1. **å› æœæ€§çº¦æŸ**: \n",
    "   - ä½ç½® t çš„tokenåªèƒ½\"çœ‹åˆ°\"ä½ç½® 1 åˆ° t çš„token\n",
    "   - ä¸èƒ½çœ‹åˆ°ä½ç½® t+1 åŠä¹‹åçš„token\n",
    "   - è¿™æ˜¯è®­ç»ƒå’Œæ¨ç†ä¸€è‡´æ€§çš„ä¿è¯\n",
    "\n",
    "2. **KV Cacheçš„æœºä¼š**:\n",
    "   - æ—¢ç„¶ä½ç½® t çš„Key/Valueåœ¨æœªæ¥æ­¥éª¤ä¸­ä¸ä¼šè¢«\"é‡æ–°è§‚å¯Ÿ\"\n",
    "   - é‚£ä¹ˆå®ƒä»¬å°±å¯ä»¥å®‰å…¨åœ°è¢«ç¼“å­˜å’Œå¤ç”¨\n",
    "   - é¿å…äº†é‡å¤è®¡ç®—ç›¸åŒçš„å†…å®¹\n",
    "\n",
    "3. **è®¡ç®—æ¨¡å¼**:\n",
    "   ```\n",
    "   æ­¥éª¤1: Qâ‚ Ã— [Kâ‚] â†’ è¾“å‡ºâ‚\n",
    "   æ­¥éª¤2: Qâ‚‚ Ã— [Kâ‚, Kâ‚‚] â†’ è¾“å‡ºâ‚‚  (Kâ‚æ¥è‡ªç¼“å­˜)\n",
    "   æ­¥éª¤3: Qâ‚ƒ Ã— [Kâ‚, Kâ‚‚, Kâ‚ƒ] â†’ è¾“å‡ºâ‚ƒ  (Kâ‚,Kâ‚‚æ¥è‡ªç¼“å­˜)\n",
    "   ```\n",
    "\n",
    "### ğŸ§  æ·±å±‚å«ä¹‰\n",
    "\n",
    "- **è¯­è¨€çš„å•å‘æ€§**: æˆ‘ä»¬å†™ä½œå’Œè¯´è¯éƒ½æ˜¯ä»å·¦åˆ°å³çš„è¿‡ç¨‹\n",
    "- **é¢„æµ‹çš„æœ¬è´¨**: åŸºäºå·²çŸ¥é¢„æµ‹æœªçŸ¥ï¼Œè€Œä¸æ˜¯åŸºäºå…¨çŸ¥é¢„æµ‹\n",
    "- **æ•ˆç‡çš„æ¥æº**: åˆ©ç”¨è®¡ç®—çš„å•è°ƒæ€§ï¼ˆå·²ç®—è¿‡çš„ä¸éœ€è¦é‡ç®—ï¼‰\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆKV Cacheèƒ½å¤Ÿåœ¨ä¿æŒå®Œå…¨ç›¸åŒç»“æœçš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆæ•ˆç‡çš„æ ¹æœ¬åŸå› ï¼\n",
    "\n",
    "æ‚¨æŠ“ä½äº†è¿™ä¸ªæŠ€æœ¯æœ€æ ¸å¿ƒçš„æ´å¯Ÿç‚¹ã€‚ğŸ‘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
