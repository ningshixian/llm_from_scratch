> 梯度下降法详解，如 GD、SGD等可以参考 [https://www.yuque.com/docs/share/f2c47350-9342-43ed-b3d6-ec4fb47d63c3?#](https://www.yuque.com/docs/share/f2c47350-9342-43ed-b3d6-ec4fb47d63c3?#) 《2022-03-25-梯度下降法》
>

<font style="color:rgb(18, 18, 18);">这章主要讲了深度学习的优化方法，主要涉及的内容有SGD，Momentum, Adagrad, RMSProp, Adam等</font>

<font style="color:rgb(18, 18, 18);"></font>

## **<font style="color:rgb(18, 18, 18);">Stochastic Gradient Descent</font>**
<font style="color:rgb(18, 18, 18);">之前已经在</font>[梯度下降算法](https://zhuanlan.zhihu.com/p/38644738)<font style="color:rgb(18, 18, 18);">介绍过Gradient Descent 算法，它每次计算梯度时利用所有的数据样本，其优点是这样计算的梯度较为准确，但缺点是计算量较大。</font>

<font style="color:rgb(18, 18, 18);"></font>

<font style="color:rgb(18, 18, 18);">例如我们有两种方法：</font>

1. <font style="color:rgb(18, 18, 18);">利用100个样本计算梯度 </font>
2. <font style="color:rgb(18, 18, 18);">利用10000个样本计算梯度</font>

<font style="color:rgb(18, 18, 18);">第二种方法是第一种的100倍的计算量，但只将均值的标准差减少了10倍，我们通常希望我们能更快的达到我们的优化目标，所以不需要每次计算梯度都严格的利用全部样本，而是进行多次iteration，而每次估算的梯度有合适的准确度即可。在优化算法里，我们通常所说的stochastic gradient descent指的是minibatch stochastic methods，即每次计算梯度时利用一部分样本，其样本量是新引入的超参数batch size。</font>

<font style="color:rgb(18, 18, 18);"></font>

<font style="color:rgb(18, 18, 18);">对于SGD，常常会遇到两个问题，一种情况是当遇到</font>**<font style="color:rgb(18, 18, 18);">局域极小值</font>**<font style="color:rgb(18, 18, 18);">或是</font>**<font style="color:rgb(18, 18, 18);">鞍点</font>**<font style="color:rgb(18, 18, 18);">时，梯度为零，SGD无法继续更新而卡在这一局域，如下图所示。</font>

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1675955747323-b9f12cb0-87aa-4427-ad7d-98625791ca35.png)

<font style="color:rgb(18, 18, 18);">还有一种是在不同的方向上曲率不同，而SGD过程会在曲率较高的方向上来回震荡，而在曲率较低的方向上进展缓慢（</font>**<font style="color:rgb(18, 18, 18);">山谷震荡</font>**<font style="color:rgb(18, 18, 18);">），如下图所示，不能选取最优路径达到极值点。</font>

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2023/webp/8420697/1675955747616-26371152-2f06-4b81-9602-64c6f140da67.webp)

<font style="color:rgb(18, 18, 18);">为了解决这些问题，不同的基于SGD的修正算法被提出，如以下常用的几种。</font>

<font style="color:rgb(18, 18, 18);"></font>

## _<font style="color:rgb(18, 18, 18);">Momentum</font>_
前面提到SGD算法存在<u>山谷震荡</u>和<u>鞍点停滞</u>的问题，在解决这两个问题之前。先做一个思维实验。我们分别想象一个纸团和铁球从山顶往下滚。

+ 假如遇到的是山谷地形。纸团由于质量小，在山谷中滚动过程中极易受到山壁弹力的影响，从而来回震荡，这有点类似于随机梯度下降法在梯度更新时的不稳定。而铁球由于自身质量大，在下降过程中受到山壁弹力的干扰小，从而平稳地滚到山底。
+ 假如遇到的是鞍点地形。纸团在进入平原时，由于质量小导致惯性也较小，容易停滞不前。而铁球由于自身惯性较大，在来到鞍点中心时，也有一定机会冲出去平坦的陷阱。

从上述情况综合看来，铁球由于自身质量较大，在下降过程中即便有受到其它方向的力，但是**惯性**却使得它的运动轨迹不易出现山谷震荡和鞍点停滞的情况。



由此引入 **<font style="color:rgb(18, 18, 18);">Momentum</font>**<font style="color:rgb(18, 18, 18);">，顾名思义，来自物理学中的动量概念，即我们用一个新的变量 </font><font style="color:rgb(18, 18, 18);"></font>$ v $<font style="color:rgb(18, 18, 18);"> 来记录之前的梯度变化的速度（</font>**<font style="color:rgb(18, 18, 18);">保留惯性</font>**<font style="color:rgb(18, 18, 18);">），其优势在于对于局域极小值或鞍点的情况，由于保持了原有的速度，模型可以继续更新。</font>

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1675956125799-c0179867-adef-40b8-9ab6-e02c7c8dcc38.png)

<font style="color:rgb(18, 18, 18);">而对于不同方向的曲率不同的问题，由于其保持一定原有方向的速度，不会在曲率较大的山峰间进行剧烈波动，如下图蓝线所示：</font>

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2023/webp/8420697/1675956125814-2ce5c1ee-5d2e-4664-8208-81fe89c88095.webp)



Momentum 的<font style="color:rgb(18, 18, 18);">优化公式如下：</font>具体做法是<font style="color:#F5222D;">给当前梯度加上一个比例因子为</font>$ \rho $<font style="color:#F5222D;">的历史梯度向量</font>$ v $，使得<font style="color:rgb(18, 18, 18);">每一次的参数更新方向不仅取决于当前位置的梯度，还受到上一次参数更新方向的影响。</font>Momentum 的<font style="color:rgb(18, 18, 18);">优化公式如下：</font>

<!-- 这是一张图片，ocr 内容为：SGD WITH MOMENTUM SGD 梯度 上一步计算的梯度 权重 (0)ZEA+RAD,. 日;O;SSL(E) UT+1 OJ-EUT+EUT+1 学习率 -->
![](https://cdn.nlark.com/yuque/0/2022/png/8420697/1648206882800-23829eec-7e51-459d-8534-2520ca12e319.png)



其中$ \rho<1 $，一般设为 0.9；$ v $是所有历史梯度的加权和。我们假设$ v $初始为 0，$ \rho $为 0.9，那么迭代过程如下：

<!-- 这是一张图片，ocr 内容为：VLI(F(OI)) 7L2(F(02)) VLI(F(E1)) 2 0.9*VLI(F(01))+ VL2(F(02)) VL3(F(03)) *(VL1)+0.9*(VL2)+(VL3) 0.81 -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673345472211-05d3d338-9add-4f38-834b-7903c234c5f6.png)



我们可以看到之前的梯度会一直存在后面的迭代过程中，只是越靠前的梯度其权重越小。将这些向量求和用于参数更新，<u><font style="color:rgb(64, 64, 64);">梯度指向相同的维度上动量增长，梯度变向的维度上动量减小。</font></u>

<font style="color:rgb(64, 64, 64);"></font>

回到前面介绍的铁球实验。沿着山谷向下的铁球，会同时受到沿坡道向下的力和左右山谷碰撞的力。由于<u>向下的力稳定不变，产生的动量不断累加，左右的弹力来回摇摆，动量在累积后相互抵消，从而减弱了铁球来回的震荡。</u>



这里的动量法由于引入了惯性，所以它的参数轨迹类似铁球下山的轨迹。而随机梯度下降法则与纸团下山的轨迹类似，震荡不稳定。因此，<font style="color:#E8323C;">相比较于随机梯度下降法，动量法收敛速度更快，收敛曲线也更稳</font><font style="color:#DF2A3F;">定</font>。



## _<font style="color:rgb(18, 18, 18);">Nestrov Momentum</font>_
除了利用惯性跳出局部沟壑以外，我们还可以尝试往前看一步。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前多看一步。

<font style="color:rgb(64, 64, 64);"></font>

[Nesterov, Y. 1983](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)<font style="color:rgb(64, 64, 64);"> 提出的 </font>NAG (Nesterov Accelerated Gradient) <font style="color:rgb(64, 64, 64);">就是这样一种赋予动量项</font><font style="color:#722ED1;">前瞻</font><font style="color:rgb(64, 64, 64);">能力的方法。</font>Momentum 在时刻 t 的主要下降方向是由历史梯度（惯性）决定的，当前时刻的梯度权重较小，那不如先看看如果跟着惯性先走一步，那个时候外面的世界是怎祥的。也即在 Momentum 的基础上将当前时刻的梯度$ \nabla L(\theta_t) $换成下一时刻梯度$ \nabla L(\theta_t-\beta\nabla_{t-1}) $，<font style="color:rgb(64, 64, 64);">不用当前参数 θ 计算梯度而是用参数在未来大概率所处的位置来计算，我们就有了很强的预见能力：</font>

<!-- 这是一张图片，ocr 内容为：MT MVJ(0T-BMT-1) AOT三M. 0T+1 0T-( BMT-1 +N7J(0T-BMT-1)) -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673348938574-c4ec6401-23fe-4cd2-b21e-c5693736dd88.png)

<font style="color:rgb(64, 64, 64);">以下图做示例进行图解。Momentum 方法会先计算当前梯度（图 4 中的小蓝色向量）然后跨一大步迈向更新后的累积梯度方向（大蓝色向量），而 NAG 会先往之前累积梯度的方向迈一大步（棕色向量），算下梯度然后做些修正（红色向量），二者结合实现完整的 NAG 更新（绿色向量）。这种有预见性的更新可以防止我们走得太快，加快了响应速度，</font>[Bengio 等人，2012](https://arxiv.org/abs/1212.0901)<font style="color:rgb(64, 64, 64);"> 证实该方法可以显著增强 RNN 在许多任务上的性能。</font>

<!-- 这是一张图片，ocr 内容为： -->
![NAG 与 Momentum 更新](https://cdn.nlark.com/yuque/0/2022/png/8420697/1670321766432-bce401aa-8903-48e5-9dff-9a1b4f8f1272.png)

## _<font style="color:rgb(18, 18, 18);">Adagrad</font>_
<font style="color:rgb(51, 51, 51);">当我们在对模型进行参数优化时，学习率的设定也是很重要的。如果学习率过小，训练时会耗费大量时间，过大会导致训练无法收敛。为了加快收敛速度，同时又提高求解精度，通常会采用衰减学习率的方案：一开始算法采用较大的学习率，在误差曲线平缓以后，减小学习率做出更精细的调整。</font>

<font style="color:rgb(51, 51, 51);"></font>

<font style="color:rgb(51, 51, 51);">随机梯度下降法对环境的感知是指在参数空间中，根据不同参数的经验性判断，自适应地确定参数的学习率，不同参数的在更新时更新的步长不一样。例如，在推荐系统中，如果我们要学习某个物品的嵌入表示，若与该物品交互过的用户不多，那么它的梯度更新就会很慢。这就是由于数据稀疏性导致了梯度的稀疏性。</font>



AdaGrad 引入<u>二阶动量</u>来自适应地确定参数的学习率。AdaGrad 会结合<u>历史累计梯度的平方和</u>（均方根）调整$ \theta $在时间步$ t $的通用学习率$ \alpha $。这样，就能使得那些更新幅度很大的参数的学习率变小，使得那些更新幅度很小的参数学习率变大。

<!-- 这是一张图片，ocr 内容为：T++ VL(O)O O O  X &三一 G+ IP -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673349792578-9cbd085e-8451-4275-9330-4d6c716ee6cc.png)

这里 r 是一个对角矩阵，每个对角元素都是$ \theta_i $截至时间步 t 时梯度的平方和，而$ \delta $是防止除零的平滑项（一般量级在 $ 1e-8 $）。有意思的地方在于，不取平方根算法性能会早好很多。



+ Adagrad 一大优点在于省去了手工调整学习率的麻烦，绝大多数实现都是选了默认值 0.01 后就扔那了。
+ <font style="color:#DF2A3F;">Adagrad 的主要缺陷</font><font style="color:rgb(64, 64, 64);">出在分母上的梯度平方和：</font><font style="color:rgb(41, 41, 41);">随着时间步长“t”的增加，平方梯度的总和“r”会一直增加，这导致学习率“α”</font><font style="color:rgb(64, 64, 64);">萎缩并最终变得微乎其微</font><font style="color:rgb(41, 41, 41);"></font><font style="color:rgb(64, 64, 64);">，</font><font style="color:rgb(18, 18, 18);">最后可能会导致参数无法更新</font><font style="color:rgb(64, 64, 64);">。</font>

<!-- 这是一张图片，ocr 内容为：T-1 M (VL(0:)) 二 TR+8 0一? -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673346474437-0241126a-069f-47b2-881c-cffcda129724.png)



**<font style="color:rgb(36, 41, 47);">💡</font>****<font style="color:rgb(36, 41, 47);">AdaGrad Optimizer 实现: </font>**

<!-- 这是一张图片，ocr 内容为：IMPORT NUMPY AS NP CLASS ADAGRAD: "ADAGRAD PARAMETERS: LEARNING RATE: FLOAT -0.001 ED WHEN FOLLOWING THE NEGATIVE GRADIENT. THE STEP LENGTH USED WLE AL ACCUMULATOR VALUE:FLOAT 0.1 INITIAL STARTING VALUE FOR THE ACCUMULATORS, MUST BE NON-NEGATIVE. EPSILON:FLOAT - 1E-07 SMALL FLOATING POINT VALUE TO AVOID ZERO DENOMINATOR. ; LEAMING-RATE: FLOAT - 0.901, INITIALLACCUMULATOR-VALUE; FLOAT - O.1, EPSILON; FLOAT - 10-97)-> HONE _INIT_(SELF, LEARNIN DEF  SELF.LEARNING RATE - LEARNING RATE SELF.INITIAL ACCUMULATOR VALUE - INITIAL ACCUMULATOR_VALUE SELF.EPSILON - EPSILON SELF.G - NP.ARRAY(() # SUM OF SQUARES OF THE GRADIENTS - SELF.INITIAL ACCUMULATOR-VALUE > O, "INITIAL ACCUMULATOR.VALUE MUST BE NON-NEGATIVE ASSERT SE DEF UPDATE(SELF, W: NP.NDARRAY, GRAD WRT W: NP.NDARRAY) -> N >NP.NDARRAY: # INITIALIZE W UPDATE IF NOT INITIALIZED YET IF NOT SELF.G.ANY(): SELF.G - NP.FULL(NP.SHAPE(W), SELF.INITIAL ACCUMULATOR_VALUE) # ADD THE SQUARE OF THE GRE E GRADIENT OF THE LOSS FUNCTION AT W SELF.G +- NP.POWER(GRAD_WRT_W, 2) RETURN W - SELF.LEARNING_RATE * GRAD WRT W / NP.SQRT(SELF.G + SELF.EPSILON) -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1676455682856-af22c039-646c-4454-9b6b-0f88e8d11c90.png)<font style="color:rgb(36, 41, 47);">  
</font>

## <font style="color:rgb(64, 64, 64);">RMSprop</font>
<font style="color:rgb(64, 64, 64);">RMSprop 是一个未发表的自适应学习算法，是 Geoff Hinton 在其 </font>[Coursera 课程](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)<font style="color:rgb(64, 64, 64);">上提出来的，用于解决 Adagrad 的学习率快速衰减的问题。</font>

由于AdaGrad单调递减的学习率变化过于激进，<font style="color:rgb(64, 64, 64);">RMSprop </font>考虑一个改变<u>二阶动量</u>计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。它在 AdaGrad 的基础上将普通的历史累计梯度平方和换成历史累计梯度平方和的<u>指数加权移动平均值</u>，所以只需将 AdaGrad 中的 r 改成指数加权移动平均值的形式即可，也即

<!-- 这是一张图片，ocr 内容为：(1 - P U -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673346722116-e444cf33-a892-40c2-b6c2-e08575f8ed38.png)

引入一个衰减系数，让r每次都以一定的比例衰减，类似于Momentum中的做法。衰减系数使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样



<font style="color:#E8323C;">为什么使用指数加权移动平均？</font>

<!-- 这是一张图片，ocr 内容为：(指数加权移动平均值(EXPONENTIALLYWEIGHTEDMOVINGAVERAGE,EWMA):假设UT-1是 T-1时刻的指数加权移动平均值,0.是时刻的观测值,那么时刻的指数加权移动平均 值为 UT-BUT-1 + 1 - B)OT . T-1 . (1-B)PIOE-I +(1-8)01+ I-1 其中0<8<1,00三0.显然,由上式可知,时刻的指数加权移动平均值其实可以看, 做前时刻所有观测值的指数加权平均值,除了第时刻的观测值权重为1-B外,其他时 刻的观测值权重为(1-8).由于通常对于那些权重小于那些权重小于 的观测值可以忽略不计,所 以忽略掉那些观测值以后,上式就可以看做在求指数加权移动平均值. -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673350257576-79df677b-9d64-43d5-993b-c4875d9e0451.png)

<!-- 这是一张图片，ocr 内容为：那么哪些项的权重会小于'呢? N 1 0.3679 LIM U 11个 若令N三--则 N 1 1一 (B)1二六月 0.3679 LIM LIM N E 8-1 106个 所以,当B-1时,那些的O-的权重(1-B)6-定小于,例如当T三 20,80.9时,01.2,.99,010的权重都是小于三的,因此可以忽略不计,那么此时 就相当于在求011,012,....... ,019,020这最近10个时刻的加权移动平均值.所以指数移动平 20.9. 均值可以近似看做在求最近一 6个时刻的加权移动平均值,8常取>0 -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673350271982-932c4af3-2934-4f2d-a27b-a5efe19e3f2f.png)

<font style="color:rgb(51, 51, 51);"></font>

**<font style="color:rgb(36, 41, 47);">💡</font>****<font style="color:rgb(36, 41, 47);">RMS Prop 实现:</font>**

<!-- 这是一张图片，ocr 内容为：1 IMPORT NUMPY AS NP 2  CLASS RMSPROP: 3 RMSPROP 4 PARAMETERS: 5 6 LEARNING_RATE: FLOAT : 0.001 7 THE STEP LENGTH USED WHEN FOLLOWING THE NEGATIVE GRADIENT. 8 RHO: FLOAT : 0.9 9  DISCOUNTING FAD CING FACTOR FOR THE HISTORY/COMING GRADIENT. 1E-07 10 FLOAT EPSILON: FLOATING ATING POINT VALUE TO AVOID ZERO DENOMINATOR. 11 A SMALL 12 FLOAT : 0.9, EPSILON: FLOAT - 1E-7) -> NONE: LEARNING_RATE: FLOAT ; 0.001, RHO: FLOAT ; INIT DEF I (SELF, 13 LEARNING_RATE SELF.LEARNING RATE 14 RHO 15 SELF.RHO 三 EPSILON 16 SELF.EPSILON # RUNNING AVERAGE OF THE SQUARE GRADIENTS NONE SELF.E_GRAD 17 AT W 18 DEF UPDATE(SELF, W: NP.NDARRAY, GRAD WRT_W: NP.NDARRAY) RRAY) -> NP.NDARRAY: 19 IF SELF.E GRAD 20 NONE: SELF.E_GRAD - NP.ZEROS(NP.SHAPE(GRAD_WRT_W)) 21 22 # UPDATE AVERAGE OF GRADIENTS AT W 23 5* SEL 24 SELF.E GRAD - SELF.RHO  SELF.E_GRAD + (1 - SELF.RHO) NP.POWER(GRAD_WRT_W, 2) 25 26 RETURN W - SELF.LEARNING.RATE * GRAD WRT-W / NP.SQRT(SELF.E GRAD + SELF.EPSILON) -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1676455710161-97d20271-b5aa-4429-9c04-9a236807e0f8.png)<font style="color:rgb(36, 41, 47);">  
</font>

## _<font style="color:rgb(18, 18, 18);">Adadelta</font>_
[Zeiler，2012](https://arxiv.org/abs/1212.5701)<font style="color:rgb(64, 64, 64);"> 提出的 Adadelta 是对 Adagrad 的拓展，希望“拯救”一下它那迅速单边减小的学习率。</font>AdaDelta 除了<u>对二阶动量计算指数加权移动平均</u>以外（同上），还对当前时刻的梯度平方也计算一个指数加权移动平均 m，并用此值替换我们预先设置的学习率 𝛼，这样我们<font style="color:rgb(64, 64, 64);">不再需要人工设置默认学习率了！</font>

<!-- 这是一张图片，ocr 内容为：1 - P) M+8 & 2 + - Y) - A+-1 0 4 MT-1 - MT-2 -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673350606483-28f9acbf-8a0a-41a8-b80e-d949ff48cdbf.png)

<font style="color:rgb(18, 18, 18);"></font>

## _<font style="color:rgb(18, 18, 18);">Adam </font>__<font style="color:rgb(18, 18, 18);">🔥</font>_
自适应矩估计 Adam (Adaptive Moment Estimation) 是一种常用于训练深度神经网络的优化算法。它结合了两种其他优化算法的思想：RMSprop 和动量。



核心概念

1. **Adaptive Learning Rates**:  自适应学习率：  
Adam 根据梯度的第一和第二矩估计，为不同参数计算各自的适应学习率。
2. **Momentum**:  动量：  
它跟踪过去梯度的指数衰减平均值，类似于动量。
3. **RMSprop**:  RMSprop：  
它还跟踪过去平方梯度的指数衰减平均值。
4. **Bias Correction**:  偏差校正：  
Adam 包含用于校正第一和第二矩估计初始化的偏差校正项。



Adam 算法流程

给定参数 _θ_ 、目标函数 _f_(_θ_) 及其梯度 $ ∇_θf(θ)  $：

1. **Initialize**:  初始化：
    - Time step $ t=0 $
    - Parameters $ θ_0 $
    - 一阶动量 $ m_0=0 $
    - 二阶动量 $ v_0=0 $
    - 超参数：$ α $、$ β_1 $、$ β_2 $、$ ϵ $
2. **While not converged, do**: 
    1. 增加时间步长：$ t=t+1 $
    2. 计算梯度： $ g_t=∇_θf_t(θ_t−1) $
    3. 一阶动量估计（`Momentum`）：$ m_t=β_1⋅m_{t−1}+(1−β_1)⋅g_t $
    4. 二阶动量估计（`RMSProp`）： $ v_t=β_2⋅v_{t−1}+(1−β_2)⋅g_{t}^2 $
    5. 一阶动量偏差校正： $ \hat{m}=m_t/(1−β_1^t) $
    6. 二阶动量偏差校正： $ \hat{v}=v_t/(1−β_2^t) $
    7. 更新参数： $ θ_t=θ_{t−1}−α⋅\hat{m_t}/(\sqrt{\hat{v_t}}+ϵ) $



具体地，$ β1 $<font style="color:rgb(64, 64, 64);">的默认值为 0.9，</font>$ β2 $<font style="color:rgb(64, 64, 64);">是 0.999，</font>$ ϵ $<font style="color:rgb(64, 64, 64);">则是 </font>$ 10^{-8} $<font style="color:rgb(64, 64, 64);">。</font>

| <!-- 这是一张图片，ocr 内容为：MO , UO 0 I MUTI BIMT+ ( 1 - BI)VEL( 0) MOMENTUM UTT1 B20T+(1-B2)VEL(0)2 RMS PROP 日;-- MT+1 RMS PROP + MOMENTUM /UT+1 +LE-5 -->
![Parameter update rule for Adam Optimizer](https://cdn.nlark.com/yuque/0/2022/png/8420697/1648207784875-17f0fbd9-8139-407e-bd01-4644d95f8b4f.png) | <!-- 这是一张图片，ocr 内容为：INTTI A LOMNT 1 1 - 1 - 1 1 ) ( ) ( ) ( 1 - 1 - 1 1 1 1 1 1 1 1 1 1 1 1 ) ( ) ( 9 ( ) ( ) ( 1 1 1 1 ( MOMENTUM I VET1 B2UT+ ( 1 - BA) 2 RMS PROP MT+1 MT+1 1-B克 BIAS CORRECTION UT+1 UT+1 1-B左 E 0;- RMS PROP+MOMENTUM MT+1 UTT1+LE-5' -->
![Bias Correction for Adam Optimizer](https://cdn.nlark.com/yuque/0/2022/png/8420697/1648207875863-d4a275f0-cbc1-4176-8cb7-48983b1c4ee7.png) |
| :---: | --- |


<font style="color:#E8323C;">在参数更新的</font>**<font style="color:#E8323C;">最初几步</font>**<font style="color:#E8323C;">中，由于</font>$ m_t $<font style="color:#E8323C;">和</font>$ v_t $<font style="color:#E8323C;">是初始化为0向量，它们在最初几步的更新过程会偏向0（有偏差）。为了防止这种情况，Adam利用</font>$ β_i $<font style="color:#E8323C;">的 </font>`<font style="color:#E8323C;">t</font>`<font style="color:#E8323C;"> 次幂来修正这种偏差（</font>`<font style="color:#E8323C;">t</font>`<font style="color:#E8323C;"> 每次更新加1）。</font>

<!-- 这是一张图片，ocr 内容为：由于当十较小时,指数加权移动平均值的偏差较大,例如:设8,30,8,那么, U1三BUG+(1-B)01-0.9*0+0.1*40三4,显然U,和0;相差太大,所以通常 会加上一个修正因子1-8T,加了修正因子后的公式为 BUT-1 + 1 - 8)0T UT 二 1 - BT (显然,当娘小时,修正因子1-8*会起作用,当女足够大时8'-8%)-8%)-8" 修正因子会自动退场. -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673347269762-4ca349b0-daeb-4c92-a72d-0ac6a80518e3.png)

<font style="color:rgb(18, 18, 18);"></font>

**<font style="color:rgb(36, 41, 47);">💡</font>****<font style="color:rgb(36, 41, 47);">Adam Optimization 实现:</font>**

<!-- 这是一张图片，ocr 内容为：IMPORT NUMPY AS NP CLASS ADAM: SELF.LEARNING_RATE - LEARNING_RATE SELF.EPSILON EPSILON 8 SELF.BETA_1 - BETA_1 SELF.BETA_BETA_2 10 11 SELF.T O RES OF PAST GRADIENTS # DECAYING AVERAGES 12 SELF.M NONE F DECAYING AVERAGES OF PAST SQUARED GRADIENTS 13 SELF.V NONE 14 15 DEF UPDATE(SELF, W: NP.NDARRAY, GRAD WRT W: NP.NDARRAY) -> NP.NDARRAY: 16 SELF.T+1 IF SELF.M IS NONE: 17 SELF.M : NP.ZEROS(NP.SHAPE(GRAD_WRT_W)) 18 SELF.V 三 NP.ZEROS(NP.SHAPE(GRAD_WRT_W)) 19 20 (1 21 SELFMSELF.BETA_1 SELF.M+ SELF.BETA GRAD_WRTW 22 (1 NP.POWER(GRAD_WRT_W,2) SELF.V SELF.BETA_2 SELF.V+ SELF.BETA_2) 23 24 M HAT 三 SELF.M / (1 - SELF.BETA_1**SELF.T) 25 V_HAT ; SELF.V / (1 - SELF.BETA_2**SELF.T) 26 27 SELF.EPSILON) (HAT) W_UPDATE - SELF.LEARNING RATE * M_HAT / (NP.SQRT() 28 29 RETURN W-W UPDATE -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1676455737137-642299cd-1ac1-43ad-81d4-4f665ea42968.png)<font style="color:rgb(36, 41, 47);">  
</font>

## <font style="color:#2f5597;">AMSGrad</font>
在实际应用中，使用 RMSProp 和 Adam 算法有时候会出现模型不收敛的问题，但用 AdaGrad 算法却可以收敛。通过分析发现是指数滑动平均出现了问题。 AdaGrad 算法可以保证累计平方梯度 r_t≥r_(t−1) ，从而有自适应学习率始终下降，有利于收敛。但对于 Adam 等引入指数滑动平均的算法，不能保证  r_t≥r_(t−1) 恒成立，就有可能出现学习率增大的情况，这就有可能造成模型不收敛。为解决这个问题，AMSGrad 提出在每次迭代中，将  r_t 与上一时刻  r_(t−1) 进行比较，保留大者，这样就可以保证学习率不出现递增

<!-- 这是一张图片，ocr 内容为：(1 - B) G (1 - B) (1 - P) (1 - P) D D MAX QUI T 三ST -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673350771061-3af098f3-5520-487a-89eb-2030c1f52e29.png)



## <font style="color:rgb(64, 64, 64);">NAdam</font>
<font style="color:rgb(64, 64, 64);">如前文所见，Adam 可以看作是 RMSprop 和动量法的结合：RMSprop 贡献了指数衰减的过往梯度平方均值 </font><font style="color:rgb(64, 64, 64);">vt</font><font style="color:rgb(64, 64, 64);">，而 Momentum 捐赠了指数衰减的过往梯度均值 </font><font style="color:rgb(64, 64, 64);">mt</font><font style="color:rgb(64, 64, 64);"> 。我们也知道了 Nesterov 梯度加速法（NAG）要比一般动量法强。</font>

<font style="color:rgb(64, 64, 64);"></font>

[Dozat，2016](https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf)<font style="color:rgb(64, 64, 64);"> 设计的 Nesterov 加速自适应矩估计（Nesterov-accelerated Adaptive Moment Estimation，Nadam）就这样把 Adam 和 NAG 结合在了一起。</font>

<font style="color:rgb(18, 18, 18);"></font>

## <font style="color:rgb(18, 18, 18);">RAdam</font>
<font style="color:rgb(41, 41, 41);">全名是Rectified Adam，是一種改進Adam的方法，更精確來說，就是自動warmup版的Adam。</font>

<!-- 这是一张图片，ocr 内容为：RADAM ADAM A .MT TTINT WT+1 WT+1-WT WT VIT+E VOT +E WHERE WHERE MT MT INT  TNT  1-85 1 8 UT TA IFPT>4 I-BR 1-8克 三10 OTHERWISE AND TE (PL-4)(PT-2)POO IF PT>4 MTBIMT-11-1 B1) (POO-4)(POO-2)PT OWT PT OTHERWISE 1 01 1. UT-1 -1 +(1-82) AND OWT TO MT-1MT-1-1 - 1-81) LWT 72 OT UTB20T-11-1-82) TMT 2T PT POO- 1 -8 2 POO-1-82 -->
![](https://cdn.nlark.com/yuque/0/2022/png/8420697/1648205709838-2bbf7f90-ca64-4c15-b491-bdb9e08e2bc8.png)

<font style="color:rgb(18, 18, 18);"></font>

## <font style="color:rgb(64, 64, 64);">AdamW</font>
**<u><font style="color:rgb(64, 64, 64);">权重衰减是训练神经网络时用到的一种技术，目的在于保持权重值够小，从而不容易受到数值过大的数据的影响</font></u>****<font style="color:rgb(64, 64, 64);">。</font>**<font style="color:rgb(64, 64, 64);">一般认为大权重会有过拟合的倾向，权重想大理由要充分。衰减技术的实现是通过在损失函数中加一个权重值的惩罚函数，这样</font>本身比较大的一些权重对应的梯度也会比较大，惩罚也越大<font style="color:rgb(64, 64, 64);">。最流行的权重衰减形式莫过于 L2 正则化了，它对权重值的平方进行惩罚，能方便的同时处理正、负权重，可微性良好。</font>

<font style="color:rgb(64, 64, 64);"></font>

AdamW是在 Adam+L2正则化 的基础上进行改进的算法。由于Adam计算步骤中减去项会有除以梯度平方的累积，引入 L2 正则会使得梯度大的减去项偏小，从而具有大梯度的权重不会像解耦权重衰减那样得到正则化。 按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。相比梯度较小的参数，大权重参数的更新值反而更小一些。这显然是不合理的。<font style="color:rgb(64, 64, 64);">AdamW 修改了 Adam 里权重衰减正则化的传统实现方式，将权重衰减和梯度更新解耦。</font>

<!-- 这是一张图片，ocr 内容为：ADAM WITH L2 REGULARIZATION ADAM WITH DECOUPLED WEIGHT DECAY(ADAMW) ALGORITHM 2 AND 1:GIVENA001,B1,B1VEN.999 E10-8,入E R 2:INITIALIZE TIME STEP T O,PARAMETER VECTOR OT O EIR", R",FIRST MOMENT VECTOR MT-0 <0, SECOND MOMENT VECTOR VT-O, SCHEDULE MULTIPLIER NT-O E IR REPEAT T￥1 SELECT BATCH AND RETURN THE CORRESPONDING GRADIENT FT(0T-1)SELECTBATCH(0T-1) GTVFT(OT-1)+入0T-1 MT+BIMT-1 +(1-B1)GT HERE AND BELOW ALL OPERATIONS ARE ELEMENT-WISE NT B2VT-1 - B2)8? 9: MT/(1-B OF T B1 IS TAKEN TO THE POWER 10: ITVT/(1B克) 62 IS TAKEN TO THE POWER OF T 11: MTSETSCHEDULEMULTIPLIER(T) CAN BE FIXED, DECAY,OR ALSO BE USED FOR WARM RESTARTS AMT/(VVT+E)+Z0T-1 OTOT-MT 12: 13:UNTIL STOPPING CRITERION IS MET 14: RETURN OPTIMIZED PARAMETERS O -->
![](https://cdn.nlark.com/yuque/0/2023/png/8420697/1673351066245-ccb890e4-8152-4f92-860c-3a43420d0553.png)

<font style="color:rgb(64, 64, 64);">在实践中该方法取得了一定的效果，有被机器学习社区的一些人采纳。就这么一点小改动可以对性能产生那么大的影响，很有趣不是吗？</font>

<font style="color:rgb(18, 18, 18);"></font>

## <font style="color:rgb(64, 64, 64);">动态演示</font>
<font style="color:rgb(64, 64, 64);">下面是两幅</font><font style="color:rgb(64, 64, 64);"> </font>[Alec Radford](https://twitter.com/alecrad)<font style="color:rgb(64, 64, 64);"> </font><font style="color:rgb(64, 64, 64);">绘制的动态图，可以直观感受一下上面多数方法的优化表现。</font>

<!-- 这是一张图片，ocr 内容为： -->
![损失轮廓线上的 SGD 优化](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_contours_evaluation_optimizers.gif)

<font style="color:rgb(64, 64, 64);">上图显示的是损失表面（</font>[Beale 函数](https://www.sfu.ca/~ssurjano/beale.html)<font style="color:rgb(64, 64, 64);">）上随时间推移不同方法的行为表现。可以看到 Adagrad，Adadelta 和 RMSprop 几乎是立刻转向正确方向并快速在相似位置收敛，而 Momentum 和 NAG 则偏离了轨道，感觉像是要往坡下滚的球。但由于有前瞻获得的增量反馈， NAG 很快进行了修正并达到最小值。</font>

<font style="color:rgb(64, 64, 64);"></font>

<font style="color:rgb(64, 64, 64);">下面的图 6 则展示了算法在鞍点上的表现，鞍点就是一个维度是正斜率，另一维度是负斜率的点。前文中我们已经说了这对 SGD 是个麻烦，可以看到 SGD，Momentum 和 NAG 很难打破均衡，尽管后两者最终还是设法离开了鞍点。而 Adagrad，RMSprop 和 Adadelta 很快就走向了负斜率。</font>

<!-- 这是一张图片，ocr 内容为： -->
![鞍点上的 SGD 优化](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2021-01-06_saddle_point_evaluation_optimizers.gif)

<font style="color:rgb(64, 64, 64);">如我们所见，自适应学习率方法，即 Adagrad，Adadelta，RMSprop 和 Adam 是最适合且能得到最佳收敛结果的方法。</font>[  
](http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/)

# <font style="color:rgb(64, 64, 64);">选哪个优化器</font>
<font style="color:rgb(64, 64, 64);">所以该选哪个优化器呢？如果输入数据稀疏，那选一个自适应学习率方法大概率上效果不会差。这样做的额外一点好处在于不需要调学习率，用默认值就能取得最佳效果。</font>

[https://mp.weixin.qq.com/s/V8rdtCRYUQXrN6jS2oDlxQ](https://mp.weixin.qq.com/s/V8rdtCRYUQXrN6jS2oDlxQ)

<font style="color:rgb(26, 26, 26);">目前，最流行并且使用很高的优化器（算法）包括SGD、具有动量的SGD、RMSprop、具有动量的RMSProp、AdaDelta和Adam。在实际应用中，选择哪种优化器应结合具体问题；同时，也优化器的选择也取决于使用者对优化器的熟悉程度（比如参数的调节等等）。</font>

+ <font style="color:rgb(1, 1, 1);">对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</font>
+ <font style="color:rgb(1, 1, 1);">SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</font>
+ <font style="color:rgb(1, 1, 1);">如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</font>
+ <font style="color:rgb(1, 1, 1);">Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</font><font style="color:rgb(64, 64, 64);">RMSprop 是对 Adagrad 的扩展，解决了后者学习率快速消失的问题。Adadelta 也一样，只是 Adadelta 是在分子更新中用参数的 RMS 进行更新。最后 Adam 给 RMSprop 加上了动量和偏差修正。所以 RMSprop，Adadelta 和 Adam 在相似环境下差别并不算大</font>
+ <font style="color:rgb(1, 1, 1);">在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</font>
+ <font style="color:rgb(1, 1, 1);">如果验证损失较长时间没有得到改善，可以停止训练。</font>
+ <font style="color:rgb(1, 1, 1);">添加梯度噪声（高斯分布  ）到参数更新，可使网络对不良初始化更加健壮，并有助于训练特别深而复杂的网络。</font>

# <font style="color:rgb(18, 18, 18);">参考</font>
+ [CS231n Convolutional Neural Networks for Visual Recognition](https://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/)
+ [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.6980.pdf)
+ <font style="color:rgb(18, 18, 18);">《统计学习方法》李航</font>
+ [机器学习各优化算法的简单总结-知乎](https://zhuanlan.zhihu.com/p/25572077)
+ [梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)
+ [梯度下降与优化综述-明月轩](https://libertydream.github.io/2021/01/25/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96/)



